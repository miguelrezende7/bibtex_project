,author,title,keywords,abstract,year,type_publication,doi,book_journal,Title_SCI,Title_JCS,SCI_FACTOR,JCS_FACTOR
0,"Osvaldo Gogliano Sobrinho,Liedi Ledi Mariani Bernucci,Pedro Luiz Pizzigatti Corrêa,Rosângela dos Santos Motta,Jeaneth Machicao,Angelo Samuel Junqueira,Robson Correia da Costa,Wellingthon Dias de Queiroz,Thales Cesar Giriboni de Mello e Silva,Pedro Lopes Ferraz,Luciano Cassaro,Luciano Oliveira",Big data analytics in support of the under-rail maintenance management at Vitória – Minas Railway,"Geometry,Instruments,Railway accidents,Conferences,Big Data,Rail transportation,Sensors","This paper describes an ongoing study using data collected by an instrumented ore car on Vitória–Minas Railway, operated by Vale in Brazil. The research uses big data analysis methods over collected data by the instrumented car during its voyages. Railway geometry issues can cause undesirable movements on the wagons that can cause discomfort for passengers or instability for the cargo. In the worst scenario, derailments can occur. Each second, several sensors installed on the instrumented car collect data about velocity, acceleration, and movements on the wagon. The volume of collected data is impressive since the railway has about 2,000 km of extension. That volume compels us to use big data analytics methods. As the result of the research, the team aims to establish some levels of operational conditions, named as severity indexes, which can indicate to the maintenance teams the necessity of intervention on the railway.",2021,Big data analytics in support of the under-rail maintenance management at Vitória – Minas Railway,10.1109/BigData52589.2021.9671739,2021 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
1,"Mohammad Sultan Mahmud,Joshua Zhexue Huang,Salman Salloum,Tamer Z. Emara,Kuanishbay Sadatdiynov",A survey of data partitioning and sampling methods to support big data analysis,"Big Data,Distributed databases,Computational modeling,Data models,Computer architecture,Sampling methods","Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratified sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and block-level sampling. Record-level sampling is not as efficient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.",2020,A survey of data partitioning and sampling methods to support big data analysis,10.26599/BDMA.2019.9020015,Big Data Mining and Analytics,N/A,N/A,0.0,0.0
2,"Domenico Redavid,Roberto Corizzo,Donato Malerba",An OWL Ontology for Supporting Semantic Services in Big Data Platforms,"Ontologies,OWL,Data models,Big Data,Semantics,Analytical models","In the last years, there was a growing interest in the use of Big Data models to support advanced data analysis functionalities. Many companies and organizations lack IT expertise and adequate budget to have benefits from them. In order to fill this gap, a model-based approach for Big Data Analytics-as-a-service (MBDAaaS) can be used. The proposed model, composed by declarative, procedural and deployment (sub) models, can be used to select a deployable set of services based on a set of user preferences shaping a Big Data Campaign (BDC). The deployment of a BDC requires that the selection of services has to be carried out on the basis of coherent and non conflictual user preferences. In this paper we propose an OWL ontology in order to solve this issue.",2018,An OWL Ontology for Supporting Semantic Services in Big Data Platforms,10.1109/BigDataCongress.2018.00039,2018 IEEE International Congress on Big Data (BigData Congress),N/A,N/A,0.0,0.0
3,"Fang Peng,Honggang Wang,Li Zhuang,Minnan Wang,Chengyue Yang",Methods of enterprise electronic file content information mining under big data environment,"Scalability,Big Data,Information age,Search problems,Data mining,Electronic countermeasures,Software engineering","As the product of the digital age, big data technology and computer information technology can greatly improve the efficiency and quality of file management and promote the development of enterprises. Based on this, this paper first analyzes the current status of enterprise archives management; Secondly, this paper discusses the countermeasures of information mining of electronic documents of innovative enterprises in the digital age. Text information mining is beneficial to improve the efficiency of text information search and utilization, aiming at the existing problems of traditional methods, the text information mining method is proposed.",2020,Methods of enterprise electronic file content information mining under big data environment,10.1109/ICBASE51474.2020.00008,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),N/A,N/A,0.0,0.0
4,"Justin McHugh,Paul E. Cuddihy,Jenny Weisenberg Williams,Kareem S. Aggour,Vijay S. Kumar,Varish Mulwad",Integrated access to big data polystores through a knowledge-driven framework,"Semantics,Big Data,Data models,Ontologies,Time series analysis,Databases,Triples (Data structure)","The recent successes of commercial cognitive and AI applications have cast a spotlight on knowledge graphs and the benefits of consuming structured semantic data. Today, knowledge graphs are ubiquitous to the extent that organizations often view them as a “single source of truth” for all of their data and other digital artifacts. In most organizations, however, Big Data comes in many different forms including time series, images, and unstructured text, which often are not suitable for efficient storage within a knowledge graph. This paper presents the Semantics Toolkit (SemTK), a framework that enables access to polyglot-persistent Big Data stores while giving the appearance that all data is fully captured within a knowledge graph. SemTK allows data to be stored across multiple storage platforms (e.g., Big Data stores such as Hadoop, graph databases, and semantic triple stores) - with the best-suited platform adopted for each data type - while maintaining a single logical interface and point of access, thereby giving users a knowledge-driven veneer across their data. We describe the ease of use and benefits of constructing and querying polystore knowledge graphs with SemTK via four industrial use cases at GE.",2017,Integrated access to big data polystores through a knowledge-driven framework,10.1109/BigData.2017.8258083,2017 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
5,"Kyounghyun Park,Minh Chau Nguyen,Heesun Won",Web-based collaborative big data analytics on big data as a service platform,"Big data,Portals,Collaboration,Web services,Monitoring,Streaming media","As data has been increasing explosively due to development of social networks and cloud computing, there has been a new challenge for storing, processing, and analyzing a large volume of data. The traditional technologies do not become a proper solution to process big data so that a big data platform has begun to emerge. It is certain that big data platform helps users develop analysis service effectively. However, it still takes a long time to collect data, develop algorithms and analytics services. We present a collaborative big data analytics platform for big data as a service. Developers can collaborate with each other on the platform by sharing data, algorithms, and services. Therefore, this paper describes big data analytics platform that effectively supports to manage big data and develop analytics algorithms and services, collaborating with data owners, data scientists, and service developers on the Web. Finally, we introduce a CCTV metadata analytics service developed on the platform.",2015,Web-based collaborative big data analytics on big data as a service platform,10.1109/ICACT.2015.7224859,2015 17th International Conference on Advanced Communication Technology (ICACT),N/A,N/A,0.0,0.0
6,"Liang Lixin,Lin Lin",The big data analysis and mining of people's livelihood appeal based on time series modeling and algorithm,"Analytical models,Data analysis,Time series analysis,Big Data,Market research,Prediction algorithms,Data models","In order to analyze the big data of people's livelihood appeal, this paper proposes a time series modeling and algorithm to decompose the time series {x(t)} of data into long-term change trend L(t), short-term change trend S(t) and occasional change e(t). Then use this method to break down the data of six types of people's livelihood appeal such as unlicensed vendor, industrial noise, sewer cover, academic qualification, out-of-store operation and public transportation, combine other data for correlation analysis, find out the cause of the appeal event and make predictions. The experimental results verify the effectiveness of time series analysis in big data analysis and mining of people's livelihood appeal, and it is an useful attempt in the analysis of e-government big data.",2020,The big data analysis and mining of people's livelihood appeal based on time series modeling and algorithm,10.1109/HPBDIS49115.2020.9130588,2020 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS),N/A,N/A,0.0,0.0
7,"Ardi Imawan,Joonho Kwon",A timeline visualization system for road traffic big data,"Roads,Big data,Data visualization,Data mining,Conferences,Cloud computing,Image color analysis","The rapid converging of big data and IoT (Internet of Things) technologies provides more opportunities in the area of road traffic applications. In this paper, we discuss a timeline visualization tool which enables us to better understand of traffic behaviors from road traffic big data.",2015,A timeline visualization system for road traffic big data,10.1109/BigData.2015.7364125,2015 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
8,"He Liu,Fupeng Huang,Han Li,Weiwei Liu,Tongxun Wang",A Big Data Framework for Electric Power Data Quality Assessment,"Big Data,Data integrity,Power grids,History,Real-time systems,Sensors","Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.",2017,A Big Data Framework for Electric Power Data Quality Assessment,10.1109/WISA.2017.29,2017 14th Web Information Systems and Applications Conference (WISA),N/A,N/A,0.0,0.0
9,"Darlan Arruda,Nazim H. Madhavji",Towards a big data requirements engineering artefact model in the context of big data software development projects: Poster extended abstract,"Big Data,Software,Requirements engineering,Data models,Computational modeling,Context modeling,Software engineering","In this paper, we describe our ongoing research aimed at defining a Requirements Engineering Artefact Model (REAM) in the context of Big Data software applications. This model aims to provide a “big picture” of the Requirements Engineering work products created and used in Big Data software development projects. REAM are important tools that can be used as references for the definition of domain-specific RE models, system life-cycle processes and artefact-centered processes, currently bereft in the Big Data Software Engineering research.",2017,Towards a big data requirements engineering artefact model in the context of big data software development projects: Poster extended abstract,10.1109/BigData.2017.8258521,2017 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
10,"Guanlin Zhai,Yan Yang,Heng Wang,Shengdong Du",Multi-attention fusion modeling for sentiment analysis of educational big data,"Two dimensional displays,Task analysis,Sentiment analysis,Data models,Data mining,Context modeling,Analytical models","As an important branch of natural language processing, sentiment analysis has received increasing attention. In teaching evaluation, sentiment analysis can help educators discover the true feelings of students about the course in a timely manner and adjust the teaching plan accurately and timely to improve the quality of education and teaching. Aiming at the inefficiency and heavy workload of college curriculum evaluation methods, a Multi-Attention Fusion Modeling (Multi-AFM) is proposed, which integrates global attention and local attention through gating unit control to generate a reasonable contextual representation and achieve improved classification results. Experimental results show that the Multi-AFM model performs better than the existing methods in the application of education and other fields.",2020,Multi-attention fusion modeling for sentiment analysis of educational big data,10.26599/BDMA.2020.9020024,Big Data Mining and Analytics,N/A,N/A,0.0,0.0
11,"Paul Cuddihy,Jenny Weisenberg Williams,Vijay S. Kumar,Kareem S. Aggour,Andrew Crapo,Sharad Dixit",FDC Cache: Semantics-driven Federated Caching and Querying for Big Data,"Semantic Web,Semantics,Merging,Memory,Big Data,Data models,Turbines","To deliver business value, most data-driven enterprises and applications require data to be extracted and merged from otherwise siloed data storage platforms. FDC Cache has been designed and developed to enable the fusion and caching of data drawn from multiple small and/or Big Data stores. This capability executes a sequence of queries, wherein the results from one query may be used to constrain subsequent queries. The results of each query are linked with results from previous queries, incrementally building a cache of semantically linked data that can be used to support multiple independent data requests. FDC Cache uses Semantic Web technologies, and knowledge graphs in particular, to describe the relevant data and relationships in a computable model. This enables applications to reason over the graph, for example to dynamically retrieve targeted subsets of data comprised of previously disparate information. We have successfully applied FDC Cache to two distinct industrial use cases: (i) merging data across multiple sources to assemble information about current parts in a gas turbine, and (ii) dynamically aligning siloed data from electric grid transmission and distribution networks to an industry-standard common model, in which the cache creation time has been shown to scale sub-linearly with the number of data elements. FDC Cache has been open-sourced as part of the GE-developed open source Semantics Toolkit.",2020,FDC Cache: Semantics-driven Federated Caching and Querying for Big Data,10.1109/BigData50022.2020.9377876,2020 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
12,Thuan L Nguyen,A Framework for Five Big V’s of Big Data and Organizational Culture in Firms,"Big Data,Cultural differences,Data mining,Productivity,Companies","Big data analytics has come out as a new important field of study for both researchers and practitioners, demonstrating the significant demand for solutions to business problems in a data-driven knowledge-based economy. Additionally, empirical studies examining the impacts of the nascent technology on organizational performance, especially the influence of organizational culture on the five Big V's of big data, remain scarce. The present study aims to fill the gap. Based on Cameron and Quinn's organizational cultural model, this study proposes a theoretical framework that describes how each type of organizational culture - hierarchy, clan, adhocracy, and market - has impacts on each Big V of big data. The framework suggests that firms, influenced by their organizational culture, have different views on how important each Big V's should be. The study argues that organizations should develop, nurture, and maintain an adhocracy organizational culture that has a positive impact on each of the five Big V's to harness the full potential of big data.",2018,A Framework for Five Big V’s of Big Data and Organizational Culture in Firms,10.1109/BigData.2018.8622377,2018 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
13,Jayesh Patel,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,"Data models,Big Data,Business,Analytical models,Computational modeling,Lakes,Solid modeling","The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",2019,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,10.1109/BigData47090.2019.9005614,2019 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
14,"Carlos Ordonez,Sikder Tahsin Al-Amin,Ladjel Bellatreche",An ER-Flow Diagram for Big Data,"Visualization,Unified modeling language,Software algorithms,Relational databases,Big Data,Software systems,Erbium","ER diagrams have a proven track record to rep-resent data structure and relationships, in many CS problems, beyond relational databases. The ER diagram strengths are abstraction, generality, flexibility, and intuitive visual representation, with few weaknesses; hence its popularity. The main con is the old box-diamond-ellipse-line notation, which has been subsumed by the more modern and simpler UML box-line notation. Given the broad, varied, and dynamic nature of big data ER diagrams are mostly ignored, except when the data sources are databases. It is common wisdom raw big data needs significant pre-processing before computing any analytics, resulting in a long chain of data transformations computed in SQL, Python, or R languages, for instance. On the other hand, flow diagrams remain the main mechanism to visualize major components of a software system or main processing steps of an algorithm, showing rectangles (verbs) connected by arrows (processing order, dependence). In this work, we propose to combine both diagrams into one. We propose a hybrid diagram, which we call ER-Flow, based on modern UML notation, to assist analysts in data pre-processing and exploration. Aiming to introduce a minimal change to the ER diagram, we extend relationships lines with an arrow, indicating processing flow and we annotate entities coming from pre-processing with numbers and transformation labels. We illustrate how our novel ER-Flow diagram can help the user navigate big data at the metadata level, providing an integrated view of data and source code, with many practical benefits.",2020,An ER-Flow Diagram for Big Data,10.1109/BigData50022.2020.9378088,2020 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
15,"Huang Xiaoyan,Jiang Rong,Hu Xiaoming,Wang Luxiao",Thoughts On The Ecological Environment Management Innovation Driven By Big Data,"Big Data,Standards,Security,Government,Decision making,Biological system modeling","Based on the review of the technical characteristics and development process of big data, combined with the development trend of ecological environment big data, this paper puts forward the thinking of using big data to drive the innovation of ecological environment management and the countermeasures and suggestions for the intelligent application of ecological environment big data.",2020,Thoughts On The Ecological Environment Management Innovation Driven By Big Data,10.1109/ICBAIE49996.2020.00015,"2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",N/A,N/A,0.0,0.0
16,"Kareem S. Aggour,Vijay S. Kumar,Paul Cuddihy,Jenny Weisenberg Williams,Vipul Gupta,Laura Dial,Tim Hanlon,Justin Gambone,Joseph Vinciquerra",Federated Multimodal Big Data Storage &amp; Analytics Platform for Additive Manufacturing,"Three-dimensional printing,Semantics,Data models,Big Data,Powders,Solid modeling","Additive technologies are expected to revolutionize manufacturing across almost every industry, but there is a sizable gap between the current state of the technology and the maturity required for it to achieve widespread adoption. Advanced analytics are required to improve the reliability and repeatability of additive manufacturing, and those analytics require data. Large volumes of multimodal data are generated and used throughout the additive manufacturing lifecycle, from material design to part design and simulation, part printing to post-processing and inspection. To capture and link that diverse Big Data together, we have designed and developed a federated multimodal Big Data storage and analytics platform comprised of three tiers-a distributed polyglot data storage and analysis tier with different repositories for different data structures, a metadata knowledge graph tier for modeling the data and their relationships across the various repositories, and a user interface tier for visualizing, exploring and invoking analytics on the data. The platform has been used to integrate a collection of previously disparate steps to optimize the process parameters used to build a new additive material, enabling materials scientists and other non-software experts to capture, visualize and analyze the requisite data through a single user interface.",2019,Federated Multimodal Big Data Storage &amp; Analytics Platform for Additive Manufacturing,10.1109/BigData47090.2019.9006495,2019 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
17,"Ryan Norman,Jason Bolin,Edward T. Powell,Sanket Amin,John Nacker",Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter,"Big Data,Knowledge management,Tools,US Department of Defense,Cloud computing,Computer architecture,Data analysis","The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&amp;E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&amp;E community can support the demands of next-generation weapon systems.The true product of T&amp;E is knowledge ascertained through the collection of information about a system or item under test. However, the T&amp;E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&amp;E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.",2018,Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter,10.1109/BigData.2018.8622388,2018 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
18,"Xiao HongJu,Wang Fei,Wang FenMei,Wang XiuZhen",Some key problems of data management in army data engineering based on big data,"Data engineering,Data analysis,Data integration,Big Data,Distributed databases,Data models,Uncertainty","This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.",2017,Some key problems of data management in army data engineering based on big data,10.1109/ICBDA.2017.8078796,2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA),N/A,N/A,0.0,0.0
19,"Yong-Hong Kuo,Janny M.Y. Leung,Kelvin K.F. Tsoi,Helen M. Meng,Colin A. Graham",Embracing Big Data for Simulation Modelling of Emergency Department Processes and Activities,"Data models,Computational modeling,Hidden Markov models,Hospitals,Computers,Big data","Simulation has been demonstrated to be a powerful tool to mimic processes and activities in emergency departments. However, most applications only rely on the data that were manually input by the staff in the departments. First, this practice does not guarantee that the required data to build the simulation models are captured in the computer system, as some information about the processes of emergency departments are not electronically stored. Second, human errors and missing data are also common for manual inputs. A simulation model that is incapable of representing the actual system of the emergency department will deliver wrong conclusions to hospital administrators and may lead to negative consequences if they trust the simulation results. In this paper, we present a case study of developing a simulation model of an emergency department in Hong Kong and discuss the data challenges. Then we propose an RFID-enabled infrastructure to automatically capture large volumes of data regarding the patient activities in the ED in order to build simulation models of more details and a higher accuracy.",2015,Embracing Big Data for Simulation Modelling of Emergency Department Processes and Activities,10.1109/BigDataCongress.2015.52,2015 IEEE International Congress on Big Data,N/A,N/A,0.0,0.0
20,"Mardhani Riasetiawan,Ferian Anggara,Ahmad Ashari,Sarju Winardi,Bambang Nurcahyo Prastowo",Data Model and Analysis for Big Data Mapping and Management in the Energy Data Platform,"Renewable energy sources,Oils,Standardization,Production,Big Data,Data science,Data models","The energy data scope is very broad including oil and gas, coal, minerals, new energy, renewable and conversion energy, electricity, and others. The different volume, variety, veracity, and velocity of data have challenge to address with the energy data model. The works focus on the development of data model for big data storage implementation schemes in the energy data. The data model has produces by analyzed the national energy big data architecture, develop the data mapping and correlation, big data master data management with energy industry standardization, big data management portal for upstream energy and downstream energy. The research has goal to establishing a foundation for building technology and big data management in the energy sector which includes petroleum, coal, geothermal and renewable energy, and in the future can be the basis for predictive analysis and national energy production.",2021,Data Model and Analysis for Big Data Mapping and Management in the Energy Data Platform,10.1109/DATABIA53375.2021.9650117,"2021 International Conference on Data Science, Artificial Intelligence, and Business Analytics (DATABIA)",N/A,N/A,0.0,0.0
21,"Alfredo Cuzzocrea,Ernesto Damiani",Pedigree-ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments,"Big Data,Data privacy,Distributed databases,Proposals,Cloud computing,Protocols,Data models","This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the ""pedigree"" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacypreserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.",2018,Pedigree-ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments,10.1109/CCGRID.2018.00100,"2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",N/A,N/A,0.0,0.0
22,"Alfredo Cuzzocrea,Ernesto Damiani","Privacy-Preserving Big Data Exchange: Models, Issues, Future Research Directions","Data privacy,Social networking (online),Smart cities,Conferences,Biological system modeling,Data integration,Big Data","Big data exchange is an emerging problem in the context of big data management and analytics. In big data exchange, multiple entities exchange big datasets beyond the common data integration or data sharing paradigms, mostly in the context of data federation architectures. How to make big data exchange while ensuring privacy preservation constraintsƒ The latter is a critical research challenge that is gaining momentum on the research community, especially due to the wide family of application scenarios where it plays a critical role (e.g., social networks, bio-informatics tools, smart cities systems and applications, and so forth). Inspired by these considerations, in this paper we provide an overview of models and issues in the context of privacy-preserving big data exchange research, along with a selection of future research directions that will play a critical role in next-generation research.",2021,"Privacy-Preserving Big Data Exchange: Models, Issues, Future Research Directions",10.1109/BigData52589.2021.9671686,2021 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
23,"Roberto Espinosa,Larisa Garriga,Jose Jacobo Zubcoff,Jose-Norberto Mazón",Linked Open Data mining for democratization of big data,"Data mining,Big data,Resource description framework,Data models,Knowledge based systems,Data analysis,Proposals","Data is everywhere, and non-expert users must be able to exploit it in order to extract knowledge, get insights and make well-informed decisions. The value of the discovered knowledge from big data could be of greater value if it is available for later consumption and reusing. In this paper, we present an infrastructure that allows non-expert users to (i) apply user-friendly data mining techniques on big data sources, and (ii) share results as Linked Open Data (LOD). The main contribution of this paper is an approach for democratizing big data through reusing the knowledge gained from data mining processes after being semantically annotated as LOD, then obtaining Linked Open Knowledge. Our work is based on a model-driven viewpoint in order to easily deal with the wide diversity of open data formats.",2014,Linked Open Data mining for democratization of big data,10.1109/BigData.2014.7004479,2014 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0
24,"Joan L. Aron,Brand Niemann",Sharing best practices for the implementation of Big Data applications in government and science communities,"Big data,Government,Semantics,Communities,Conferences,Best practices,Data privacy","The Federal Big Data Working Group supports the Federal Big Data Initiative but is not endorsed by the Federal Government or its agencies. This working group uses meetups with onsite and virtual participation to share best practices for the implementation of Big Data applications in government and science communities. Decision-makers and the scientific community interact with data science in order to take advantage of the Big Data transformation of how information is used in science, decision support, data discovery and data publishing. The working group federates use cases, data publications, solutions and technologies. The range of topics is illustrated in a keynote and panel discussion at a recent Big Data conference and in a summary of recent working group meetups.",2014,Sharing best practices for the implementation of Big Data applications in government and science communities,10.1109/BigData.2014.7004469,2014 IEEE International Conference on Big Data (Big Data),N/A,N/A,0.0,0.0

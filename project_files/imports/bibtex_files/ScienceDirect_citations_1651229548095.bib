@article{VYDRA2019101383,
title = {Techno-optimism and policy-pessimism in the public sector big data debate},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302326},
author = {Simon Vydra and Bram Klievink},
keywords = {Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance},
abstract = {Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.}
}
@article{BELL2021453,
title = {Exploring future challenges for big data in the humanitarian domain},
journal = {Journal of Business Research},
volume = {131},
pages = {453-468},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306172},
author = {David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan},
keywords = {Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value},
abstract = {This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.}
}
@article{WANG202116,
title = {A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {16-32},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301990},
author = {Pei Wang and Ming Luo},
keywords = {Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing},
abstract = {Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.}
}
@article{LIU2020123,
title = {Urban big data fusion based on deep learning: An overview},
journal = {Information Fusion},
volume = {53},
pages = {123-133},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301393},
author = {Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang},
keywords = {Urban computing, Big data, Data fusion, Deep learning},
abstract = {Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.}
}
@incollection{PLOTKIN2021245,
title = {Chapter 10 - Big Data Stewardship and Data Lakes},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {245-255},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000103},
author = {David Plotkin},
keywords = {Big data, data lake, unstructured data, zone},
abstract = {Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.}
}
@article{ELIA2020617,
title = {A multi-dimension framework for value creation through Big Data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {617-632},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120302212},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{MAJOR202056,
title = {Using big data in pediatric oncology: Current applications and future directions},
journal = {Seminars in Oncology},
volume = {47},
number = {1},
pages = {56-64},
year = {2020},
note = {Pediatric Oncology},
issn = {0093-7754},
doi = {https://doi.org/10.1053/j.seminoncol.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093775420300063},
author = {Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum},
keywords = {Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics},
abstract = {Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.}
}
@article{TAMYM2021102,
title = {A big data based architecture for collaborative networks: Supply chains mixed-network},
journal = {Computer Communications},
volume = {175},
pages = {102-111},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001924},
author = {Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}},
keywords = {Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness},
abstract = {Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.}
}
@article{ZHAO2021101196,
title = {Prediction model of ecological environmental water demand based on big data analysis},
journal = {Environmental Technology & Innovation},
volume = {21},
pages = {101196},
year = {2021},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2020.101196},
url = {https://www.sciencedirect.com/science/article/pii/S2352186420314966},
author = {Lihong Zhao},
keywords = {Big data analysis, Ecological environment, Water demand, Prediction},
abstract = {The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.}
}
@article{VALENCIAPARRA2020101180,
title = {Unleashing Constraint Optimisation Problem solving in Big Data environments},
journal = {Journal of Computational Science},
volume = {45},
pages = {101180},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101180},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304816},
author = {Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López},
keywords = {Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format},
abstract = {The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.}
}
@article{KUN2019556,
title = {Application of Big Data Technology in Scientific Research Data Management of Military Enterprises},
journal = {Procedia Computer Science},
volume = {147},
pages = {556-561},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302406},
author = {Wang Kun and Liu Tong and Xie Xiaodan},
keywords = {big data technology, scientific research data, data analysis, decision},
abstract = {Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.}
}
@article{CARPIOPINEDO2020102859,
title = {Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data},
journal = {Cities},
volume = {106},
pages = {102859},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312075},
author = {José Carpio-Pinedo and Javier Gutiérrez},
keywords = {Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid},
abstract = {While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.}
}
@article{TRUJILLO2021101911,
title = {Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101911},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101911},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000380},
author = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
keywords = {Conceptual modeling, Big Data, Machine learning, Artificial Intelligence},
abstract = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.}
}
@article{MIRACOLO2022S206,
title = {POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review},
journal = {Value in Health},
volume = {25},
number = {1, Supplement },
pages = {S206},
year = {2022},
note = {Emerging Frontiers and Opportunities},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1002},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521027972},
author = {A Miracolo and M Mills and P Kanavos}
}
@article{LV2021103298,
title = {Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data},
journal = {Cities},
volume = {116},
pages = {103298},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121001980},
author = {Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng},
keywords = {Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities},
abstract = {With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.}
}
@article{SBAI2020938,
title = {A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems},
journal = {Procedia Computer Science},
volume = {176},
pages = {938-947},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319876},
author = {Ines Sbai and Saoussen Krichen},
keywords = {Big data analytic, Decision Support System, DVRP, S-GA, Spark},
abstract = {Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.}
}
@article{SMALEC20215156,
title = {Big Data as a tool helpful in communication management},
journal = {Procedia Computer Science},
volume = {192},
pages = {5156-5165},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.293},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020329},
author = {Agnieszka Smalec},
keywords = {Big Data, marketing communication, management, data processing, collection, communication management},
abstract = {The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.}
}
@article{HADJSASSI2019534,
title = {A New Architecture for Cognitive Internet of Things and Big Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {534-543},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
author = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{YIN2022104285,
title = {Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104285},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004764},
author = {Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan},
keywords = {TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning},
abstract = {The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{DECAMARGOFIORINI2018112,
title = {Management theory and big data literature: From a review to a research agenda},
journal = {International Journal of Information Management},
volume = {43},
pages = {112-129},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830553X},
author = {Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}},
keywords = {Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda},
abstract = {The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.}
}
@article{BALTI2020101136,
title = {A review of drought monitoring with big data: Issues, methods, challenges and research directions},
journal = {Ecological Informatics},
volume = {60},
pages = {101136},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300868},
author = {Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing},
abstract = {Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.}
}
@article{LV2020101937,
title = {Achieving secure big data collection based on trust evaluation and true data discovery},
journal = {Computers & Security},
volume = {96},
pages = {101937},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302133},
author = {Denglong Lv and Shibing Zhu},
keywords = {Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network},
abstract = {Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.}
}
@article{MOHARM2019100945,
title = {State of the art in big data applications in microgrid: A review},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100945},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100945},
url = {https://www.sciencedirect.com/science/article/pii/S147403461830702X},
author = {Karim Moharm},
keywords = {Big data, Microgrid},
abstract = {The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.}
}
@article{PAIGE20211467,
title = {A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health},
journal = {Heart, Lung and Circulation},
volume = {30},
number = {10},
pages = {1467-1476},
year = {2021},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2021.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1443950621005175},
author = {Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree},
keywords = {Big data, Datasets, Cardiovascular disease, National platform},
abstract = {Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.}
}
@article{HUGHES2020120300,
title = {Sowing the seeds of value? Persuasive practices and the embedding of big data analytics},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120300},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120300},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311264},
author = {Jeffrey Hughes and Kirstie Ball},
keywords = {Big data analytics, Persuasion, Practice, Capabilities, Value},
abstract = {This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.}
}
@article{VANAM2021,
title = {Analysis of twitter data through big data based sentiment analysis approaches},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.486},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391501},
author = {Harika Vanam and Jeberson {Retna Raj R}},
keywords = {Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm},
abstract = {The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.}
}
@article{SU2020138984,
title = {Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis},
journal = {Science of The Total Environment},
volume = {733},
pages = {138984},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138984},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720325018},
author = {Yuan Su and Yanni Yu and Ning Zhang},
keywords = {Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis},
abstract = {Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.}
}
@article{HAJLI2020135,
title = {Understanding market agility for new product success with big data analytics},
journal = {Industrial Marketing Management},
volume = {86},
pages = {135-143},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304735},
author = {Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem},
keywords = {Big data analytics, Customer agility, Effective use of data, New product success},
abstract = {The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.}
}
@article{NEILSON201935,
title = {Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications},
journal = {Big Data Research},
volume = {17},
pages = {35-44},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303866},
author = {Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra},
keywords = {Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero},
abstract = {Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.}
}
@article{YADEGARIDEHKORDI2018199,
title = {Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {199-210},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518304141},
author = {Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim},
keywords = {Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS},
abstract = {Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.}
}
@article{TSAI2019306,
title = {Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {306-310},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300347},
author = {Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez},
abstract = {In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.}
}
@incollection{FENG2021145,
title = {Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {145-160},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000061},
author = {Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu},
keywords = {Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence},
abstract = {With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.}
}
@article{ATITALLAH2020100303,
title = {Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions},
journal = {Computer Science Review},
volume = {38},
pages = {100303},
year = {2020},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100303},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304032},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala},
keywords = {Internet of Things, Deep Learning, Smart city, Big data analytics, Review},
abstract = {The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.}
}
@incollection{WANG202135,
title = {Chapter 2 - Big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {35-49},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000179},
author = {Lidong Wang and Cheryl Alexander},
keywords = {Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning},
abstract = {Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.}
}
@article{BAG2021120420,
title = {Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120420},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120420},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312464},
author = {Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi},
keywords = {Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing},
abstract = {ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.}
}
@article{NARAYANAN2020,
title = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
author = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@article{LI202149,
title = {The critical need to establish standards for data quality in intelligent medicine},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {49-50},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000073},
author = {Ruiyang Li and Yahan Yang and Haotian Lin},
keywords = {Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management},
abstract = {Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.}
}
@article{SHAMIM2019103135,
title = {Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view},
journal = {Information & Management},
volume = {56},
number = {6},
pages = {103135},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618302854},
author = {Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan},
keywords = {Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China},
abstract = {This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.}
}
@incollection{CYCHOSZ20221,
title = {Chapter One - Using big data from long-form recordings to study development and optimize societal impact},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {1-36},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000434},
author = {Margaret Cychosz and Alejandrina Cristia},
keywords = {Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children},
abstract = {Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.}
}
@article{SILVA2020111,
title = {Ion beam analysis and big data: How data science can support next-generation instrumentation},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {478},
pages = {111-115},
year = {2020},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2030272X},
author = {Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães},
keywords = {Ion beam analysis, Big data, Data quality assurance, Artificial intelligence},
abstract = {With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.}
}
@article{LIN201949,
title = {Strategic orientations, developmental culture, and big data capability},
journal = {Journal of Business Research},
volume = {105},
pages = {49-60},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304333},
author = {Canchu Lin and Anand Kunnathur},
keywords = {Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture},
abstract = {Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.}
}
@article{MA2021107580,
title = {A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107580},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221004848},
author = {Qiuping Ma and Hongyan Li and Anders Thorstenson},
keywords = {Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network},
abstract = {Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.}
}
@article{GARCIAGIL2019135,
title = {Enabling Smart Data: Noise filtering in Big Data classification},
journal = {Information Sciences},
volume = {479},
pages = {135-152},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309460},
author = {Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera},
keywords = {Big Data, Smart Data, Classification, Class noise, Label noise.},
abstract = {In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.}
}
@article{LV2020103,
title = {Analysis of healthcare big data},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {103-110},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304829},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Health care, Privacy security risk, Privacy measures},
abstract = {In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{COZZINI2022133422,
title = {Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family},
journal = {Chemosphere},
volume = {292},
pages = {133422},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133422},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038960},
author = {Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani},
keywords = {Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology},
abstract = {According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.}
}
@article{GILL202051,
title = {Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {1},
pages = {51-59},
year = {2020},
note = {Direct-to-Consumer Testing: The Role of Laboratory Medicine},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300927},
author = {Emily L. Gill and Stephen R. Master},
keywords = {Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization}
}
@article{ARBEX2020102671,
title = {Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data},
journal = {Journal of Transport Geography},
volume = {85},
pages = {102671},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2020.102671},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319300092},
author = {Renato Arbex and Claudio B. Cunha},
keywords = {Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability},
abstract = {Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.}
}
@article{BAIG2019102095,
title = {Big data adoption: State of the art and research challenges},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102095},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319301773},
author = {Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi},
keywords = {Big data adoption, Technology–Organization–Environment, Diffusion of Innovations},
abstract = {Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.}
}
@article{ALIC2019243,
title = {BIGSEA: A Big Data analytics platform for public transportation information},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {243-269},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304448},
author = {Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira},
abstract = {Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).}
}
@article{SUNDARAKANI2021102452,
title = {Big data driven supply chain design and applications for blockchain: An action research using case study approach},
journal = {Omega},
volume = {102},
pages = {102452},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2021.102452},
url = {https://www.sciencedirect.com/science/article/pii/S030504832100061X},
author = {Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran},
keywords = {Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management},
abstract = {Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.}
}
@article{ELIA2020508,
title = {A multi-dimension framework for value creation through big data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {508-522},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118307600},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{WANG202110,
title = {An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting},
journal = {Data Science and Management},
volume = {1},
number = {1},
pages = {10-12},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000011},
author = {Shouyang Wang},
keywords = {Big data, Economic forecasting, Data mining, Spatio-temporal},
abstract = {The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.}
}
@article{VIEIRA2020101985,
title = {On the use of simulation as a Big Data semantic validator for supply chain management},
journal = {Simulation Modelling Practice and Theory},
volume = {98},
pages = {101985},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.101985},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301182},
author = {António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira},
keywords = {Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0},
abstract = {Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.}
}
@article{LOZADA2019e02541,
title = {Big data analytics capability and co-innovation: An empirical study},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02541},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02541},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362012},
author = {Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry},
keywords = {Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation},
abstract = {There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.}
}
@article{GHOLIZADEH2020120640,
title = {A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120640},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120640},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620306879},
author = {Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh},
keywords = {logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint},
abstract = {Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.}
}
@article{JHA2020113382,
title = {A note on big data analytics capability development in supply chain},
journal = {Decision Support Systems},
volume = {138},
pages = {113382},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113382},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301378},
author = {Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai},
keywords = {Big data, Analytics, Capability development, Qualitative study, Supply chain},
abstract = {Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.}
}
@article{KEZUNOVIC2020106788,
title = {Big data analytics for future electricity grids},
journal = {Electric Power Systems Research},
volume = {189},
pages = {106788},
year = {2020},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106788},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620305915},
author = {Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa},
keywords = {Electricity grids, Analytics, Big data, Decision-making},
abstract = {This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.}
}
@incollection{KOLTAY202249,
title = {Chapter 3 - Data quality, the essential “ingredient”},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {49-75},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000047},
author = {Tibor Koltay},
keywords = {Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance},
abstract = {This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.}
}
@article{CUI2020101861,
title = {Manufacturing big data ecosystem: A systematic literature review},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101861},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101861},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300559},
author = {Yesheng Cui and Sami Kara and Ka C. Chan},
keywords = {Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL},
abstract = {Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.}
}
@article{LOPEZMARTINEZ2021263,
title = {A big data-centric architecture metamodel for Industry 4.0},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {263-284},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002156},
author = {Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla},
keywords = {Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel},
abstract = {The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.}
}
@article{NORTHCOTT202096,
title = {Big data and prediction: Four case studies},
journal = {Studies in History and Philosophy of Science Part A},
volume = {81},
pages = {96-104},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368119300652},
author = {Robert Northcott},
keywords = {Big data, Prediction, Case studies, Explanation, Elections, Weather},
abstract = {Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.}
}
@article{TRIPATHI20201245,
title = {Big-data driven approaches in materials science: A survey},
journal = {Materials Today: Proceedings},
volume = {26},
pages = {1245-1249},
year = {2020},
note = {10th International Conference of Materials Processing and Characterization},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.02.249},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320310026},
author = {Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi},
keywords = {Material science, Big data, Machine learning, Data analytics, Predictive Algorithms},
abstract = {The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.}
}
@article{FAHEEM2021100236,
title = {CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100236},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100236},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000364},
author = {Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor},
keywords = {Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network},
abstract = {Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.}
}
@incollection{BACHHETY202145,
title = {2 - Big Data Analytics for healthcare: theory and applications},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {45-67},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000084},
author = {Shivam Bachhety and Shivani Kapania and Rachna Jain},
keywords = {Big Data, healthcare, Big Data Analytics, Hadoop},
abstract = {In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.}
}
@article{MIKALEF2020103361,
title = {The role of information governance in big data analytics driven innovation},
journal = {Information & Management},
volume = {57},
number = {7},
pages = {103361},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103361},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620302998},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS},
abstract = {The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.}
}
@article{ABOELMAGED2020102234,
title = {Influencing models and determinants in big data analytics research: A bibliometric analysis},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102234},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102234},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313366},
author = {Mohamed Aboelmaged and Samar Mouakket},
keywords = {Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks},
abstract = {Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}
@incollection{MORRA2021841,
title = {Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {841-855},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.00111-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084001119},
author = {Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley},
keywords = {Big Data, High performance computing, Education, Modeling, Geodynamics},
abstract = {Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.}
}
@article{OPREA2021106902,
title = {Insights into demand-side management with big data analytics in electricity consumers’ behaviour},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106902},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620307540},
author = {Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu},
keywords = {Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics},
abstract = {The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.}
}
@article{HUANG2021144535,
title = {An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms},
journal = {Science of The Total Environment},
volume = {769},
pages = {144535},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144535},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720380669},
author = {Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan},
keywords = {Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation},
abstract = {An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.}
}
@article{GE2018601,
title = {Big Data for Internet of Things: A Survey},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {601-614},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316953},
author = {Mouzhi Ge and Hind Bangui and Barbora Buhnova},
keywords = {Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities},
abstract = {With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.}
}
@article{RAHUL2020364,
title = {Data Life Cycle Management in Big Data Analytics},
journal = {Procedia Computer Science},
volume = {173},
pages = {364-371},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920315465},
author = {Kumar Rahul and Rohitash Kumar Banyal},
keywords = {Data life cycle, Data creation, Data usability, Healthcare, Big data},
abstract = {Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{RAUT2021102170,
title = {Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {145},
pages = {102170},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102170},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520308139},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu},
keywords = {Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability},
abstract = {The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.}
}
@article{YILDIRIM2021114840,
title = {Big data analytics for default prediction using graph theory},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114840},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114840},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002815},
author = {Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir},
keywords = {Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value},
abstract = {With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.}
}
@article{MATHIS2020582,
title = {Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {34},
number = {3},
pages = {582-585},
year = {2020},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053077019311590},
author = {Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren}
}
@article{RAKIPI2021100357,
title = {Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {42},
pages = {100357},
year = {2021},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2020.100357},
url = {https://www.sciencedirect.com/science/article/pii/S1061951820300586},
author = {Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza},
keywords = {Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit},
abstract = {In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.}
}
@article{ZHANG2020483,
title = {Linking big data analytical intelligence to customer relationship management performance},
journal = {Industrial Marketing Management},
volume = {91},
pages = {483-494},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308762},
author = {Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han},
keywords = {Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance},
abstract = {This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@article{HAUSLADEN2020101721,
title = {Towards a maturity model for big data analytics in airline network planning},
journal = {Journal of Air Transport Management},
volume = {82},
pages = {101721},
year = {2020},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2019.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0969699718304988},
author = {Iris Hausladen and Maximilian Schosser},
keywords = {Maturity model, Network planning, Big data analytics, Airlines, Case study},
abstract = {The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.}
}
@article{SHAH2020106970,
title = {Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106970},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106970},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300363},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning},
abstract = {As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.}
}
@article{ZHANG2019103231,
title = {Orchestrating big data analytics capability for sustainability: A study of air pollution management in China},
journal = {Information & Management},
pages = {103231},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619302010},
author = {Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu},
keywords = {Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration},
abstract = {Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.}
}
@article{LIU2020123646,
title = {Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123646},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123646},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033691X},
author = {Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He},
keywords = {Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination},
abstract = {Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.}
}
@article{REN2021128154,
title = {Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province},
journal = {Journal of Cleaner Production},
volume = {316},
pages = {128154},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128154},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621023726},
author = {Dongfang Ren and Xiaopeng Guo and Cunbin Li},
keywords = {Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power},
abstract = {With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.}
}
@article{MAJEED2021102026,
title = {A big data-driven framework for sustainable and smart additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102026},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302374},
author = {Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin},
keywords = {Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization},
abstract = {From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.}
}
@article{XIANG2020106538,
title = {Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence},
journal = {Computers & Industrial Engineering},
volume = {145},
pages = {106538},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220302722},
author = {Zehua Xiang and Minli Xu},
keywords = {Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence},
abstract = {In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.}
}
@incollection{CHANG202221,
title = {Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas F. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {21-66},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000076},
author = {Victor Chang and Mohamed Aleem Ali and Alamgir Hossain},
keywords = {COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19},
abstract = {This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.}
}
@article{RUSSELL2022108709,
title = {Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring},
journal = {Mechanical Systems and Signal Processing},
volume = {168},
pages = {108709},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021010293},
author = {Matthew Russell and Peng Wang},
keywords = {Physics-informed deep learning, Prognostics and health management, Data compression, Big data},
abstract = {The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@article{SEDLMAYR202081,
title = {Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen},
journal = {Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen},
volume = {158-159},
pages = {81-91},
year = {2020},
issn = {1865-9217},
doi = {https://doi.org/10.1016/j.zefq.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1865921720301744},
author = {Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr},
keywords = {Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases},
abstract = {Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.}
}
@article{BRESCIANI2021102347,
title = {Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda},
journal = {International Journal of Information Management},
volume = {60},
pages = {102347},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000402},
author = {Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris},
keywords = {Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review},
abstract = {This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.}
}
@article{YADEGARIDEHKORDI2020100921,
title = {The impact of big data on firm performance in hotel industry},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100921},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2019.100921},
url = {https://www.sciencedirect.com/science/article/pii/S1567422319300985},
author = {Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}},
keywords = {Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling},
abstract = {Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.}
}
@article{MARIANI2020338,
title = {Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies},
journal = {Journal of Business Research},
volume = {121},
pages = {338-352},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320305956},
author = {Marcello M. Mariani and Samuel {Fosso Wamba}},
keywords = {Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data},
abstract = {The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.}
}
@article{JI2021108267,
title = {Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach},
journal = {Building and Environment},
volume = {205},
pages = {108267},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108267},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321006673},
author = {Sukwon Ji and Bumho Lee and Mun Yong Yi},
keywords = {Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network},
abstract = {Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.}
}
@article{LIOUTAS2019100297,
title = {Key questions on the use of big data in farming: An activity theory approach},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100297},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302197},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}},
keywords = {Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory},
abstract = {Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.}
}
@article{LI2019393,
title = {Functional Neuroimaging in the New Era of Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {393-401},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301603},
author = {Xiang Li and Ning Guo and Quanzheng Li},
keywords = {Big data, Neuroimaging, Machine learning, Health informatics, fMRI},
abstract = {The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.}
}
@article{VIEIRA2020132,
title = {Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {132-139},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305825},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Data issues, Industry 4.0},
abstract = {Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.}
}
@article{LUNDBERG2021100244,
title = {Editorial to the Special Issue on Big Data in Industrial and Commercial Applications},
journal = {Big Data Research},
volume = {26},
pages = {100244},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100244},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000617},
author = {Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab}
}
@article{MCNUTT2019326,
title = {Use of Big Data for Quality Assurance in Radiation Therapy},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {326-332},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300384},
author = {Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright},
abstract = {The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.}
}
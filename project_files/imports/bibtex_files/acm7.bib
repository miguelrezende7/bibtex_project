@inproceedings{10.1145/3486640.3491391,
author = {Devarakonda, Ranjeet and Guntupally, Kavya and Thornton, Michele and Wei, Yaxing and Singh, Debjani and Lunga, Dalton},
title = {FAIR Interfaces for Geospatial Scientific Data Searches},
year = {2021},
isbn = {9781450391238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486640.3491391},
doi = {10.1145/3486640.3491391},
abstract = {Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
pages = {1–4},
numpages = {4},
keywords = {ARM Data Center, Geospatial search interfaces, FAIR data principle for scientific data, ORNL DAAC},
location = {Beijing, China},
series = {GeoSearch'21}
}

@article{10.1145/3340286,
author = {Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng},
title = {DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics Tools},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3340286},
doi = {10.1145/3340286},
abstract = {The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {98},
numpages = {30},
keywords = {third-generation sequencing (TGS), tools, technology, software, history, bioinformatics, data protocols, DNA sequencing, computational biology}
}

@inproceedings{10.1145/3289402.3289526,
author = {Dahbi, Kawtar Younsi and Lamharhar, Hind and Chiadmi, Dalila},
title = {Exploring Dimensions Influencing the Usage of Open Government Data Portals},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289526},
doi = {10.1145/3289402.3289526},
abstract = {Governments are considered as one of the major producers of data. Opening up and publishing this Big Government Data in national portals have significant impact on fostering innovation, improving transparency, public accountability and collaboration. Thus, the expected benefits are hindered by several factors that influence the usage of Open Government Data portals, exploring and investigating these factors is the first step to propose an evaluation approach for OGD portals and promote their usage. In this work, we identified a set of evaluation dimensions that affect OGD portal's usage and fulfillment of users' needs and requirements. According to the identified dimensions, we propose an evaluation of two national OGD portals},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {26},
numpages = {6},
keywords = {Open Government Data portals, usage, Evaluation, Open Government Data},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3428502.3428576,
author = {Hanbal, Rajesh Dinesh and Prakash, Amit and Srinivasan, Janaki},
title = {Who Drives Data in Data-Driven Governance? The Politics of Data Production in India's Livelihood Program},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428576},
doi = {10.1145/3428502.3428576},
abstract = {The increased digitisation of government information systems, as well as emerging data analytics and visualization techniques, have led lately to a surge in interest in the role of data in governance and development. The latest buzzwords in governance now include data-driven governance, data-for-development, evidence-based policy-making, and open government data. However, not much attention has been paid to understand the process of the production of data in government information systems. Our findings are based on six months of an ethnographic study of India's livelihood program- Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA) in a rural district of Karnataka. We argue that the practice of data production is carefully managed and controlled by local power elites providing an illusion of transparency in a digital information system. Understanding and recognizing the political nature of data production can help in better evaluation of development interventions, policy-making as well as in the design of more just information systems.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {485–493},
numpages = {9},
keywords = {data justice, Open government data, Data production},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1109/JCDL.2019.00088,
author = {Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu},
title = {Practice of Constructing Name Authority Database Based on Multi-Source Data Integration},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00088},
doi = {10.1109/JCDL.2019.00088},
abstract = {Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {398–399},
numpages = {2},
keywords = {name disambiguation, NSTL, name authority, multi-source},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inbook{10.1145/3378393.3402248,
author = {Ramanujapuram, Arun and Malemarpuram, Charan Kumar},
title = {Enabling Sustainable Behaviors of Data Recording and Use in Low-Resource Supply Chains},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378393.3402248},
abstract = {Public services, such as public health supply chains, in low- and middle-income countries can be characterized as low-resource environments, where both infrastructure and human capacity are limited. There is no strong culture of data recording or use, with ad hoc reporting practices, poor planning and lack of coordination. All these lead to poor supply chain performance, thereby restricting access to medicines, and eventually resulting in poorer health and mortality.We describe the ground-up design of Logistimo SCM, a supply chain management software, offered as a service, that has enabled a transformative change in public health supply chains, leading to improved performance. Our approach is rooted in bottom-up empowerment of the human value chain, based on the principle that higher self-efficacy amongst health workers and managers can lead to sustained changes in data recording and use behaviors. This is achieved through a service that optimizes data collection effort, maximizes supervisory bandwidth, promotes proactive and collaborative operations, and enables frictionless performance recognition. We describe the guiding principles of inclusive software service design and four mechanisms that enable the appropriate conditions for stimulating a behavior of data recording and use. We demonstrate their effectiveness in achieving good supply chain performance through case studies in India and Africa. The principles and methods discussed here are generic and can be applied to any low-resource environment.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {65–75},
numpages = {11}
}

@inproceedings{10.1145/1963192.1963325,
author = {Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc},
title = {The 1st Temporal Web Analytics Workshop (TWAW)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963325},
doi = {10.1145/1963192.1963325},
abstract = {The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {307–308},
numpages = {2},
keywords = {temporal web analytics, distributed data analytics, web scale data analytics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/3265757.3265766,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Developing a Theoretically Founded Data Literacy Competency Model},
year = {2018},
isbn = {9781450365888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265757.3265766},
doi = {10.1145/3265757.3265766},
abstract = {Today, data is everywhere: Our digitalized world depends on enormous amounts of data that are captured by and about everyone and considered a valuable resource. Not only in everyday life, but also in science, the relevance of data has clearly increased in recent years: Nowadays, data-driven research is often considered a new research paradigm. Thus, there is general agreement that basic competencies regarding gathering, storing, processing and visualizing data, often summarized under the term data literacy, are necessary for every scientist today. Moreover, data literacy is generally important for everyone, as it is essential for understanding how the modern world works. Yet, at the moment data literacy is hardly considered in CS teaching at schools. To allow deeper insight into this field and to structure related competencies, in this work we develop a competency model of data literacy by theoretically deriving central content and process areas of data literacy from existing empirical work, keeping a school education perspective in mind. The resulting competency model is contrasted to other approaches describing data literacy competencies from different perspectives. The practical value of this work is emphasized by giving insight into an exemplary lesson sequence fostering data literacy competencies.},
booktitle = {Proceedings of the 13th Workshop in Primary and Secondary Computing Education},
articleno = {9},
numpages = {10},
keywords = {data, data management, competency model, data literacy, CS education, data science},
location = {Potsdam, Germany},
series = {WiPSCE '18}
}

@inproceedings{10.1145/3176349.3176901,
author = {Bogers, Toine and G\"{a}de, Maria and Freund, Luanne and Hall, Mark and Koolen, Marijn and Petras, Vivien and Skov, Mette},
title = {Workshop on Barriers to Interactive IR Resources Re-Use},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176901},
doi = {10.1145/3176349.3176901},
abstract = {The goal of this workshop is to serve as a starting point for a community-driven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for IIR experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. However, we believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community»s resources are organized, the lack of awareness of their existence, documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. We believe that an online platform dedicated to the collection and organization of IIR resources could be a promising way of overcoming these barriers. The workshop therefore aims to serve both as a brainstorming opportunity about the shape this iRepository should take, as well as a way of building support in the community for its implementation.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {382–385},
numpages = {4},
keywords = {repository, intertactive information retrieval, evaluation},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@article{10.1145/3466160,
author = {Sambasivan, Nithya},
title = {Seeing like a Dataset from the Global South},
year = {2021},
issue_date = {July - August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1072-5520},
url = {https://doi.org/10.1145/3466160},
doi = {10.1145/3466160},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {jun},
pages = {76–78},
numpages = {3}
}

@article{10.1145/2070736.2070750,
author = {Badia, Antonio and Lemire, Daniel},
title = {A Call to Arms: Revisiting Database Design},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2070736.2070750},
doi = {10.1145/2070736.2070750},
journal = {SIGMOD Rec.},
month = {nov},
pages = {61–69},
numpages = {9}
}

@article{10.1145/3516515,
author = {Sambasivan, Nithya},
title = {All Equation, No Human: The Myopia of AI Models},
year = {2022},
issue_date = {March - April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3516515},
doi = {10.1145/3516515},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {feb},
pages = {78–80},
numpages = {3}
}

@inproceedings{10.1145/3299819.3299850,
author = {Li, Ying and Zhang, AiMin and Zhang, Xinman and Wu, Zhihui},
title = {A Data Lake Architecture for Monitoring and Diagnosis System of Power Grid},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299850},
doi = {10.1145/3299819.3299850},
abstract = {In this paper, a data lake architecture is proposed for a class of monitoring and diagnostic systems applied to power grid. The differences between data lake and data warehouse is studied to make an informed decision on how to manage a huge amount of data. To adapt to the characteristics and performances of historical data and real-time data of power grid equipment, a monitoring and diagnosis system based on data lake storage architecture is designed. The application of the framework indicates the applicability and effectiveness of data lake architecture.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {192–198},
numpages = {7},
keywords = {Data Pond, Monitoring And Diagnostic, Data Lake, Power Grid},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@inproceedings{10.1145/2908131.2908172,
author = {Weller, Katrin and Kinder-Kurlanda, Katharina E.},
title = {A Manifesto for Data Sharing in Social Media Research},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908172},
doi = {10.1145/2908131.2908172},
abstract = {More and more researchers want to share research data collected from social media to allow for reproducibility and comparability of results. With this paper we want to encourage them to pursue this aim -- despite initial obstacles that they may face. Sharing can occur in various, more or less formal ways. We provide background information that allows researchers to make a decision about whether, how and where to share depending on their specific situation (data, platform, targeted user group, research topic etc.). Ethical, legal and methodological considerations are important for making this decision. Based on these three dimensions we develop a framework for social media sharing that can act as a first set of guidelines to help social media researchers make practical decisions for their own projects. In the long run, different stakeholders should join forces to enable better practices for data sharing for social media researchers. This paper is intended as our call to action for the broader research community to advance current practices of data sharing in the future.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {166–172},
numpages = {7},
keywords = {data protection, privacy, social media, archiving, data archives, methodology, data sharing, legal issues, reproducibility},
location = {Hannover, Germany},
series = {WebSci '16}
}

@article{10.1145/3154525,
author = {Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim},
title = {Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154525},
doi = {10.1145/3154525},
abstract = {Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {29},
numpages = {53},
keywords = {large-scale data, wireless sensor network (WSN), discovery, indexing, Internet of things (IoT), ranking}
}

@inproceedings{10.1145/3297280.3297354,
author = {Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung},
title = {Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297354},
doi = {10.1145/3297280.3297354},
abstract = {Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {762–770},
numpages = {9},
keywords = {ACM proceedings, text tagging},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3333165.3333168,
author = {Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss},
title = {Passage Challenges from Data-Intensive System to Knowledge-Intensive System Related to Process Mining Field},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333168},
doi = {10.1145/3333165.3333168},
abstract = {Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {3},
numpages = {6},
keywords = {Data-intensive, Process Mining, Knowledge-intensive, Process mining challenges, Business Process Management, Adaptive Case Management},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@inproceedings{10.1145/3267305.3274762,
author = {Budde, Matthias and Riedel, Till},
title = {Challenges in Capturing and Analyzing High Resolution Urban Air Quality Data},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3274762},
doi = {10.1145/3267305.3274762},
abstract = {Classic measurement grids with their static and expensive infrastructure are unfit to realize modern air quality monitoring needs, such as source appointment, pollution tracking or the assessment of personal exposure. Fine grained air quality assessment (both in time and space) is the future. Different approaches, ranging from measurement with low-cost sensors over advanced modeling and remote sensing to combinations of these have been proposed. This position paper summarizes our previous contributions in this field and lists what we see as open challenges for future research.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {1162–1165},
numpages = {4},
keywords = {challenges, Air quality, urban air, PM2.5, particulate matter, PM10, sensing},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@inproceedings{10.1145/2957276.2957280,
author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
title = {Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957280},
doi = {10.1145/2957276.2957280},
abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {3–8},
numpages = {6},
keywords = {supervised learning, machine learning, unsupervised learning, grounded theory, axial coding, coding families},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/3495018.3495486,
author = {Liu, Ying and Yu, Wei and Xiao, Suhong},
title = {Digital Protection of Nanfeng Nuo Mask Based on AR Technology},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495486},
doi = {10.1145/3495018.3495486},
abstract = {The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the "live inheritance protection mode", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1792–1796},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/3185504,
author = {Liu, Jinwei and Shen, Haiying and Narman, Husnu S. and Chung, Wingyan and Lin, Zongfang},
title = {A Survey of Mobile Crowdsensing Techniques: A Critical Component for The Internet of Things},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3185504},
doi = {10.1145/3185504},
abstract = {Mobile crowdsensing serves as a critical building block for emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy, and storage) and may sacrifice the Quality-of-Service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing redundant data, data size and load can be significantly reduced, thereby reducing resource cost and facilitating the timely delivery of unique, probably critical information and enhancing QoS. This article presents a survey of existing works on mobile crowdsensing strategies with an emphasis on reducing resource cost and achieving high QoS. We start by introducing the motivation for this survey and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss future research directions for mobile crowdsensing for IoT. The survey addresses a broad range of techniques, methods, models, systems, and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in prior works, but also to discuss their applicability toward the IoT and provide guidance on future research directions for mobile crowdsensing.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jun},
articleno = {18},
numpages = {26},
keywords = {Internet of Things, Mobile crowdsensing, cost-effectiveness, quality of service, redundancy elimination}
}

@inproceedings{10.1145/3306446.3340821,
author = {Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup},
title = {Bringing Open Data into Danish Schools and Its Potential Impact on School Pupils},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340821},
doi = {10.1145/3306446.3340821},
abstract = {Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {9},
numpages = {10},
keywords = {educational themes, impact, educational resource, school pupils, open data},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/2666158.2666183,
author = {Nakuc\c{c}i, Emona and Theodorou, Vasileios and Jovanovic, Petar and Abell\'{o}, Alberto},
title = {Bijoux: Data Generator for Evaluating ETL Process Quality},
year = {2014},
isbn = {9781450309998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666158.2666183},
doi = {10.1145/2666158.2666183},
abstract = {Obtaining the right set of data for evaluating the fulfillment of different quality standards in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. Additionally, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over data, and automatically generates testing datasets. At the same time, it considers different dataset and transformation characteristics (e.g., size, distribution, selectivity, etc.) in order to cover a variety of test scenarios. We report our experimental findings showing the effectiveness and scalability of our approach.},
booktitle = {Proceedings of the 17th International Workshop on Data Warehousing and OLAP},
pages = {23–32},
numpages = {10},
keywords = {process quality, ETL, data generator},
location = {Shanghai, China},
series = {DOLAP '14}
}

@article{10.14778/2536274.2536300,
author = {Okcan, Alper and Riedewald, Mirek and Panda, Biswanath and Fink, Daniel},
title = {Scolopax: Exploratory Analysis of Scientific Data},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536300},
doi = {10.14778/2536274.2536300},
abstract = {The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1298–1301},
numpages = {4}
}

@article{10.1145/3360000,
author = {Andersen, Kim Normann and Lee, Jungwoo and Henriksen, Helle Zinner},
title = {Digital Sclerosis? Wind of Change for Government and the Employees},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-199X},
url = {https://doi.org/10.1145/3360000},
doi = {10.1145/3360000},
abstract = {Contrasting the political ambitions on the next generation of government, the uptake of technology can lead to digital sclerosis characterized by stiffening of the governmental processes, failure to respond to changes in demand, and lowering innovation feedback from workers. In this conceptual article, we outline three early warnings of digital sclerosis: decreased bargaining and discretion power of governmental workers, enhanced agility and ability at shifting and extended proximities, and panopticonization. To respond proactively and take preventive care initiatives, policy makers and systems developers need to be sensitized about the digital sclerosis, prepare the technology, and design intelligent augmentations in a flexible and agile approach.},
journal = {Digit. Gov.: Res. Pract.},
month = {feb},
articleno = {9},
numpages = {14},
keywords = {changing nature of work, workplace, future work, digital sclerosis, public sector, work, digitalization, e-Government}
}

@inproceedings{10.1145/3421766.3421800,
author = {Wang, Deli},
title = {Research on Bank Marketing Behavior Based on Machine Learning},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421800},
doi = {10.1145/3421766.3421800},
abstract = {At present, under the background that data mining technology is becoming more mature and widely used in various fields, and due to the advent of the customer-oriented era and increased competition from banks, data mining technology is being widely used in the field of banking and finance to determine the target customer group And promote bank sales. Therefore, based on the Bank Marketing data in the UCI Machine Learning Repository database, this article uses the C5.0 algorithm to classify customers on the clementine experimental platform, and proposes corresponding suggestions for bank marketing based on the classification results.This article first explores and understands the Bank Marketing data set, and describes the distribution of the customer background in the data set. The quality of the data set was further explored, and the outliers and outliers were corrected by replacing them with normal data that were closest to the outliers or extreme values.This paper further selects the optimal feature variable. First, use the Filter node to filter the unimportant variables of the classification, and further select one of the more relevant variables to reduce the redundancy of the variables. The final variables are: previous, age, duration, outcome, contact, housing, job, loan, marital, education.Secondly, this paper uses sampling nodes to perform undersampling to balance the data set. On this basis, the C5.0 algorithm is used to establish a classification model and optimize parameters, and finally obtain eight classification rules. Based on this, suggestions are provided for target group determination.Finally, this article introduces the remaining four classification algorithms: C&amp;T, QUEST, CHAID, Neural Networks, and compares the C5.0 algorithm with the four classification algorithms based on the balanced data set. It is concluded that several algorithms have certain differences and the overall prediction accuracy is good.This article combines data mining theory with practical problems of banking business, and establishes a bank target customer classification model based on C5.0 algorithm. The obtained classification rules can effectively help banks to divide customer groups and take targeted measures to improve marketing efficiency.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {150–154},
numpages = {5},
keywords = {Customer segmentation, Bank Direct Sales Project, Classification algorithm, C5.0 algorithm},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@article{10.1145/3517189,
author = {Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas},
title = {Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3517189},
doi = {10.1145/3517189},
abstract = {Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross-validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, Digital Twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use-cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of Artificial Intelligence (AI) in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {feb},
keywords = {Internet of Things (IoT), Industrial Control Systems (ICSs), Blockchain, Industry 4.0, Artificial Intelligence (AI), Cyber-Physical Systems (CPSs), Digital Twins (DTs)}
}

@article{10.1145/3423923,
author = {Tentori, Monica and Ziviani, Artur and Muchaluat-Saade, D\'{e}bora C. and Favela, Jesus},
title = {Digital Healthcare in Latin America: The Case of Brazil and Mexico},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3423923},
doi = {10.1145/3423923},
journal = {Commun. ACM},
month = {oct},
pages = {72–77},
numpages = {6}
}

@inproceedings{10.5555/3370272.3370329,
author = {Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa},
title = {Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren't Working},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {384–385},
numpages = {2},
keywords = {data science adoption, legal, challenges, business practices, organisational},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3405962.3405983,
author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
title = {The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405983},
doi = {10.1145/3405962.3405983},
abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {21–32},
numpages = {12},
keywords = {ontology, datasets, aviation, queries, services},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3197026.3200209,
author = {Klein, Martin and Xie, Zhiwu and Fox, Edward A.},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3200209},
doi = {10.1145/3197026.3200209},
abstract = {The 2018 edition of the Workshop on Web Archiving and Digital Libraries (WADL) will explore the integration of Web archiving and digital libraries. The workshop aims at addressing aspects covering the entire life cycle of digital resources and will also explore areas such as community building and ethical questions around web archiving.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {425–426},
numpages = {2},
keywords = {web archiving, digital preservation, community building},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3482632.3487461,
author = {Tang, Xinzhong and Zhuang, Bing and Yao, Ying and Dong, Xuesong},
title = {Research on High-Reliability Intelligent-Sensing Health Service Support Platform and Key Technologies Based on Biometrics and Blockchain Security Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487461},
doi = {10.1145/3482632.3487461},
abstract = {A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2514–2518},
numpages = {5},
keywords = {blockchain, deep learning, High-reliability, health services, biological characteristics},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3482632.3484010,
author = {Yang, Rui},
title = {Statistics and Mining Analysis of Lightning Monitoring Data in Power Grid Based on Classical Metrology Model},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484010},
doi = {10.1145/3482632.3484010},
abstract = {Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1649–1653},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3401025.3406443,
author = {Baban, Philsy},
title = {Pre-Processing and Data Validation in IoT Data Streams},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3406443},
doi = {10.1145/3401025.3406443},
abstract = {In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {226–229},
numpages = {4},
keywords = {data validation, stream processing, data pre-processing, resiliency},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3341162.3347758,
author = {Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan},
title = {LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3347758},
doi = {10.1145/3341162.3347758},
abstract = {Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {878–881},
numpages = {4},
keywords = {mobile devices, panel technique, in situ, human sensing, longitudinal studies, human subject studies},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3047273.3047327,
author = {Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega},
title = {An Ontology for Open Government Data Business Model},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047327},
doi = {10.1145/3047273.3047327},
abstract = {Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {195–203},
numpages = {9},
keywords = {and business model ontology, formal conceptualization, open data-driven organization, open data business model, e-Business ontology, Open government data, e-Commerce ontology},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3078564.3078572,
author = {Song, Zekun and Zhang, Lvyang and Liu, Tao and Chen, Ying},
title = {Ranking Learning Algorithm of Information Retrieval Based on WeChat Public Numbers},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078572},
doi = {10.1145/3078564.3078572},
abstract = {On the basis of obtaining the data of mass WeChat public1, in order to improve the operational efficiency and quality of WeChat public number. On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established. A sort learning algorithm based on model retrieval is proposed. Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort. The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality WeChat number of public articles.},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {4},
numpages = {5},
keywords = {WeChatpublic number, Recommendation system, Meta data model, Rank learning algorithm},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3132218.3132241,
author = {Beek, Wouter and Fern\'{a}ndez, Javier D. and Verborgh, Ruben},
title = {LOD-a-Lot: A Single-File Enabler for Data Science},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132241},
doi = {10.1145/3132218.3132241},
abstract = {Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {181–184},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@article{10.1145/2826686.2826692,
author = {Resch, Bernd and Blaschke, Thomas},
title = {Fusing Human and Technical Sensor Data: Concepts and Challenges},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/2826686.2826692},
doi = {10.1145/2826686.2826692},
abstract = {As geo-sensor webs have not grown as quickly as expected, new, alternative data sources have to be found for near real-time analysis in areas like emergency management, environmental monitoring, public health, or urban planning. This paper assesses the ability of human sensors, i.e., user-generated observations in a wide range of social networks, the mobile phone network, or micro-blogs, to complement geo-sensor networks. We clearly delineate the concepts of People as Sensors, Collective Sensing and Citizen Science. Furthermore, we point out current challenges in fusing data from technical and human sensors, and sketch future research areas in this field.},
journal = {SIGSPATIAL Special},
month = {sep},
pages = {29–35},
numpages = {7}
}

@inproceedings{10.1145/2910896.2926734,
author = {Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926734},
doi = {10.1145/2910896.2926734},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {299–300},
numpages = {2},
keywords = {bibliometrics, digital libraries, information retrieval, natural language processing, text mining},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@article{10.1145/3376915,
author = {De Aguiar, Erikson J\'{u}lio and Fai\c{c}al, Bruno S. and Krishnamachari, Bhaskar and Ueyama, J\'{o}},
title = {A Survey of Blockchain-Based Strategies for Healthcare},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3376915},
doi = {10.1145/3376915},
abstract = {Blockchain technology has been gaining visibility owing to its ability to enhance the security, reliability, and robustness of distributed systems. Several areas have benefited from research based on this technology, such as finance, remote sensing, data analysis, and healthcare. Data immutability, privacy, transparency, decentralization, and distributed ledgers are the main features that make blockchain an attractive technology. However, healthcare records that contain confidential patient data make this system very complicated because there is a risk of a privacy breach. This study aims to address research into the applications of the blockchain healthcare area. It sets out by discussing the management of medical information, as well as the sharing of medical records, image sharing, and log management. We also discuss papers that intersect with other areas, such as the Internet of Things, the management of information, tracking of drugs along their supply chain, and aspects of security and privacy. As we are aware that there are other surveys of blockchain in healthcare, we analyze and compare both the positive and negative aspects of their papers. Finally, we seek to examine the concepts of blockchain in the medical area, by assessing their benefits and drawbacks and thus giving guidance to other researchers in the area. Additionally, we summarize the methods used in healthcare per application area and show their pros and cons.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {27},
numpages = {27},
keywords = {distributed ledger technology, healthcare, Distributed systems, blockchain, medical, survey}
}

@inproceedings{10.1145/3236024.3236056,
author = {Wang, Ying and Wen, Ming and Liu, Zhenwei and Wu, Rongxin and Wang, Rui and Yang, Bo and Yu, Hai and Zhu, Zhiliang and Cheung, Shing-Chi},
title = {Do the Dependency Conflicts in My Project Matter?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236056},
doi = {10.1145/3236024.3236056},
abstract = {Intensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {319–330},
numpages = {12},
keywords = {Empirical study, static analysis, third party library},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inbook{10.1145/3310205.3310206,
title = {Preface},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310206},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1007/s00778-017-0486-1,
author = {Herschel, Melanie and Diestelk\"{a}mper, Ralf and Ben Lahmar, Houssem},
title = {A Survey on Provenance: What for? What Form? What From?},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0486-1},
doi = {10.1007/s00778-017-0486-1},
abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.},
journal = {The VLDB Journal},
month = {dec},
pages = {881–906},
numpages = {26},
keywords = {Provenance requirements, Provenance types, Provenance capture, Provenance applications, Survey, Workflow provenance, Data provenance}
}

@inproceedings{10.1145/3373477.3373499,
author = {Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan},
title = {Real-Time Dynamic Data Desensitization Method Based on Data Stream},
year = {2019},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373499},
doi = {10.1145/3373477.3373499},
abstract = {With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.},
booktitle = {Proceedings of the International Conference on Advanced Information Science and System},
articleno = {22},
numpages = {6},
keywords = {data desensitization, stream data, dynamic desensitization},
location = {Singapore, Singapore},
series = {AISS '19}
}

@inproceedings{10.1145/3339252.3342112,
author = {Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald},
title = {A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence Sources},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3342112},
doi = {10.1145/3339252.3342112},
abstract = {Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {83},
numpages = {10},
keywords = {trust indicators, Cooperative and collaborative cybersecurity, cyber threat intelligence source evaluation, quality parameters, cyber threat information sharing},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@article{10.1145/3448888,
author = {Koesten, Laura and Simperl, Elena},
title = {UX of Data: Making Data Available Doesn't Make It Usable},
year = {2021},
issue_date = {March - April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3448888},
doi = {10.1145/3448888},
abstract = {This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors},
journal = {Interactions},
month = {mar},
pages = {97–99},
numpages = {3}
}

@article{10.1007/s00778-019-00588-3,
author = {Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang},
title = {Making Data Visualization More Efficient and Effective: A Survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00588-3},
doi = {10.1007/s00778-019-00588-3},
abstract = {Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.},
journal = {The VLDB Journal},
month = {jan},
pages = {93–117},
numpages = {25},
keywords = {Data visualization, Efficient data visualization, Visualization languages, Data visualization recommendation}
}

@inproceedings{10.1145/2987491.2987521,
author = {de Jager, Tiaan and Brown, Irwin},
title = {A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals},
year = {2016},
isbn = {9781450348058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987491.2987521},
doi = {10.1145/2987491.2987521},
abstract = {Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
articleno = {14},
numpages = {10},
keywords = {Typology, Business Intelligence, IT Skills, IS Profession, Analytics},
location = {Johannesburg, South Africa},
series = {SAICSIT '16}
}

@inproceedings{10.5555/2814058.3252433,
author = {Siqueira, Sean W. M. and Carvalho, Sergio T.},
title = {Session Details: Main Track - Management, Governance, and Government},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@article{10.1145/3292384.3292389,
author = {Al-Jaroodi, Jameela and Mohamed, Nader and Jawhar, Imad},
title = {A Service-Oriented Middleware Framework for Manufacturing Industry 4.0},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
url = {https://doi.org/10.1145/3292384.3292389},
doi = {10.1145/3292384.3292389},
abstract = {The advantages of the Internet of things (IoT) initiated the vision of Industry 4.0 in Europe and smart manufacturing in USA. Both visions aim to implement the smart factory to achieve similar objectives by utilizing new technologies. These technologies include cloud computing, fog computing, cyber-physical systems (CPS), and data analytics. Together they help automate and autonomize the manufacturing processes and controls to optimize the productivity, reliability, quality, cost-effeteness, and safety of these processes. While both visions are promising, developing and operating Industry 4.0 applications are extremely challenging. This is due to the complexity of the manufacturing processes as well as their management, controls, and integration dynamics. This paper introduces Man4Ware, a service-oriented middleware for Industry 4.0. Man4Ware can help facilitate the development and operations of cloud and fog-integrated smart manufacturing applications. Man4Ware offers many advantages through service level interfaces to enable easy utilization of new technologies and integration of different services to relax many of the challenges facing the development and operations of such applications1.},
journal = {SIGBED Rev.},
month = {nov},
pages = {29–36},
numpages = {8},
keywords = {fog computing, cloud computing, cyber-physical systems, smart manufacturing, IoT, industry 4.0, middleware}
}

@article{10.1145/1978542.1978562,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
title = {An Overview of Business Intelligence Technology},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978562},
doi = {10.1145/1978542.1978562},
abstract = {BI technologies are essential to running today's businesses and this technology is going through sea changes.},
journal = {Commun. ACM},
month = {aug},
pages = {88–98},
numpages = {11}
}

@inproceedings{10.1145/3384544.3384596,
author = {Valachamy, Mageshwari and Sahibuddin, Shamsul and Ahmad, Noor Azurati and Bakar, Nur Azaliah Abu},
title = {Geospatial Data Sharing: Preliminary Studies on Issues and Challenges in Natural Disaster Management},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384596},
doi = {10.1145/3384544.3384596},
abstract = {The rapid development of information technology has led to the demand for the latest, precise and easy to understand data. Data especially geospatial data is becoming increasingly crucial in all types of planning and decision making. Geospatial data sharing can be categorized into different disciplines such as public safety, disaster management, transportation, traffic control, tracking, health, environment, natural resources, mining, agriculture, utilities and many more. Whether as a way of distribution or retrieval of data, geospatial data has become an essential component of government GIS operations. Despite the prominence of this activity and its centrality to the day-to-day function of many government systems, the geospatial data sharing is still given less attention in the field of natural disaster management. Preliminary information is gathered from Literature Reviews (LR) and unstructured interviews with experts to seek information in depth. Thirteen (13) issues and challenges of geospatial data sharing in Malaysia Public Sector (MPS) for natural disaster management have been identified.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {51–56},
numpages = {6},
keywords = {Spatial Data Infrastructure, Spatial data, Issues and Challenges, Sharing, Geospatial data},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/2935752,
author = {Morstatter, Fred and Liu, Huan},
title = {Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935752},
doi = {10.1145/2935752},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {15},
numpages = {4},
keywords = {crowdsourcing, Artificial intelligence, data mining, automation, evaluation}
}

@inproceedings{10.1145/3384772.3385138,
author = {H. Gyldenkaerne, Christopher and From, Gustav and M\o{}nsted, Troels and Simonsen, Jesper},
title = {PD and The Challenge of AI in Health-Care},
year = {2020},
isbn = {9781450376068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384772.3385138},
doi = {10.1145/3384772.3385138},
abstract = {In its promise to contribute to considerable cost savings and improved patient care through efficient analysis of the tremendous amount of data stored in electronic health records (EHR), there is currently a strong push for the proliferation of artificial intelligence (AI) in health-care. We identify, through a study of AI being used to predict patient no-show’s, that for the AI to gain full potential there lies a need to balance the introduction of AI with a proper focus on the patients and the clinicians’ interests. We call for a Participatory Design (PD) approach to understand and reconfigure the socio-technical setup in health-care, especially where AI is being used on EHR data that are manually being submitted by health-care personnel.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2},
pages = {26–29},
numpages = {4},
keywords = {Artificial Intelligence, Primary- and Secondary Use data, precision medicine, Electronic Health Record data, Participatory Design},
location = {Manizales, Colombia},
series = {PDC '20}
}

@inproceedings{10.1145/3297280.3297477,
author = {Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson},
title = {Towards a Seamless Coordination of Cloud and Fog: Illustration through the Internet-of-Things},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297477},
doi = {10.1145/3297280.3297477},
abstract = {With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes "anything-as-a-service" and the latter promotes "process data next to where it is located". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective "duties". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2008–2015},
numpages = {8},
keywords = {internet-of-things, coordination, cloud, healthcare, fog},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3209581,
author = {Baeza-Yates, Ricardo},
title = {Bias on the Web},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3209581},
doi = {10.1145/3209581},
abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
journal = {Commun. ACM},
month = {may},
pages = {54–61},
numpages = {8}
}

@inbook{10.1145/3310205.3310215,
title = {References},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310215},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1007/s00778-017-0477-2,
author = {Ali, Syed Muhammad and Wrembel, Robert},
title = {From Conceptual Design to Performance Optimization of ETL Workflows: Current State of Research and Open Problems},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0477-2},
doi = {10.1007/s00778-017-0477-2},
abstract = {In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.},
journal = {The VLDB Journal},
month = {dec},
pages = {777–801},
numpages = {25},
keywords = {ETL logical design, ETL optimization, ETL workflow, ETL physical implementation, ETL conceptual design}
}

@inproceedings{10.1145/3371425.3371459,
author = {Tan, Wenan and Jiang, Zihui},
title = {A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371459},
doi = {10.1145/3371425.3371459},
abstract = {While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {70},
numpages = {6},
keywords = {incentive mechanism, sensor network, fairness competition, mobile crowdsensing},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/3360322.3360836,
author = {Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"{e}lle and Evins, Ralph},
title = {Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat Data},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360836},
doi = {10.1145/3360322.3360836},
abstract = {The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {223–232},
numpages = {10},
keywords = {Buildings, thermal characteristics, smart thermostats, gray box models},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3277593.3277642,
author = {Truong, Hong-Linh},
title = {Dynamic IoT Data, Protocol, and Middleware Interoperability with Resource Slice Concepts and Tools: Tutorial},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277642},
doi = {10.1145/3277593.3277642},
abstract = {Dealing with interoperability in the IoT domain is a complex matter that requires various techniques for tackling data, protocol and middleware interoperability. We cannot solve IoT interoperability problems by just developing (new) software components and (semantic) data models. In this tutorial, we will present interoperability techniques for complex IoT Cloud applications by leveraging dynamic solutions of provisioning and reconfiguring of IoT data processing pipelines, protocol bridges, IoT middleware and cloud services. First, the tutorial will examine cross-layered, cross-system inter-operability issues and present a DevOps IoT Interoperability approach for defining metadata, selecting resources and software artifacts, and provisioning and connecting resources to create various potential solutions for IoT Cloud interoperability using resource slice concepts. Second, the tutorial will present techniques for dynamically provisioning data pipelines, middleware services, protocol adapters and custom solutions to address cross-layered, cross-system interoperability for IoT Cloud applications. Such solutions also allow dynamic reconfiguration of resources to add/remove interoperability support. We will present the concepts and techniques with hands-on examples using our research tools rsiHub and IoTCloudSamples.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {48},
numpages = {4},
keywords = {cloud computing, IoT interoperability, resource slice},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/2661829.2663539,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14): CIKM 2014 Workshop},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2663539},
doi = {10.1145/2661829.2663539},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remains to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side - the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues - and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization - are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects - how to not creep out users?},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2094–2095},
numpages = {2},
keywords = {graph search, query suggest, semantic annotation},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3472163.3472173,
author = {Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine},
title = {Data Management in the Data Lake: A Systematic Mapping},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472173},
doi = {10.1145/3472163.3472173},
abstract = { The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.},
booktitle = {25th International Database Engineering &amp; Applications Symposium},
pages = {280–284},
numpages = {5},
keywords = {Systematic mapping, Data management, Data lake},
location = {Montreal, QC, Canada},
series = {IDEAS 2021}
}

@article{10.1145/3446373,
author = {Li, Xi and Wang, Zehua and Leung, Victor C. M. and Ji, Hong and Liu, Yiming and Zhang, Heli},
title = {Blockchain-Empowered Data-Driven Networks: A Survey and Outlook},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446373},
doi = {10.1145/3446373},
abstract = {The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {58},
numpages = {38},
keywords = {networking technologies, blockchain, Data-driven networks, blockchain-empowered data-driven networks}
}

@inproceedings{10.1145/2815833.2816944,
author = {Rizzo, Giuseppe and Corcho, Oscar and Troncy, Rapha\"{e}l and Plu, Julien and Hermida, Juan Carlos Ballesteros and Assaf, Ahmad},
title = {The 3cixty Knowledge Base for Expo Milano 2015: Enabling Visitors to Explore the City},
year = {2015},
isbn = {9781450338493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815833.2816944},
doi = {10.1145/2815833.2816944},
abstract = {In this paper, we present the 3cixty Knowledge Base, which collects and harmonizes descriptions of events, places, transportation facilities and user-generated data such as reviews of the city and Expo site of Milan. This knowledge base is used by a set of web and mobile applications to guide Expo Milano 2015 visitors in the city and in the exhibit, allowing them to find places, satellite events and transportation facilities around Milan. As of July 24th, 2015 the knowledge base contains 18665 unique events, 225821 unique places, 94789 reviews, and 9343 transportation facilities, collected from several static, near- and real time local and global data providers, including Expo Milano 2015 official services and numerous social media platforms. The ontologies used as a backbone for structuring the knowledge base follow a rigorous development method where the design principle has generally been to re-use existing ontologies when they exist. We think that the lessons learned from this development will be useful for similar endeavors in other cities or large events around the world with a similar ecosystem of data provisioning services.},
booktitle = {Proceedings of the 8th International Conference on Knowledge Capture},
articleno = {18},
numpages = {4},
keywords = {Expo 2015, Smart City, Knowledge base, 3cixty, Data Integration, Data Reconciliation},
location = {Palisades, NY, USA},
series = {K-CAP 2015}
}

@article{10.1145/3191750,
author = {Lin, Yuxiang and Dong, Wei and Chen, Yuan},
title = {Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191750},
doi = {10.1145/3191750},
abstract = {Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {18},
numpages = {18},
keywords = {Sensor calibration, Mobile sensor network, Low-cost sensors, Air quality}
}

@inproceedings{10.1145/3077136.3082060,
author = {Deng, Alex and Dmitriev, Pavel and Gupta, Somit and Kohavi, Ron and Raff, Paul and Vermeer, Lukas},
title = {A/B Testing at Scale: Accelerating Software Innovation},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3082060},
doi = {10.1145/3077136.3082060},
abstract = {The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and evaluation of online controlled experiments at scale (100's of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial we will give an introduction to A/B testing, share key lessons learned from scaling experimentation at Bing to thousands of experiments per year, present real examples, and outline promising directions for future work. The tutorial will go beyond applications of A/B testing in information retrieval and will also discuss on practical and research challenges arising in experimentation on web sites and mobile and desktop apps. Our goal in this tutorial is to teach attendees how to scale experimentation for their teams, products, and companies, leading to better data-driven decisions. We also want to inspire more academic research in the relatively new and rapidly evolving field of online controlled experimentation.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1395–1397},
numpages = {3},
keywords = {experimentation, a/b testing},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3283207.3283210,
author = {Jilani, Musfira and Corcoran, Padraig and Bertolotto, Michela},
title = {A Multi-Layer CRF Based Methodology for Improving Crowdsourced Street Semantics},
year = {2018},
isbn = {9781450360371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283207.3283210},
doi = {10.1145/3283207.3283210},
abstract = {This paper presents an intuitive and novel method for improving the semantic quality of streets in crowdsourced maps. Two factors negatively affecting the quality are incorrect and ambiguous semantics. Toward overcoming these, a multi-layer CRF based model is proposed that performs a simultaneous hierarchical classification of streets into fine-grained (crowdsourced; therefore, rich but ambiguous) and coarse-grained (familiar and standard) semantics. Inference is performed using Lazy Flipper algorithm which is fast for street network consisting of several hundred thousand streets. The model achieves a classification accuracy of 61% for fine-grained classification and 77% for coarse-grained classification respectively.},
booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
pages = {29–38},
numpages = {10},
keywords = {Conditional Random Fields, OpenStreetMap, Hierarchical Classification, Street Networks, Semantics},
location = {Seattle, WA, USA},
series = {IWCTS'18}
}

@inproceedings{10.1145/2938503.2938519,
author = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
title = {On the Discovery of Relaxed Functional Dependencies},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938519},
doi = {10.1145/2938503.2938519},
abstract = {Functional dependencies (fds) express important relationships among data, which can be used for several goals, including schema normalization and data cleansing. However, to solve several issues in emerging application domains, such as the identification of data inconsistencies or patterns of semantically related data, it has been necessary to relax the fd definition through the introduction of approximations in data comparison and/or validity. Moreover, while fds were originally specified at design time, with the availability of massive data and computational power many algorithms have been devised to automatically discover them from data, including algorithms for discovering some types of relaxed fds. In this paper we present a technique that exploits lattice-based algorithms for the discovery of fds from data, in order to detect relaxed fds. Moreover, we introduce an algorithm to determine a proper distance threshold for a given relaxed fd holding over the entire database.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {53–61},
numpages = {9},
keywords = {Database integration, functional dependency, discovery algorithm, approximate match},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@inbook{10.1145/3310205.3310214,
title = {Conclusion and Future Thoughts},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310214},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3085580,
author = {Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta},
title = {QoS in Body Area Networks: A Survey},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3085580},
doi = {10.1145/3085580},
abstract = {Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {25},
numpages = {46},
keywords = {QoS, medical care, healthcare, Body area networks, cloud computing}
}

@inproceedings{10.1145/2523429.2523458,
author = {Nyk\"{a}nen, Ossi},
title = {Datamap Visualization Technique for Interactively Visualizing Large Datasets},
year = {2013},
isbn = {9781450319928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523429.2523458},
doi = {10.1145/2523429.2523458},
abstract = {This article describes the novel datamap visualization technique which enables visualizing large datasets interactively and fairly, inspired by geographic maps and microscopes. The main contributions include introducing the datamap metaphor and datamap visualization architecture, specifying efficient methods of approximate rendering, and illustrating the basic concepts in terms of example applications.},
booktitle = {Proceedings of International Conference on Making Sense of Converging Media},
pages = {52–58},
numpages = {7},
keywords = {Datamap visualization, Interactive data exploration and discovery, Data and knowledge visualization, Visualization techniques and methodologies},
location = {Tampere, Finland},
series = {AcademicMindTrek '13}
}

@inproceedings{10.1145/3141880.3141886,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Key Concepts of Data Management: An Empirical Approach},
year = {2017},
isbn = {9781450353014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141880.3141886},
doi = {10.1145/3141880.3141886},
abstract = {When preparing new topics for teaching, it is important to identify their central aspects. Sets of fundamental ideas, great principles or big ideas have already been described for several parts of computer science. Yet, existing catalogs of ideas, principles and concepts of computer science only consider the field data management marginally. However, we assume that several concepts of data management are fundamental to CS and, despite the significant changes in this field in recent years, have long-term relevance. In order to provide a comprehensive overview of the key concepts of data management and to bring relevant parts of this field to school, we describe and use an empirical approach to determine such central aspects systematically. This results in a model of key concepts of data management. On the basis of examples, we show how the model can be interpreted and used in different contexts and settings.},
booktitle = {Proceedings of the 17th Koli Calling International Conference on Computing Education Research},
pages = {30–39},
numpages = {10},
keywords = {model, CS education, principles, data management, mechanics, key concepts, practices, core technologies},
location = {Koli, Finland},
series = {Koli Calling '17}
}

@inproceedings{10.1145/3485768.3485770,
author = {Guo, Yuanyuan},
title = {Data Protection Measures in E-Society: Policy Implications of British Data Protection Act to China},
year = {2021},
isbn = {9781450390156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485768.3485770},
doi = {10.1145/3485768.3485770},
booktitle = {2021 5th International Conference on E-Society, E-Education and E-Technology},
pages = {171–176},
numpages = {6},
keywords = {Private information, Public policy, Data protection, E-society},
location = {Taipei, Taiwan},
series = {ICSET 2021}
}

@inproceedings{10.1145/3487664.3487783,
author = {Diamantini, Claudia and Potena, Domenico and Storti, Emanuele},
title = {A Semantic Data Lake Model for Analytic Query-Driven Discovery},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487783},
doi = {10.1145/3487664.3487783},
abstract = { Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {183–186},
numpages = {4},
keywords = {indicator, ontology, query-driven discovery, Data Lake},
location = {Linz, Austria},
series = {iiWAS2021}
}

@article{10.1145/2676566,
author = {MacLean, Diana Lynn},
title = {Gathering People to Gather Data},
year = {2014},
issue_date = {Winter 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/2676566},
doi = {10.1145/2676566},
abstract = {An interview with Paul Wicks, Vice President of Innovation at PatientsLikeMe, a patient network and real-time research platform.},
journal = {XRDS},
month = {dec},
pages = {18–22},
numpages = {5}
}

@inproceedings{10.1145/3210586.3210587,
author = {Baker, Karen S. and Karasti, Helena},
title = {Data Care and Its Politics: Designing for Local Collective Data Management as a Neglected Thing},
year = {2018},
isbn = {9781450363716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210586.3210587},
doi = {10.1145/3210586.3210587},
abstract = {In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask "How to design for data care?" and "How to account for the politics of data care in design?" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.},
booktitle = {Proceedings of the 15th Participatory Design Conference: Full Papers - Volume 1},
articleno = {10},
numpages = {12},
keywords = {science and technology studies, local collective data management, infrastructuring, information infrastructure, partnering designer, politics, information management, participatory design, matters of care, data care},
location = {Hasselt and Genk, Belgium},
series = {PDC '18}
}

@inproceedings{10.1145/3093338.3104170,
author = {Pugmire, David and Bozda\u{g}, Ebru and Lefebvre, Matthieu and Tromp, Jeroen and Komatitsch, Dmitri and Peter, Daniel and Podhorszki, Norbert and Hill, Judith},
title = {Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104170},
doi = {10.1145/3093338.3104170},
abstract = {In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {75},
numpages = {4},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@inproceedings{10.1145/2721956.2721968,
author = {Leibold, Christian F. and Spies, Marcus},
title = {Towards a Pattern Language for Cognitive Systems Integration},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721968},
doi = {10.1145/2721956.2721968},
abstract = {This paper discusses the influence of recent advances in cognitive computing systems on enterprise software architecture and design/development. Specifically, building on key features and capabilities of cognitive computing systems, we propose a new schema of enterprise application integration patterns in the tradition of the design pattern literature. Our schema has three groups of patterns addressing essential scoping, security and service integration issues related to cognitive components in enterprise architecture. While some patterns are modifications or refinements of known Enterprise Application Integration patterns, some of them are new and require dedicated consideration by enterprise architects and software designers.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {9},
keywords = {pattern language, cognitive systems, scope identification, requirements engineering, enterprise integration pattern, social and ethical impact},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@article{10.14778/3352063.3352066,
author = {Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong},
title = {Cleanits: A Data Cleaning System for Industrial Time Series},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352066},
doi = {10.14778/3352063.3352066},
abstract = {The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1786–1789},
numpages = {4}
}

@inproceedings{10.1145/3485314.3485323,
author = {Su, Hang and He, Qian and Guo, Biao},
title = {KPI Anomaly Detection Method for Data Center AIOps Based on GRU-GAN},
year = {2021},
isbn = {9781450384957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485314.3485323},
doi = {10.1145/3485314.3485323},
abstract = {The system architecture and application services of the data center are becoming increasingly large. To ensure the stable operation of the systems and businesses carried by the data center, the operations engineer needs to collect and monitor the generating KPIs during the operation of the systems and services. Traditional KPI anomaly detection methods are faced with the challenges of the huge amount of KPIs and constantly changing data characteristics, which are gradually no longer suitable for highly dynamic systems and services. With the popularity of artificial intelligence algorithms, machine learning and deep learning methods have also begun to be applied in operation and maintenance scenarios, that is the emergence of Artificial Intelligence for IT Operations (AIOps). KPI anomaly detection is the underlying core technology of AIOps. This paper proposes a hybrid model based on GRU-GAN (GGAN) for KPI anomaly detection in data center AIOps. The Gated Recurrent Unit (GRU) network is selected as the generator and discriminator of Generative adversarial network (GAN) in this model, which get the time correlation and data distribution of KPI through the adversarial training between the generator and the discriminator to make use of the reconstruction ability of the generator and the discriminant ability of the discriminator at the same time. At the anomaly detection stage, the anomaly score is formed by integrating reconstruction difference and discrimination loss to complete the anomaly detection task. Experimental results show that the proposed method can more accurately capture the variable data characteristics of KPI compared with the traditional KPI anomaly detection method and the general unsupervised method, as well as achieve better performance in the KPI anomaly detection task.},
booktitle = {2021 10th International Conference on Internet Computing for Science and Engineering},
pages = {23–29},
numpages = {7},
keywords = {KPI Anomaly Detection, GRU, GAN, AIOps, Data Center},
location = {Guilin, China},
series = {ICICSE 2021}
}

@inproceedings{10.1145/3465336.3475107,
author = {Nurmikko-Fuller, Terhi and Pickering, Paul},
title = {Reductio Ad Absurdum?: From Analogue Hypertext to Digital Humanities},
year = {2021},
isbn = {9781450385510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465336.3475107},
doi = {10.1145/3465336.3475107},
abstract = {In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.},
booktitle = {Proceedings of the 32nd ACM Conference on Hypertext and Social Media},
pages = {245–250},
numpages = {6},
keywords = {hypertext, linked data, information aggregation, political history, australian history},
location = {Virtual Event, USA},
series = {HT '21}
}

@inproceedings{10.1145/3085228.3085283,
author = {Brolch\'{a}in, Niall \'{O} and Porwol, Lukasz and Ojo, Adegboyega and Wagner, Tilman and Lopez, Eva Tamara and Karstens, Eric},
title = {Extending Open Data Platforms with Storytelling Features},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085283},
doi = {10.1145/3085228.3085283},
abstract = {1Research into Data-Driven Storytelling using Open Data has led to considerable discussion into many possible futures for storytelling and journalism in a Data-Driven world, in particular, into the Open Data directives framed by various governments across the globe as a means of facilitating governments, transparency enabled citizens and journalists to get more insights into government actions and enable deeper and easier monitoring of governments' work. While progress in the development of Open Data platforms (usually funded by national and local governments) has been significant, it is only now that we are beginning to see the emergence of more practical and more applied use of Open Data platforms. Previous works have highlighted the potential for storytelling using Open Data as a source of information for journalistic stories. Nevertheless, there is a paucity of studies into Open Data platform affordances to support Data-Driven Storytelling. In this paper, we elaborate on existing Open Data platforms in terms of support for storytelling and analyse feedback from stakeholder focus groups, to discover what methods and tools can introduce or facilitate the storytelling capabilities of Open Data platforms.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {48–53},
numpages = {6},
keywords = {Open Data, Data-Driven Journalism, YDS Platform, Journalism, Data-Driven Storytelling, Usable Open Data Platform},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.1145/2641383.2641390,
author = {Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti},
title = {Evaluation Methodologies in Information Retrieval Dagstuhl Seminar 13441},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2641383.2641390},
doi = {10.1145/2641383.2641390},
journal = {SIGIR Forum},
month = {jun},
pages = {36–41},
numpages = {6}
}

@inproceedings{10.5555/3017447.3017493,
author = {Bishop, Bradley Wade and Hank, Carolyn},
title = {Data Curation Profiling of Biocollections},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In the contexts of the data deluge and open data, scientists studying biodiversity benefit from online access to global datasets of existing vouchered biological and paleontological collections. Using biocollections collected over time across the world allows for the advancement of scientific knowledge concerning evolution in process as well as species poleward migrations, an indicator of climate change. This study's purpose was to validate and expand the Data Curation Profiles (DCP) to digital biocollections and inform a DCP framework for worldwide biota. Ten biocollection producers, curating various types of specimens affiliated with the project building the United States' national biodiversity infrastructure, were interviewed using the DCP questionnaire. Results indicate there is extreme diversity in the curation of biocollections and additional DCP questions should be added to reflect the complicated approaches to biological data curation. Although discipline specific metadata creation tools, standards, and practices enable long-term sustainability of the U.S. digitization effort, some scientists would benefit from further clarification and guidance on the information needs of consumers beyond designated communities of expert users, and the long-term preservation of biocollections.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {46},
numpages = {9},
keywords = {biocollections, data curation profiles, data curation, data provenance, biology},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3209281.3209297,
author = {Yoon, Sang-Pil and Joo, Moon-Ho and Kwon, Hun-Yeong},
title = {Role of Law as a Guardian of the Right to Use Public Sector Information: Case Study of Korean Government},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209297},
doi = {10.1145/3209281.3209297},
abstract = {With data revolution, data is emerging as a new raw material. As the importance of data has increased, interest in the availability of public sector information (PSI) has also grown. PSI created in the public sector comprises public attributes and directly impacts national administration and citizen's lives.Korea has almost the highest level of Information and Communication Technology (ICT) infrastructure and considerable data through government-led policies. As a result of such policies, Korea has demonstrated excellent results in the United Nation's e-government survey, ITU's ICT development index, and OECD's public data openness index. Paradoxically, however, this history and experience is a stumbling block to a new era. PSI, which is a basic resource for realizing the value of openness, sharing, cooperation, and communication, should be actively managed and opened by government to provide support for reuse in the sense that the government is its main producer and manager. However, no matter how good the quality of PSI through data management is and how excellent policies and institutions are established, if the private sector cannot actively use it, it is useless. What is the role of government and law in the context of changing the way data is managed and blurring sectoral boundaries?This paper aims to propose core challenges by analyzing the case of Korea in order to derive a basis for discussions to coordinate public and private cooperation and legal relations in the process. To begin with, we analyze the changes in the management environment of data and PSI and identify the role of government and law in responding to changes in the legal rights. Then, we discuss how Korea responds to change, examines related policies by function and discussions on the data law, which seem to have the greatest effect on government's role, and suggests essential tasks to change its role accordingly.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {83},
numpages = {10},
keywords = {legal right management, openness, data management, the right to know, public-private cooperation, public sector information, public data, reuse of PSI},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.5555/3522802.3522903,
author = {Hunker, Joachim and Wuttke, Alexander and Scheidler, Anne Antonia and Rabe, Markus},
title = {A Farming-for-Mining-Framework to Gain Knowledge in Supply Chains},
year = {2021},
publisher = {IEEE Press},
abstract = {Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {130},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3465481.3470055,
author = {P\"{o}hn, Daniela and Seeber, Sebastian and Hanauer, Tanja and Ziegler, Jule A. and Schmitz, David},
title = {Towards Improving Identity and Access Management with the IdMSecMan Process Framework},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470055},
doi = {10.1145/3465481.3470055},
abstract = { In today’s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work. },
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {89},
numpages = {10},
keywords = {Security, Identity Management, Server, Security Management},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.1145/3340964.3340975,
author = {Xu, Jianqiu and Lu, Hua and G\"{u}ting, Ralf Hartmut},
title = {Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects Database},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340975},
doi = {10.1145/3340964.3340975},
abstract = {The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {222–225},
numpages = {4},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/2912160.2912206,
author = {Li, Hongqin and Zhai, Jun},
title = {Constructing Investment Open Data of Chinese Listed Companies Based on Linked Data},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912206},
doi = {10.1145/2912160.2912206},
abstract = {Linked Data can provide the data according to user's demand, promote the availability of half structured and unstructured data on the network, improve the interoperability of open data. With the development of Linked Data, Linked Enterprise Data becomes a research hotspot. This article draw lessons from foreign investment Linked Data research of listed companies, selected the listed company information and XBRL reports from Shanghai stock exchange and Shenzhen stock exchange, industry information from the China securities regulatory commission and daily stock price from Flush as data source, built investment open data of Chinese listed companies based on Linked Data. This work could promote the internationalization of the Chinese data and the commercial use of open government data, lay the foundation for the global data ecological system, at the same time prepare for the challenge of Shanghai-Hong Kong Stock Connect and Shenzhen-Hong Kong Stock Connect even the challenge of international investment.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {475–480},
numpages = {6},
keywords = {Linked Data, Open Data, XBRL, Linked Enterprise Data},
location = {Shanghai, China},
series = {dg.o '16}
}

@article{10.14778/3484224.3484233,
author = {Ammerlaan, Remmelt and Antonius, Gilbert and Friedman, Marc and Hossain, H M Sajjad and Jindal, Alekh and Orenberg, Peter and Patel, Hiren and Qiao, Shi and Ramani, Vijay and Rosenblatt, Lucas and Roy, Abhishek and Shaffer, Irene and Srinivasan, Soundarajan and Weimer, Markus},
title = {PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484233},
doi = {10.14778/3484224.3484233},
abstract = {Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3362–3375},
numpages = {14}
}

@inproceedings{10.1145/3316782.3322785,
author = {Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald},
title = {Presenting a Data Imputation Concept to Support the Continuous Assessment of Human Vital Data and Activities},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322785},
doi = {10.1145/3316782.3322785},
abstract = {Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {587–592},
numpages = {6},
keywords = {accelerometer, data imputation, data fusion, smartwatch, sensor fusion, mobile device, controlled data creation, coherent database},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@article{10.1145/3503780.3503792,
author = {Kondylakis, Haridimos and Stefanidis, Kostas and Rao, Praveen},
title = {Report on the Third International Workshop on Semantic Web Meets Health Data Management (SWH 2020)},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3503780.3503792},
doi = {10.1145/3503780.3503792},
abstract = {Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/3303772.3303821,
author = {Mitra, Ritayan and Chavan, Pankaj},
title = {DEBE Feedback for Large Lecture Classroom Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303821},
doi = {10.1145/3303772.3303821},
abstract = {Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {426–430},
numpages = {5},
keywords = {mobile application, Large lectures, live feedback, quantified self, learning analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3414274.3414509,
author = {Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang},
title = {Multidimensional Data Mining on the Early Scientific Talents of China},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414509},
doi = {10.1145/3414274.3414509},
abstract = {The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {239–246},
numpages = {8},
keywords = {Cultivation of high-level talents, Education, Data-mining},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3487075.3487147,
author = {Zhang, Jiapeng and Zhuang, Cunbo and Liu, Jianhua and Yuan, Kun and Zhang, Jin and Liu, Juan},
title = {Digital Twin-Based Three-Dimensional Visual and Global Monitoring of Assembly Shop-Floor},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487147},
doi = {10.1145/3487075.3487147},
abstract = {Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.},
booktitle = {The 5th International Conference on Computer Science and Application Engineering},
articleno = {72},
numpages = {7},
keywords = {Global monitoring, Digital twin, Assembly shop-floor, 3D visual monitoring},
location = {Sanya, China},
series = {CSAE 2021}
}

@inproceedings{10.1145/3512826.3512840,
author = {Yang, Xiaohui and Guo, Chenxi and Ren, Huan and Dong, Ming},
title = {Research on Anomaly Detection Method of Online Monitoring Data of Dissolved Gas in Transformer Oil},
year = {2022},
isbn = {9781450395489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512826.3512840},
doi = {10.1145/3512826.3512840},
abstract = {Dissolved gases in oil analysis has been a significant conventional condition detection method for condition evaluation for power transformers. But false data and wrong data do exist in DGA on-line monitoring system, which often lead to misjudgment. To handle this problem, the monitoring system often uses a threshold method based on data distribution statistics to determine the authenticity of the data. However, it is difficult to grasp the rules of the data distribution in advance, resulting in the problem of generally low detection rate of abnormal data. In this paper, according to the time series characteristics of on-line monitoring data of DGA, an abnormal data detection method based on condensed hierarchical clustering is proposed. First, the sliding time window is used to preprocess a variety of oil gas monitoring data to obtain a time series set of monitoring data, and comprehensively apply statistical indicators to classify them and establish Typical time series map; on this basis, the agglomerated hierarchical clustering model is used to perform similarity clustering on the distance between different characteristic data points and the typical abnormal map to determine the abnormal type of the monitoring data. The verification of the application of actual monitoring data shows that this method can detect data anomalies in the online monitoring data stream and determine its type in real time.},
booktitle = {2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {65–69},
numpages = {5},
keywords = {Dissolved gases in oil, Hierarchical agglomerative cluster, On-line monitoring, Time series, Anomaly detection},
location = {Bangkok, Thailand},
series = {AIEE 2022}
}

@article{10.1145/3349629,
author = {Verma, Neeta and Dawar, Savita},
title = {Digital Transformation in the Indian Government},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3349629},
doi = {10.1145/3349629},
journal = {Commun. ACM},
month = {oct},
pages = {50–53},
numpages = {4}
}

@article{10.1145/2590989.2591002,
author = {Pedersen, Torben Bach and Lehner, Wolfgang},
title = {Report on the Second International Workshop on Energy Data Management (EnDM 2013)},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2591002},
doi = {10.1145/2590989.2591002},
journal = {SIGMOD Rec.},
month = {feb},
pages = {70–72},
numpages = {3}
}


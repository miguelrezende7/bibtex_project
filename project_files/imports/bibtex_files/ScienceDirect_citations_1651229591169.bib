@article{ZHANG2018146,
title = {A survey on deep learning for big data},
journal = {Information Fusion},
volume = {42},
pages = {146-157},
year = {2018},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517305328},
author = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
keywords = {Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks},
abstract = {Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{ADDOTENKORANG2016528,
title = {Big data applications in operations/supply-chain management: A literature review},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {528-543},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360835216303631},
author = {Richard Addo-Tenkorang and Petri T. Helo},
keywords = {Big data – applications and analysis, Internet of Things (IoT), Cloud computing, Master database management, Operations/supply-chain management},
abstract = {Purpose
Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of “big data.” Therefore, this paper attempts to thoroughly investigate “big data,” its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of “big data” and its extension into “big data II”/IoT–value-adding perspectives by proposing a value-adding framework.
Methodology/research approach
The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of “big data” applications between 2010 and 2015.
Findings/results
The four main attributes or factors identified with “big data” include – big data development sources (Variety – V1), big data acquisition (Velocity – V2), big data storage (Volume – V3), and finally big data analysis (Veracity – V4). However, the study of “big data” has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding – V5) – “Big Data cloud computing perspective/Internet of Things (IoT)”. Hence, the four Vs of “big data” is now expanded into five Vs.
Originality/value of research
This paper presents original literature review research discussing “big data” issues, trends and perspectives in operations/supply-chain management in order to propose “Big data II” (IoT – Value-adding) framework. This proposed framework is supposed or assumed to be an extension of “big data” in a value-adding perspective, thus proposing that “big data” be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.}
}
@article{SHAHA2016725,
title = {Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations},
journal = {Orthopedic Clinics of North America},
volume = {47},
number = {4},
pages = {725-732},
year = {2016},
note = {Sports-Related Injuries},
issn = {0030-5898},
doi = {https://doi.org/10.1016/j.ocl.2016.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0030589816300554},
author = {Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh},
keywords = {Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database}
}
@article{MIKALEF2020103237,
title = {Big data and business analytics: A research agenda for realizing business value},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103237},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103237},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619310687},
author = {Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou}
}
@article{SAIF2018118,
title = {Performance Analysis of Big Data and Cloud Computing Techniques: A Survey},
journal = {Procedia Computer Science},
volume = {132},
pages = {118-127},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309062},
author = {Subia Saif and Samar Wazir},
keywords = {Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure},
abstract = {A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.}
}
@incollection{SHARMA2019189,
title = {Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry},
editor = {Valentina E. Balas and Le Hoang Son and Sudan Jha and Manju Khari and Raghvendra Kumar},
booktitle = {Internet of Things in Biomedical Engineering},
publisher = {Academic Press},
pages = {189-212},
year = {2019},
isbn = {978-0-12-817356-5},
doi = {https://doi.org/10.1016/B978-0-12-817356-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173565000103},
author = {Neha Sharma and Malini M. Patil and Madhavi Shamkuwar},
keywords = {Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics},
abstract = {The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.}
}
@incollection{VALEEV2021209,
title = {Chapter 6 - Big data analytics and process safety},
editor = {Sagit Valeev and Natalya Kondratyeva},
booktitle = {Process Safety and Big Data},
publisher = {Elsevier},
pages = {209-270},
year = {2021},
isbn = {978-0-12-822066-5},
doi = {https://doi.org/10.1016/B978-0-12-822066-5.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220665000017},
author = {Sagit Valeev and Natalya Kondratyeva},
keywords = {Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis},
abstract = {The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.}
}
@article{BIKAKIS2019100123,
title = {Big Data Exploration, Visualization and Analytics},
journal = {Big Data Research},
volume = {18},
pages = {100123},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2214579619302254},
author = {Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil}
}
@article{BLAZQUEZ201899,
title = {Big Data sources and methods for social and economic analyses},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {99-113},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517310946},
author = {Desamparados Blazquez and Josep Domenech},
keywords = {Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods},
abstract = {The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.}
}
@article{STOREY201750,
title = {Big data technologies and Management: What conceptual modeling can do},
journal = {Data & Knowledge Engineering},
volume = {108},
pages = {50-67},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300277},
author = {Veda C. Storey and Il-Yeol Song},
abstract = {The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.}
}
@article{RIVERA20191,
title = {Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {24},
pages = {1-6},
year = {2019},
note = {5th IFAC Symposium on Telematics Applications TA 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.364},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319322645},
author = {Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling},
keywords = {industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling},
abstract = {In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.}
}
@article{BALACHANDRAN20171112,
title = {Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence},
journal = {Procedia Computer Science},
volume = {112},
pages = {1112-1122},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314953},
author = {Bala M. Balachandran and Shivika Prasad},
keywords = {Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS},
abstract = {Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.}
}
@incollection{QING2021181,
title = {Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {181-210},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000091},
author = {Wu Qing},
keywords = {artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment},
abstract = {This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.}
}
@article{GUNTHER2017191,
title = {Debating big data: A literature review on realizing value from big data},
journal = {The Journal of Strategic Information Systems},
volume = {26},
number = {3},
pages = {191-209},
year = {2017},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717302615},
author = {Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg},
keywords = {Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity},
abstract = {Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.}
}
@article{YANG20141563,
title = {A spatiotemporal compression based approach for efficient big data processing on Cloud},
journal = {Journal of Computer and System Sciences},
volume = {80},
number = {8},
pages = {1563-1583},
year = {2014},
note = {Special Issue on Theory and Applications in Parallel and Distributed Computing Systems},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2014.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S002200001400066X},
author = {Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen},
keywords = {Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling},
abstract = {It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.}
}
@article{TIWARI2018319,
title = {Big data analytics in supply chain management between 2010 and 2016: Insights to industries},
journal = {Computers & Industrial Engineering},
volume = {115},
pages = {319-330},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217305508},
author = {Sunil Tiwari and H.M. Wee and Yosef Daryanto},
keywords = {Big data analytics, Supply chain management, Big data application},
abstract = {This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.}
}
@article{WANG20183,
title = {Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations},
journal = {Technological Forecasting and Social Change},
volume = {126},
pages = {3-13},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516000500},
author = {Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd},
keywords = {Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care},
abstract = {To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.}
}
@article{SHUKLA20191015,
title = {Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1015-1026},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301992},
author = {Manish Shukla and Lana Mattar},
keywords = {Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling},
abstract = {In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.}
}
@article{ZUO2018839,
title = {Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing},
journal = {Environmental Pollution},
volume = {240},
pages = {839-847},
year = {2018},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2018.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0269749118307681},
author = {JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong},
keywords = {Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing},
abstract = {Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.}
}
@article{ELRAGAL2014242,
title = {ERP and Big Data: The Inept Couple},
journal = {Procedia Technology},
volume = {16},
pages = {242-249},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314003168},
author = {Ahmed Elragal},
keywords = {ERP, Big Data, Research Agenda},
abstract = {The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.}
}
@article{BONNEL20181,
title = {Transport survey methods - in the era of big data facing new and old challenges},
journal = {Transportation Research Procedia},
volume = {32},
pages = {1-15},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301522},
author = {Patrick Bonnel and Marcela A. Munizaga},
keywords = {ISCTSC, transport, survey methodology, big data},
abstract = {This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.}
}
@article{SIDDIQA2016151,
title = {A survey of big data management: Taxonomy and state-of-the-art},
journal = {Journal of Network and Computer Applications},
volume = {71},
pages = {151-166},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516300583},
author = {Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin},
keywords = {Big data management, Storage, Big data, Processing, Security},
abstract = {The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.}
}
@article{JIA20191652,
title = {Opportunities and challenges of using big data for global health},
journal = {Science Bulletin},
volume = {64},
number = {22},
pages = {1652-1654},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319305523},
author = {Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang}
}
@article{REY201837,
title = {Causes of deaths data, linkages and big data perspectives},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {37-40},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X16301652},
author = {Grégoire Rey and Karim Bounebache and Claire Rondet},
keywords = {Causes of death data, Data linkages, Big data},
abstract = {The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.}
}
@article{SHIN2016837,
title = {Demystifying big data: Anatomy of big data developmental process},
journal = {Telecommunications Policy},
volume = {40},
number = {9},
pages = {837-854},
year = {2016},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0308596115000567},
author = {Dong-Hee Shin},
keywords = {Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience},
abstract = {This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.}
}
@article{AHMAD2016439,
title = {An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication},
journal = {Neurocomputing},
volume = {174},
pages = {439-453},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.04.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215012369},
author = {Awais Ahmad and Anand Paul and M. Mazhar Rathore},
keywords = {M2M, Big Data, Divide-and-conquer, Data fusion domain},
abstract = {Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.}
}
@article{HONG20191387,
title = {Energy forecasting in the big data world},
journal = {International Journal of Forecasting},
volume = {35},
number = {4},
pages = {1387-1388},
year = {2019},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301062},
author = {Tao Hong and Pierre Pinson}
}
@article{GU201722,
title = {Visualizing the knowledge structure and evolution of big data research in healthcare informatics},
journal = {International Journal of Medical Informatics},
volume = {98},
pages = {22-32},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616302556},
author = {Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang},
keywords = {Big data, Healthcare informatics, Bibliometrics, Knowledge structure, Knowledge management},
abstract = {Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.}
}
@article{HERRMANN2022194,
title = {An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {194-204},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002277},
author = {Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme},
keywords = {Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning},
abstract = {In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.}
}
@article{SANTOS2017750,
title = {A Big Data system supporting Bosch Braga Industry 4.0 strategy},
journal = {International Journal of Information Management},
volume = {37},
number = {6},
pages = {750-760},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217306023},
author = {Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão},
keywords = {Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch},
abstract = {People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).}
}
@article{DESILVA2021104305,
title = {Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data},
journal = {Computers in Biology and Medicine},
volume = {132},
pages = {104305},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104305},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521000998},
author = {Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott},
keywords = {Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining},
abstract = {Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.}
}
@incollection{APPEL2021193,
title = {Chapter 6 - Psychological targeting in the age of Big Data},
editor = {Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter},
booktitle = {Measuring and Modeling Persons and Situations},
publisher = {Academic Press},
pages = {193-222},
year = {2021},
isbn = {978-0-12-819200-9},
doi = {https://doi.org/10.1016/B978-0-12-819200-9.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192009000156},
author = {Ruth E. Appel and Sandra C. Matz},
keywords = {Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity},
abstract = {Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.}
}
@article{KARIM2020942,
title = {Big data management in participatory sensing: Issues, trends and future directions},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {942-955},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311627},
author = {Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran},
keywords = {Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing},
abstract = {Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.}
}
@article{SANCHEZMARTINEZ2016236,
title = {Workshop 5 report: Harnessing big data},
journal = {Research in Transportation Economics},
volume = {59},
pages = {236-241},
year = {2016},
note = {Competition and Ownership in Land Passenger Transport (selected papers from the Thredbo 14 conference)},
issn = {0739-8859},
doi = {https://doi.org/10.1016/j.retrec.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0739885916301494},
author = {Gabriel E. Sánchez-Martínez and Marcela Munizaga},
keywords = {Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices},
abstract = {A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.}
}
@article{VANDENBROEK2018330,
title = {Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation},
journal = {Technological Forecasting and Social Change},
volume = {129},
pages = {330-338},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314695},
author = {Tijs {van den Broek} and Anne Fleur {van Veenstra}},
keywords = {Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration},
abstract = {Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.}
}
@article{PALANISAMY2019415,
title = {Implications of big data analytics in developing healthcare frameworks – A review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {4},
pages = {415-425},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302938},
author = {Venketesh Palanisamy and Ramkumar Thirunavukarasu},
keywords = {Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools},
abstract = {The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.}
}
@article{RICHTER201929,
title = {Efficient learning from big data for cancer risk modeling: A case study with melanoma},
journal = {Computers in Biology and Medicine},
volume = {110},
pages = {29-39},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519301477},
author = {Aaron N. Richter and Taghi M. Khoshgoftaar},
keywords = {Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer},
abstract = {Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.}
}
@article{ARUNACHALAM2018416,
title = {Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {416-436},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516303799},
author = {Deepak Arunachalam and Niraj Kumar and John Paul Kawalek},
keywords = {Supply chain management, Big data analytics, Capabilities, Maturity model},
abstract = {In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.}
}
@article{SIMPAO2015350,
title = {Big data and visual analytics in anaesthesia and health care†},
journal = {British Journal of Anaesthesia},
volume = {115},
number = {3},
pages = {350-356},
year = {2015},
issn = {0007-0912},
doi = {https://doi.org/10.1093/bja/aeu552},
url = {https://www.sciencedirect.com/science/article/pii/S0007091217311479},
author = {A.F. Simpao and L.M. Ahumada and M.A. Rehman},
keywords = {decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics},
abstract = {Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.}
}
@article{YAQOOB20161231,
title = {Big data: From beginning to future},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part B},
pages = {1231-1247},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216304753},
author = {Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos},
keywords = {Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics},
abstract = {Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.}
}
@incollection{MISHRA20201,
title = {Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar and Manju Khari},
booktitle = {Handbook of Data Science Approaches for Biomedical Engineering},
publisher = {Academic Press},
pages = {1-23},
year = {2020},
isbn = {978-0-12-818318-2},
doi = {https://doi.org/10.1016/B978-0-12-818318-2.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183182000015},
author = {Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta},
keywords = {Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors},
abstract = {Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.}
}
@article{BILAL2016500,
title = {Big Data in the construction industry: A review of present status, opportunities, and future trends},
journal = {Advanced Engineering Informatics},
volume = {30},
number = {3},
pages = {500-521},
year = {2016},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301938},
author = {Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha},
keywords = {Big Data Engineering, Big Data Analytics, Construction industry, Machine learning},
abstract = {The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.}
}
@article{LABRIE201845,
title = {Big data analytics sentiment: US-China reaction to data collection by business and government},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {45-55},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517308612},
author = {Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier},
keywords = {Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences},
abstract = {As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.}
}
@article{SOHRABI2018280,
title = {Systematic method for finding emergence research areas as data quality},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {280-287},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318140},
author = {Babak Sohrabi and Ahmad Khalilijafarabad},
keywords = {Data quality, Text mining, Science mapping, Data mining, Trend analysis},
abstract = {The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.}
}
@article{NORORI2021100347,
title = {Addressing bias in big data and AI for health care: A call for open science},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100347},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100347},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002026},
author = {Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara},
keywords = {artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards},
abstract = {Summary
Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.}
}
@article{ZHU2019229,
title = {The application of big data and the development of nursing science: A discussion paper},
journal = {International Journal of Nursing Sciences},
volume = {6},
number = {2},
pages = {229-234},
year = {2019},
issn = {2352-0132},
doi = {https://doi.org/10.1016/j.ijnss.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352013218305507},
author = {Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan},
keywords = {Artificial intelligence, Data mining, Knowledge bases, Nursing},
abstract = {Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.}
}
@article{RIGGINS201723,
title = {Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {23-36},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301208},
author = {Frederick J. Riggins and Bonnie K. Klamm},
keywords = {Big Data, Data governance, Self-service business intelligence},
abstract = {This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.}
}
@incollection{SEBASTIANCOLEMAN2022229,
title = {Chapter 10 - Dimensions of Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {229-256},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000109},
author = {Laura Sebastian-Coleman},
keywords = {Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling},
abstract = {This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.}
}
@article{MCKINNEY201763,
title = {The need for ‘skeptical’ accountants in the era of Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {63-80},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301051},
author = {Earl McKinney and Charles J. Yoos and Ken Snead},
keywords = {Big Data, Analysis, Informed skepticism, Questioning},
abstract = {Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.}
}
@article{WANG2017287,
title = {Exploring the path to big data analytics success in healthcare},
journal = {Journal of Business Research},
volume = {70},
pages = {287-299},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304891},
author = {Yichuan Wang and Nick Hajli},
keywords = {Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries},
abstract = {Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.}
}
@article{BROEDERS2017309,
title = {Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {309-323},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300675},
author = {Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}},
keywords = {Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands},
abstract = {Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.}
}
@article{SMIRNOVA201825,
title = {A practical guide to big data},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {25-29},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300592},
author = {Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu},
keywords = {Big data, Wearable and implantable computing, Accelerometer},
abstract = {Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.}
}
@article{WESTRA2016286,
title = {Big Data and Perioperative Nursing},
journal = {AORN Journal},
volume = {104},
number = {4},
pages = {286-292},
year = {2016},
note = {Special Focus Issue: Technology},
issn = {0001-2092},
doi = {https://doi.org/10.1016/j.aorn.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0001209216304410},
author = {Bonnie L. Westra and Jessica J. Peterson},
keywords = {big data, perioperative nursing, quality care, nursing knowledge, nursing informatics},
abstract = {Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.}
}
@article{ZHOU2016215,
title = {Big data driven smart energy management: From big data to big insights},
journal = {Renewable and Sustainable Energy Reviews},
volume = {56},
pages = {215-225},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2015.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1364032115013179},
author = {Kaile Zhou and Chao Fu and Shanlin Yang},
keywords = {Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)},
abstract = {Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.}
}
@article{LAREYRE2020e575,
title = {Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data},
journal = {Annals of Vascular Surgery},
volume = {67},
pages = {e575-e576},
year = {2020},
issn = {0890-5096},
doi = {https://doi.org/10.1016/j.avsg.2020.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0890509620303411},
author = {Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort}
}
@article{MOGHADAM2018401,
title = {Information security governance in big data environments: A systematic mapping},
journal = {Procedia Computer Science},
volume = {138},
pages = {401-408},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316909},
author = {Reza Saneei Moghadam and Ricardo Colomo-Palacios},
keywords = {information security governance, big data, framework, systematic mapping},
abstract = {Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva (English Edition)},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {2173-5727},
doi = {https://doi.org/10.1016/j.medine.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S217357271930013X},
author = {A. {Núñez Reiz}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{ELOUAZZANI201852,
title = {A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k},
journal = {Procedia Computer Science},
volume = {127},
pages = {52-59},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830108X},
author = {Zakariae {El Ouazzani} and Hanan {El Bakkali}},
keywords = {k-anonymity, quasi identifier attributes, big data, anonymization, privacy},
abstract = {Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.}
}
@article{BEN2019403,
title = {A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources},
journal = {Environmental Pollution},
volume = {253},
pages = {403-411},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930795X},
author = {YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong},
keywords = {Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai},
abstract = {An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.}
}
@article{YASSINE2019563,
title = {IoT big data analytics for smart homes with fog and cloud computing},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {563-573},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311099},
author = {Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad},
keywords = {Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes},
abstract = {Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.}
}
@article{VALENCIAPARRA2021113450,
title = {DMN4DQ: When data quality meets DMN},
journal = {Decision Support Systems},
volume = {141},
pages = {113450},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620302050},
author = {Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López},
keywords = {Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement},
abstract = {To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.}
}
@article{LEE2017293,
title = {Big data: Dimensions, evolution, impacts, and challenges},
journal = {Business Horizons},
volume = {60},
number = {3},
pages = {293-303},
year = {2017},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317300046},
author = {In Lee},
keywords = {Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics},
abstract = {Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.}
}
@article{EREVELLES2016897,
title = {Big Data consumer analytics and the transformation of marketing},
journal = {Journal of Business Research},
volume = {69},
number = {2},
pages = {897-904},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315002842},
author = {Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne},
keywords = {Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance},
abstract = {Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.}
}
@article{WANG201864,
title = {An integrated big data analytics-enabled transformation model: Application to health care},
journal = {Information & Management},
volume = {55},
number = {1},
pages = {64-79},
year = {2018},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617303129},
author = {Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski},
keywords = {Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis},
abstract = {A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.}
}
@article{NADAL201775,
title = {A software reference architecture for semantic-aware Big Data systems},
journal = {Information and Software Technology},
volume = {90},
pages = {75-92},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304287},
author = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.}
}
@article{AHMED2017459,
title = {The role of big data analytics in Internet of Things},
journal = {Computer Networks},
volume = {129},
pages = {459-471},
year = {2017},
note = {Special Issue on 5G Wireless Networks for IoT and Body Sensors},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617302591},
author = {Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos},
keywords = {Internet of Things, Big data, Analytics, Distributed computing, Smart city},
abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.}
}
@article{KHAN2017923,
title = {A survey on scholarly data: From big data perspective},
journal = {Information Processing & Management},
volume = {53},
number = {4},
pages = {923-944},
year = {2017},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316305994},
author = {Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam},
keywords = {Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications},
abstract = {Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.}
}
@article{SHOUMY2020102447,
title = {Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals},
journal = {Journal of Network and Computer Applications},
volume = {149},
pages = {102447},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303078},
author = {Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia},
keywords = {Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications},
abstract = {Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.}
}
@article{SYED2019136,
title = {Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {136-151},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18321071},
author = {Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi},
keywords = {Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors},
abstract = {In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.}
}
@article{LI2016119,
title = {Geospatial big data handling theory and methods: A review and research challenges},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {119-133},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002439},
author = {Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng},
keywords = {Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review},
abstract = {Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.}
}
@incollection{KABALCI2019309,
title = {Chapter 8 - Big data, privacy and security in smart grids},
editor = {Ersan Kabalci and Yasin Kabalci},
booktitle = {From Smart Grid to Internet of Energy},
publisher = {Academic Press},
pages = {309-333},
year = {2019},
isbn = {978-0-12-819710-3},
doi = {https://doi.org/10.1016/B978-0-12-819710-3.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197103000089},
author = {Ersan Kabalci and Yasin Kabalci},
keywords = {Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy},
abstract = {The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.}
}
@article{KOSELEVA2017544,
title = {Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges},
journal = {Procedia Engineering},
volume = {172},
pages = {544-549},
year = {2017},
note = {Modern Building Materials, Structures and Techniques},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817305702},
author = {Natalija Koseleva and Guoda Ropaite},
keywords = {Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science},
abstract = {Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.}
}
@article{GUALO2021110938,
title = {Data quality certification using ISO/IEC 25012: Industrial experiences},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110938},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110938},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000352},
author = {Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini},
keywords = {Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040},
abstract = {The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.}
}
@article{KAMBATLA20142561,
title = {Trends in big data analytics},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2561-2573},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514000057},
author = {Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama},
keywords = {Big-data, Analytics, Data centers, Distributed systems},
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.}
}
@article{ANDREOLINI201567,
title = {Adaptive, scalable and reliable monitoring of big data on clouds},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {67-79},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373151400149X},
author = {Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi},
keywords = {Adaptivity, Monitoring, Cloud computing, Big data, Scalability},
abstract = {Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.}
}
@article{NIMMAGADDA20191155,
title = {On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1155-1164},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.284},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314814},
author = {Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood},
keywords = {Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining},
abstract = {We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.}
}
@article{SHARPLES2018105,
title = {The role of statistics in the era of big data: Electronic health records for healthcare research},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {105-110},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300890},
author = {Linda D. Sharples},
keywords = {Electronic healthcare records},
abstract = {The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.}
}
@incollection{SEBASTIANCOLEMAN2022187,
title = {Chapter 9 - Core Data Quality Management Capabilities},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {187-228},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000092},
author = {Laura Sebastian-Coleman},
keywords = {Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology},
abstract = {This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.}
}
@article{VERMA2018791,
title = {An extension of the technology acceptance model in the big data analytics system implementation environment},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {791-806},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300043},
author = {Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar},
keywords = {Technology acceptance model, Big data analytics system, System quality, Information quality},
abstract = {Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.}
}
@article{KAUR20181049,
title = {Big Data and Machine Learning Based Secure Healthcare Framework},
journal = {Procedia Computer Science},
volume = {132},
pages = {1049-1059},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830752X},
author = {Prableen Kaur and Manik Sharma and Mamta Mittal},
keywords = {Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy},
abstract = {The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.}
}
@article{OUYANG201860,
title = {Methodologies, principles and prospects of applying big data in safety science research},
journal = {Safety Science},
volume = {101},
pages = {60-71},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517304320},
author = {Qiumei Ouyang and Chao Wu and Lang Huang},
keywords = {Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge},
abstract = {It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.}
}
@article{CHATFIELD2018336,
title = {Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services},
journal = {Government Information Quarterly},
volume = {35},
number = {2},
pages = {336-347},
year = {2018},
note = {Agile Government and Adaptive Governance in the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17300394},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government},
abstract = {A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.}
}
@article{OUSSOUS2018431,
title = {Big Data technologies: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {4},
pages = {431-448},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817300034},
author = {Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih},
keywords = {Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning},
abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.}
}
@article{HUANG201846,
title = {Big-data-driven safety decision-making: A conceptual framework and its influencing factors},
journal = {Safety Science},
volume = {109},
pages = {46-56},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518300973},
author = {Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang},
keywords = {Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven},
abstract = {Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.}
}
@article{WESTRA2017549,
title = {Big data science: A literature review of nursing research exemplars},
journal = {Nursing Outlook},
volume = {65},
number = {5},
pages = {549-561},
year = {2017},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2016.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0029655416303967},
author = {Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney},
keywords = {Big data, Data science, Nursing informatics, Nursing research, Nurse scientist},
abstract = {Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.}
}
@article{SHENG201797,
title = {A multidisciplinary perspective of big data in management research},
journal = {International Journal of Production Economics},
volume = {191},
pages = {97-112},
year = {2017},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S092552731730169X},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Big data, Management research, Literature review},
abstract = {In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.}
}
@article{AKOKA2017105,
title = {Research on Big Data – A systematic mapping study},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {105-115},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917300211},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi},
keywords = {Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics},
abstract = {Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.}
}
@article{RUAN2021100110,
title = {Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level},
journal = {The Lancet Regional Health - Western Pacific},
volume = {9},
pages = {100110},
year = {2021},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2666606521000195},
author = {Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu},
abstract = {Background
A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.
Methods
We performed diagnostic concept extraction and normalisation on 13•99 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years “lost” due to disability (DLE).
Findings
Our method identified a life expectancy at birth (LE0) of 77•9 years and health-adjusted life expectancy at birth (HALE0) of 71•7 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76•3 years and 68•9 years, respectively, while the female LE0 and HALE0 were 80•0 years and 74•4 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2•67, 2•15, and 1•19 years, respectively.
Interpretation
The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.
Funding
National Key R and D Program of China (2018YFC2000400).}
}
@article{LI2019103149,
title = {Experience and reflection from China’s Xiangya medical big data project},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103149},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103149},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930067X},
author = {Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan},
keywords = {Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre},
abstract = {The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.}
}
@incollection{WHELAN2020365,
title = {Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {365-377},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00027-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000277},
author = {Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie},
keywords = {Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods},
abstract = {The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.}
}
@article{TCHAGNAKOUANOU201868,
title = {An optimal big data workflow for biomedical image analysis},
journal = {Informatics in Medicine Unlocked},
volume = {11},
pages = {68-74},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300844},
author = {Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda},
keywords = {Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark},
abstract = {Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.}
}
@article{SANTOSO201793,
title = {Data Warehouse with Big Data Technology for Higher Education},
journal = {Procedia Computer Science},
volume = {124},
pages = {93-99},
year = {2017},
note = {4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917329022},
author = {Leo Willyanto Santoso and  Yulia},
keywords = {Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis},
abstract = {Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.}
}
@article{VERBRUGGHE2019298,
title = {The electronic medical record: Big data, little information?},
journal = {Journal of Critical Care},
volume = {54},
pages = {298-299},
year = {2019},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883944119313723},
author = {Walter Verbrugghe and Kirsten Colpaert}
}
@article{YAN2018457,
title = {Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors},
journal = {Ocean Engineering},
volume = {169},
pages = {457-468},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818316421},
author = {Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn},
keywords = {Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop},
abstract = {Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.}
}
@article{SCOTT201820,
title = {The role of Statistics in the era of big data: Crucial, critical and under-valued},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {20-24},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300956},
author = {E. Marian Scott},
keywords = {Data, Sampling, Variability, Inference, Uncertainty},
abstract = {What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.}
}
@article{LOFGREN2021R1312,
title = {Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes},
journal = {Current Biology},
volume = {31},
number = {19},
pages = {R1312-R1325},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S096098222100912X},
author = {Lotus A. Lofgren and Jason E. Stajich},
abstract = {Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{WIBISONO201929,
title = {Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction},
journal = {Knowledge-Based Systems},
volume = {176},
pages = {29-39},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119301443},
author = {Ari Wibisono and Devvi Sarwinda},
keywords = {ARDEV, Big data prediction, FIMT-DD, Tree regression},
abstract = {Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.}
}
@article{AKHAVANHEJAZI201891,
title = {Power systems big data analytics: An assessment of paradigm shift barriers and prospects},
journal = {Energy Reports},
volume = {4},
pages = {91-100},
year = {2018},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352484717300616},
author = {Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad},
keywords = {Energy, Big data analytics, Internet of energy, Smart grid},
abstract = {Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.}
}
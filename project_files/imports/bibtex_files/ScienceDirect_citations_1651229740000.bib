@article{GARCIABERNARDO2018164,
title = {The effects of data quality on the analysis of corporate board interlock networks},
journal = {Information Systems},
volume = {78},
pages = {164-172},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302272},
author = {Javier Garcia-Bernardo and Frank W. Takes},
abstract = {Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.}
}
@article{PRODHAN2022105327,
title = {A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions},
journal = {Environmental Modelling & Software},
volume = {149},
pages = {105327},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105327},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222000330},
author = {Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana},
keywords = {Machine learning, Deep learning, Forecasting, Drought, Big data},
abstract = {Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.}
}
@article{DO2020100018,
title = {Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition},
journal = {Asian Transport Studies},
volume = {6},
pages = {100018},
year = {2020},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2020.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2185556020300183},
author = {Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara},
keywords = {Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization},
abstract = {The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.}
}
@incollection{BERMAN2013183,
title = {Chapter 13 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {183-199},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767000137},
author = {Jules J. Berman},
keywords = {Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.}
}
@article{PUTHAL201722,
title = {A dynamic prime number based efficient security mechanism for big sensing data streams},
journal = {Journal of Computer and System Sciences},
volume = {83},
number = {1},
pages = {22-42},
year = {2017},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000209},
author = {Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen},
keywords = {Security, Sensor networks, Big data stream, Key exchange, Security verification},
abstract = {Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.}
}
@article{D2021,
title = {A study on artificial intelligence for monitoring smart environments},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321043911},
author = {Karthika D.},
keywords = {Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies},
abstract = {Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.}
}
@article{FUCHS2014198,
title = {Big data analytics for knowledge generation in tourism destinations – A case from Sweden},
journal = {Journal of Destination Marketing & Management},
volume = {3},
number = {4},
pages = {198-209},
year = {2014},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X14000353},
author = {Matthias Fuchs and Wolfram Höpken and Maria Lexhagen},
keywords = {Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)},
abstract = {This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.}
}
@article{OYEDELE2021100158,
title = {Machine learning predictions for lost time injuries in power transmission and distribution projects},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100158},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100158},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000797},
author = {Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele},
keywords = {Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data},
abstract = {Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.}
}
@article{REIS2020232,
title = {Assessing the drivers of machine learning business value},
journal = {Journal of Business Research},
volume = {117},
pages = {232-243},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320303581},
author = {Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro},
keywords = {Machine learning, Business value, Competitive advantage, Dynamic capabilities theory},
abstract = {Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.}
}
@article{LI202156,
title = {Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {56-60},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266710262100005X},
author = {Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui},
keywords = {Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database},
abstract = {After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.}
}
@article{HUARD201518,
title = {The data quality paradox},
journal = {Network Security},
volume = {2015},
number = {6},
pages = {18-20},
year = {2015},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(15)30051-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485815300519},
author = {Boris Huard},
abstract = {After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?}
}
@article{CHO2022102477,
title = {What's driving the diffusion of next-generation digital technologies?},
journal = {Technovation},
pages = {102477},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000244},
author = {Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik},
abstract = {The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.}
}
@article{ZHOU2021103342,
title = {Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation},
journal = {Journal of Energy Storage},
volume = {44},
pages = {103342},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103342},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21010331},
author = {Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng},
keywords = {Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction},
abstract = {This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.}
}
@article{SHET2021311,
title = {Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications},
journal = {Journal of Business Research},
volume = {131},
pages = {311-326},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002174},
author = {Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi},
keywords = {Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis},
abstract = {Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.}
}
@article{BUI2021109392,
title = {Advanced data analytics for ship performance monitoring under localized operational conditions},
journal = {Ocean Engineering},
volume = {235},
pages = {109392},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109392},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008040},
author = {Khanh Q. Bui and Lokukaluge P. Perera},
keywords = {Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection},
abstract = {Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.}
}
@article{RIVAS201794,
title = {Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {94-104},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301192},
author = {Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini},
keywords = {Master data, Data quality, ISO 8000, Big data},
abstract = {During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.}
}
@article{KNEPPER201792,
title = {Forward Observer system for radar data workflows: Big data management in the field},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {92-97},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17310567},
author = {Richard Knepper and Matthew Standish},
keywords = {Microcomputers, Information storage, Physical sciences and engineering},
abstract = {There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.}
}
@article{AMEEN2021106761,
title = {Consumer interaction with cutting-edge technologies: Implications for future research},
journal = {Computers in Human Behavior},
volume = {120},
pages = {106761},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106761},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221000832},
author = {Nisreen Ameen and Sameer Hosany and Ali Tarhini},
keywords = {Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics},
abstract = {This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.}
}
@incollection{SHANG2022203,
title = {Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {203-228},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000087},
author = {Wen-Long Shang and Haoran Zhang and Yi Sui},
keywords = {MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19},
abstract = {This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.}
}
@article{MARSDEN2019113172,
title = {Perspectives on numerical data quality in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113172},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113172},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302015},
author = {James R. Marsden and David E. Pingry and Jason B. Thatcher}
}
@article{SZYMANSKA20181,
title = {Modern data science for analytical chemical data – A comprehensive review},
journal = {Analytica Chimica Acta},
volume = {1028},
pages = {1-10},
year = {2018},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2018.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0003267018306421},
author = {Ewa Szymańska},
keywords = {Chemometrics, Data science, Big data, Chemical analytical data, Methodology},
abstract = {Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.}
}
@article{JUNG2020112,
title = {Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)},
journal = {Lung Cancer},
volume = {146},
pages = {112-119},
year = {2020},
issn = {0169-5002},
doi = {https://doi.org/10.1016/j.lungcan.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0169500220304670},
author = {Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park},
keywords = {Real-time updated system, Big data, Real-world data, NSCLC, Treatment},
abstract = {Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.}
}
@article{WANG2021189,
title = {Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework},
journal = {Process Safety and Environmental Protection},
volume = {148},
pages = {189-199},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020318000},
author = {Bing Wang},
keywords = {Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making},
abstract = {In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.}
}
@article{FRANCIA2021299,
title = {Making data platforms smarter with MOSES},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {299-313},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002260},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini},
keywords = {Data lake, Metadata, Big data, Data platform},
abstract = {The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.}
}
@article{KARKOUCH201657,
title = {Data quality in internet of things: A state-of-the-art survey},
journal = {Journal of Network and Computer Applications},
volume = {73},
pages = {57-81},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301564},
author = {Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel},
keywords = {Internet of things, Data quality, Data cleaning, Outlier detection},
abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.}
}
@article{DONG2015278,
title = {Traffic zone division based on big data from mobile phone base stations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {278-291},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15002223},
author = {Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou},
keywords = {Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns},
abstract = {Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.}
}
@article{KEBISEK202011168,
title = {Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11168-11174},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.299},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320305796},
author = {M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec},
keywords = {artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis},
abstract = {This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.}
}
@article{EVANGELISTA2018112,
title = {Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny},
journal = {Molecular Phylogenetics and Evolution},
volume = {128},
pages = {112-122},
year = {2018},
issn = {1055-7903},
doi = {https://doi.org/10.1016/j.ympev.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1055790318300186},
author = {Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre},
keywords = {Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis},
abstract = {Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.}
}
@article{KOLOSSA2018775,
title = {Data quality over data quantity in computational cognitive neuroscience},
journal = {NeuroImage},
volume = {172},
pages = {775-785},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300053},
author = {Antonio Kolossa and Bruno Kopp},
keywords = {Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability},
abstract = {We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.}
}
@article{NEETHIRAJAN2021100408,
title = {Digital Livestock Farming},
journal = {Sensing and Bio-Sensing Research},
volume = {32},
pages = {100408},
year = {2021},
issn = {2214-1804},
doi = {https://doi.org/10.1016/j.sbsr.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2214180421000131},
author = {Suresh Neethirajan and Bas Kemp},
keywords = {Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture},
abstract = {As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.}
}
@article{DAVIS2017224,
title = {Residential land values in the Washington, DC metro area: New insights from big data},
journal = {Regional Science and Urban Economics},
volume = {66},
pages = {224-246},
year = {2017},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166046216301508},
author = {Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka},
keywords = {Land, Housing, House prices, Housing boom and bust, Financial crisis},
abstract = {We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.}
}
@incollection{TALBURT2015191,
title = {Chapter 11 - ISO Data Quality Standards for Master Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {191-205},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000119},
author = {John R. Talburt and Yinle Zhou},
keywords = {ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding},
abstract = {This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.}
}
@incollection{FAULKNER202081,
title = {4 - Data Fundamentals},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {81-92},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000176},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality},
abstract = {Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.}
}
@article{HARRIGAN2021102246,
title = {Identifying influencers on social media},
journal = {International Journal of Information Management},
volume = {56},
pages = {102246},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314456},
author = {Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers},
keywords = {Influencers, Market mavens, Big data, Social media, Twitter},
abstract = {The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.}
}
@article{KIM201818,
title = {Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability},
journal = {Computers in Human Behavior},
volume = {86},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830178X},
author = {Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis},
keywords = {Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory},
abstract = {The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.}
}
@article{HEINRICH201895,
title = {Assessing data quality – A probability-based metric for semantic consistency},
journal = {Decision Support Systems},
volume = {110},
pages = {95-106},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300599},
author = {Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner},
keywords = {Data quality, Data quality assessment, Data quality metric, Data consistency},
abstract = {We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.}
}
@article{STOGER2021105587,
title = {Legal aspects of data cleansing in medical AI},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105587},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105587},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000601},
author = {Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger},
keywords = {Data cleansing, Data quality, Medical AI, Medical devices},
abstract = {Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to "dirty data" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.}
}
@article{FAN2021123651,
title = {The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {123651},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123651},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336969},
author = {Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo},
keywords = {Internet of things in agriculture, Big data, High-throughput phenotype, Data mining},
abstract = {With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.}
}
@article{AHMAD2021125834,
title = {Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities},
journal = {Journal of Cleaner Production},
volume = {289},
pages = {125834},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125834},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621000548},
author = {Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen},
keywords = {Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization},
abstract = {The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.}
}
@article{MADHIKERMI2016145,
title = {Data quality assessment of maintenance reporting procedures},
journal = {Expert Systems with Applications},
volume = {63},
pages = {145-164},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S095741741630330X},
author = {Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling},
keywords = {Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance},
abstract = {Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.}
}
@article{BELLINI2019521,
title = {Data quality and blockchain technology},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {5},
pages = {521-522},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818305368},
author = {Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami},
keywords = {Machine learning, Artificial intelligence, Blockchain technology}
}
@article{HOSEINZADEH2020101518,
title = {Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101518},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101518},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302519},
author = {Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar},
keywords = {Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets},
abstract = {Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.}
}
@article{LI20189,
title = {Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China},
journal = {Computers, Environment and Urban Systems},
volume = {70},
pages = {9-23},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517301916},
author = {Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang},
keywords = {Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification},
abstract = {Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.}
}
@article{LIU201990,
title = {Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {2},
pages = {90-102},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003579},
url = {https://www.sciencedirect.com/science/article/pii/S1001929419300318},
author = {Daowen Liu and Liqi Lei and Tong Ruan and Ping He},
keywords = {electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study},
abstract = {Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.}
}
@article{ALWIS2022103624,
title = {A survey on smart farming data, applications and techniques},
journal = {Computers in Industry},
volume = {138},
pages = {103624},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103624},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000197},
author = {Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar},
keywords = {Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry},
abstract = {The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.}
}
@incollection{SEBASTIANCOLEMAN2013173,
title = {Chapter 13 - Directives for Data Quality Strategy},
editor = {Laura Sebastian-Coleman},
booktitle = {Measuring Data Quality for Ongoing Improvement},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {173-192},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397033-6},
doi = {https://doi.org/10.1016/B978-0-12-397033-6.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970336000146},
author = {Laura Sebastian-Coleman}
}
@article{ZHANG2021101336,
title = {A framework of energy-consumption driven discrete manufacturing system},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101336},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003465},
author = {Tao Zhang and Weixi Ji and Yongtao Qiu},
keywords = {Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining},
abstract = {Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.}
}
@incollection{WANG2021295,
title = {Chapter 13 - Artificial Intelligence for Flood Observation},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {295-304},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000134},
author = {Ruo-Qian Wang},
keywords = {artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video},
abstract = {Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.}
}
@article{YANG2021,
title = {Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence},
journal = {Intelligent Medicine},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621001200},
author = {Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin},
keywords = {Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management},
abstract = {Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.}
}
@article{VIDAURRE2018646,
title = {Discovering dynamic brain networks from big data in rest and task},
journal = {NeuroImage},
volume = {180},
pages = {646-656},
year = {2018},
note = {Brain Connectivity Dynamics},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.06.077},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917305487},
author = {Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich},
abstract = {Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.}
}
@article{SADIQ2017150,
title = {Open data: Quality over quantity},
journal = {International Journal of Information Management},
volume = {37},
number = {3},
pages = {150-154},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216309021},
author = {Shazia Sadiq and Marta Indulska},
keywords = {Open data, Data quality},
abstract = {Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.}
}
@article{WANG2022e97,
title = {Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care},
journal = {Clinical Oncology},
volume = {34},
number = {2},
pages = {e97-e103},
year = {2022},
note = {Artificial Intelligence in Radiation Therapy},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2021.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0936655521004593},
author = {J.W. Wang and M. Williams},
keywords = {Artificial intelligence, Big Data, database, deep learning, registries, repository},
abstract = {Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.}
}
@article{LOOTEN2019104825,
title = {What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104825},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718307089},
author = {Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance},
keywords = {Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems},
abstract = {Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).}
}
@article{JAGATHEESAPERUMAL2022107691,
title = {A holistic survey on the use of emerging technologies to provision secure healthcare solutions},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107691},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107691},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000131},
author = {Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan},
keywords = {Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks},
abstract = {Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.}
}
@article{R2020235,
title = {Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking},
journal = {Computer Communications},
volume = {150},
pages = {235-244},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419311673},
author = {Gifty R. and Bharathi R.},
keywords = {Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time},
abstract = {Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.}
}
@article{BUFFAT2017277,
title = {Big data GIS analysis for novel approaches in building stock modelling},
journal = {Applied Energy},
volume = {208},
pages = {277-290},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S030626191731454X},
author = {René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg},
keywords = {Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling},
abstract = {Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.}
}
@article{BRONSELAER201895,
title = {An incremental approach for data quality measurement with insufficient information},
journal = {International Journal of Approximate Reasoning},
volume = {96},
pages = {95-111},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17307478},
author = {A. Bronselaer and J. Nielandt and G. {De Tré}},
keywords = {Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory},
abstract = {Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.}
}
@article{WIBISONO201633,
title = {Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {33-46},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004165},
author = {Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto},
keywords = {Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization},
abstract = {Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.}
}
@article{THOMAS2021101994,
title = {Advances in monitoring and evaluation in low- and middle-income countries},
journal = {Evaluation and Program Planning},
volume = {89},
pages = {101994},
year = {2021},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2021.101994},
url = {https://www.sciencedirect.com/science/article/pii/S0149718921000896},
author = {James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar},
keywords = {Monitoring and evaluation, Health information systems, Developing countries},
abstract = {Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.}
}
@article{VALENCA2021100008,
title = {Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs},
journal = {Journal of Urban Mobility},
volume = {1},
pages = {100008},
year = {2021},
issn = {2667-0917},
doi = {https://doi.org/10.1016/j.urbmob.2021.100008},
url = {https://www.sciencedirect.com/science/article/pii/S266709172100008X},
author = {Gabriel Valença and Filipe Moura and Ana {Morais de Sá}},
keywords = {Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning},
abstract = {Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.}
}
@article{SHENG2019321,
title = {Technology in the 21st century: New challenges and opportunities},
journal = {Technological Forecasting and Social Change},
volume = {143},
pages = {321-335},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311319},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making},
abstract = {Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.}
}
@article{KHALOUFI2018294,
title = {Security model for Big Healthcare Data Lifecycle},
journal = {Procedia Computer Science},
volume = {141},
pages = {294-301},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318520},
author = {Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi},
keywords = {Big data Security, Big data in healthcare, Big data lifecycle, Security threat model},
abstract = {Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.}
}
@article{DUVIER2018358,
title = {Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities},
journal = {Sustainable Cities and Society},
volume = {39},
pages = {358-365},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717312520},
author = {Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava},
keywords = {Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence},
abstract = {Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.}
}
@article{KREGEL2021107083,
title = {Process Mining for Six Sigma: Utilising Digital Traces},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107083},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107083},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220307531},
author = {I. Kregel and D. Stemann and J. Koch and A. Coners},
keywords = {process mining, six sigma, DMAIC, big data analytics, data science, project management},
abstract = {Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.}
}
@article{LOPEZ20162128,
title = {Data Quality Control for St. Petersburg Flood Warning System},
journal = {Procedia Computer Science},
volume = {80},
pages = {2128-2140},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916310225},
author = {Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov},
keywords = {outliers, quality-control, principal components, gap filling},
abstract = {This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).}
}
@incollection{MCKNIGHT201432,
title = {Chapter Four - Data Quality: Passing the Standard},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {32-43},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000047},
author = {William McKnight},
keywords = {data quality, referential integrity, system of origination, data profiling},
abstract = {We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.}
}
@article{LIONO2019196,
title = {QDaS: Quality driven data summarisation for effective storage management in Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {127},
pages = {196-208},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S074373151830220X},
author = {Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim},
keywords = {Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation},
abstract = {The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.}
}
@article{FARROKHI2020257,
title = {Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence},
journal = {Industrial Marketing Management},
volume = {91},
pages = {257-273},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308464},
author = {Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi},
keywords = {Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics},
abstract = {Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.}
}
@article{SONG201734,
title = {Data quality management for service-oriented manufacturing cyber-physical systems},
journal = {Computers & Electrical Engineering},
volume = {64},
pages = {34-44},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302099},
author = {Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang},
keywords = {Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets},
abstract = {Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.}
}
@article{ENCINAS2022109904,
title = {Downhole data correction for data-driven rate of penetration prediction modeling},
journal = {Journal of Petroleum Science and Engineering},
volume = {210},
pages = {109904},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015217},
author = {Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui},
keywords = {Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks},
abstract = {In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.}
}
@article{VISSER2021623,
title = {Imprecision farming? Examining the (in)accuracy and risks of digital agriculture},
journal = {Journal of Rural Studies},
volume = {86},
pages = {623-632},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002217},
author = {Oane Visser and Sarah Ruth Sippel and Louis Thiemann},
keywords = {Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data},
abstract = {The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.}
}
@article{CLARKE2018467,
title = {Guidelines for the responsible application of data analytics},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {467-476},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303643},
author = {Roger Clarke},
keywords = {Big data, Data science, Data quality, Decision quality, Regulation},
abstract = {The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.}
}
@article{SALVETAT2020101602,
title = {Data determinants of the activity of SMEs automobile dealers},
journal = {Journal of Engineering and Technology Management},
volume = {58},
pages = {101602},
year = {2020},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2020.101602},
url = {https://www.sciencedirect.com/science/article/pii/S0923474820300503},
author = {David Salvetat and Jean-Sébastien Lacam},
keywords = {Big data, Smart data, Development, Automobile, SME},
abstract = {Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.}
}
@article{SCHAEFER2021156,
title = {Framework of Data Analytics and Integrating Knowledge Management},
journal = {International Journal of Intelligent Networks},
volume = {2},
pages = {156-165},
year = {2021},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666603021000208},
author = {Camilla Schaefer and Ana Makatsaria},
keywords = {Data analytics, Knowledge management, Big data, Business intelligence, Data discovery},
abstract = {Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.}
}
@article{SHARMA2020538,
title = {MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {538-550},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.05.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322173},
author = {Shivani Sharma and Durga Toshniwal},
keywords = {Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability},
abstract = {Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.}
}
@article{XIONG2021386,
title = {Anti-collusion data auction mechanism based on smart contract},
journal = {Information Sciences},
volume = {555},
pages = {386-409},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520310458},
author = {Wei Xiong and Li Xiong},
keywords = {Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum},
abstract = {Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.}
}
@article{DIVAIO2022121201,
title = {Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121201},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121201},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100634X},
author = {Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine},
keywords = {Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance},
abstract = {This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.}
}
@incollection{TAT2021395,
title = {Chapter 17 - Ethical and legal challenges},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {395-410},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000178},
author = {Emily Tat and Mark Rabbat},
keywords = {Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety},
abstract = {As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.}
}
@article{XIA2021100055,
title = {Aiding pro-environmental behavior measurement by Internet of Things},
journal = {Current Research in Behavioral Sciences},
volume = {2},
pages = {100055},
year = {2021},
issn = {2666-5182},
doi = {https://doi.org/10.1016/j.crbeha.2021.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2666518221000425},
author = {Ziqian Xia and Yurong Liu},
keywords = {Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology},
abstract = {Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.}
}
@article{EZERINS2022105569,
title = {Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health},
journal = {Safety Science},
volume = {146},
pages = {105569},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004112},
author = {Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz},
keywords = {Safety analytics, Data analytics, Readiness assessment, Occupational health},
abstract = {Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.}
}
@incollection{BERMAN2018395,
title = {19 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {395-417},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000194},
author = {Jules J. Berman},
keywords = {Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.}
}
@incollection{VAIDYA2022409,
title = {Chapter 24 - Exploring performance and predictive analytics of agriculture data},
editor = {Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani},
booktitle = {AI, Edge and IoT-based Smart Agriculture},
publisher = {Academic Press},
pages = {409-436},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823694-9},
doi = {https://doi.org/10.1016/B978-0-12-823694-9.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823694900030X},
author = {Madhavi Vaidya and Shweta Katkar},
keywords = {Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction},
abstract = {The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.}
}
@article{SIRGO2018166,
title = {Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {166-172},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618300443},
author = {Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí},
keywords = {Electronic medical record, Quality indicators, Critical care, Information processing, Data quality, Verification},
abstract = {Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.}
}
@article{OLEARY2019113139,
title = {Technology life cycle and data quality: Action and triangulation},
journal = {Decision Support Systems},
volume = {126},
pages = {113139},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113139},
url = {https://www.sciencedirect.com/science/article/pii/S016792361930168X},
author = {Daniel E. O'Leary},
keywords = {Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation},
abstract = {Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.}
}
@article{WANG2022643,
title = {Does city construction improve life quality?-evidence from POI data of China},
journal = {International Review of Economics & Finance},
volume = {80},
pages = {643-653},
year = {2022},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1059056022000041},
author = {Yang Wang and Hong Zhang and Libing Liu},
keywords = {Quality of life, Point of interest, Happiness},
abstract = {To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the "clogging point" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for "meeting the people's increasing needs for a better life".}
}
@incollection{KOLTAY201671,
title = {Chapter 5 - Digital Research Data: Where are we Now?},
editor = {David Baker and Wendy Evans},
booktitle = {Digital Information Strategies},
publisher = {Chandos Publishing},
pages = {71-84},
year = {2016},
isbn = {978-0-08-100251-3},
doi = {https://doi.org/10.1016/B978-0-08-100251-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002513000056},
author = {Tibor Koltay},
keywords = {data citation, data curation, data literacy, data management, data quality, data sharing, research data},
abstract = {The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.}
}
@article{DUPLESSIS2021100100,
title = {Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa},
journal = {Environmental Challenges},
volume = {4},
pages = {100100},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000792},
author = {Anja {du Plessis}},
keywords = {Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment},
abstract = {The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.}
}
@article{MACHADO2022263,
title = {Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures},
journal = {Procedia Computer Science},
volume = {196},
pages = {263-271},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022365},
author = {Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos},
keywords = {Big Data, Data Mesh, Data Architectures, Data Lake},
abstract = {Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.}
}
@article{ZORRILLA2022103595,
title = {A reference framework for the implementation of data governance systems for industry 4.0},
journal = {Computer Standards & Interfaces},
volume = {81},
pages = {103595},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000908},
author = {Marta Zorrilla and Juan Yebenes},
keywords = {Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT},
abstract = {The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.}
}
@article{RAMSINGH2021107423,
title = {An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm},
journal = {Applied Soft Computing},
volume = {108},
pages = {107423},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107423},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100346X},
author = {J. Ramsingh and V. Bhuvaneswari},
keywords = {Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus},
abstract = {In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.}
}
@article{WANG2018139,
title = {Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data},
journal = {Computers & Geosciences},
volume = {111},
pages = {139-147},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730609X},
author = {Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw},
abstract = {Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.}
}
@article{MARTINEZ2021100183,
title = {Data Science Methodologies: Current Challenges and Future Approaches},
journal = {Big Data Research},
volume = {24},
pages = {100183},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300514},
author = {Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola}},
keywords = {Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management},
abstract = {Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.}
}
@incollection{REIS2020179,
title = {3.10 - Data Quality and Denoising: A Review☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {179-204},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14874-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472148747},
author = {M.S. Reis and P.M. Saraiva and B.R. Bakshi},
keywords = {Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis},
abstract = {This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.}
}
@article{SAEZ2019104954,
title = {Guest editorial: Special issue in biomedical data quality assessment methods},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104954},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719309174},
author = {Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez}
}
@incollection{CHUI2019111,
title = {Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {111-124},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166390000077},
author = {Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu},
keywords = {Cyber security, Ethics, Policy-making, Security, Surveillance},
abstract = {Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.}
}
@article{KAZMIERSKA202043,
title = {From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community},
journal = {Radiotherapy and Oncology},
volume = {153},
pages = {43-54},
year = {2020},
note = {Physics Special Issue: ESTRO Physics Research Workshops on Science in Development},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2020.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S016781402030829X},
author = {Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso},
keywords = {Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making},
abstract = {Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.}
}
@article{GANT2015S36,
title = {The importance of data quality to enhance the impact of omics sciences},
journal = {Toxicology Letters},
volume = {238},
number = {2, Supplement },
pages = {S36},
year = {2015},
note = {ABSTRACTS OF THE 51st Congress of the European Societies of Toxicology (EUROTOX)},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2015.08.097},
url = {https://www.sciencedirect.com/science/article/pii/S0378427415020378},
author = {T. Gant}
}
@article{STECKLER20151803,
title = {The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {10},
pages = {1803-1807},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15001674},
author = {Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov},
keywords = {Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical},
abstract = {Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.}
}
@article{LIU2020101495,
title = {Discovering and merging related analytic datasets},
journal = {Information Systems},
volume = {91},
pages = {101495},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101495},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300065},
author = {Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski},
keywords = {Schema augmentation, Schema complement, Data quality, SAP HANA},
abstract = {The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.}
}
@article{EICHSTADT2021100232,
title = {Metrology for the digital age},
journal = {Measurement: Sensors},
volume = {18},
pages = {100232},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421001951},
author = {Sascha Eichstädt and Anke Keidel and Julia Tesch},
keywords = {Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR},
abstract = {Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.}
}
@article{MERINO2016123,
title = {A Data Quality in Use model for Big Data},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {123-130},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003817},
author = {Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini},
keywords = {Data Quality, Big Data, Measurement, Quality-in-Use, Model},
abstract = {Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.}
}
@article{SHARDT2020104,
title = {Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {104-113},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303591},
author = {Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov},
keywords = {data quality assessment, system identification, big data, Industry 4.0, soft sensors},
abstract = {As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.}
}
@article{LI2021441,
title = {A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong},
journal = {Environmental Science & Policy},
volume = {124},
pages = {441-450},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001714},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow},
keywords = {Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement},
abstract = {All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.}
}
@article{LEE2022,
title = {Quality assurance of integrative big data for medical research within a multihospital system},
journal = {Journal of the Formosan Medical Association},
year = {2022},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2021.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092966462100591X},
author = {Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang},
keywords = {Big data, Electronic health record, Evidence based healthcare management, Validation study},
abstract = {Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.}
}
@article{CREMIN2022138,
title = {Big data: Historic advances and emerging trends in biomedical research},
journal = {Current Research in Biotechnology},
volume = {4},
pages = {138-151},
year = {2022},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2590262822000090},
author = {Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang},
keywords = {Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine},
abstract = {Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.}
}
@article{ZHANG202134,
title = {Big data and human resource management research: An integrative review and new directions for future research},
journal = {Journal of Business Research},
volume = {133},
pages = {34-50},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002563},
author = {Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang},
keywords = {Human resource management research, Big data, Integrative review, Inductive and deductive paradigms},
abstract = {The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.}
}
@article{KEDDY2022e130,
title = {Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications},
journal = {The Lancet Infectious Diseases},
volume = {22},
number = {5},
pages = {e130-e142},
year = {2022},
issn = {1473-3099},
doi = {https://doi.org/10.1016/S1473-3099(21)00585-5},
url = {https://www.sciencedirect.com/science/article/pii/S1473309921005855},
author = {Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke},
abstract = {Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.}
}
@article{IBRAHIM2021121171,
title = {The convergence of big data and accounting: innovative research opportunities},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121171},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121171},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006041},
author = {Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat},
keywords = {Big data, Analytics, Accounting, Data science, Business intelligence},
abstract = {This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.}
}
@article{BRAVE2021,
title = {The perils of working with big data, and a SMALL checklist you can use to recognize them},
journal = {Business Horizons},
year = {2021},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681321001178},
author = {Scott A. Brave and R. Andrew Butters and Michael Fogarty},
keywords = {Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator},
abstract = {The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.}
}
@article{TANG2022,
title = {The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review},
journal = {World Neurosurgery},
year = {2022},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2022.02.113},
url = {https://www.sciencedirect.com/science/article/pii/S1878875022002601},
author = {Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms},
keywords = {Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS},
abstract = {Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.}
}
@article{HU2021107994,
title = {A blockchain-based trading system for big data},
journal = {Computer Networks},
volume = {191},
pages = {107994},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107994},
url = {https://www.sciencedirect.com/science/article/pii/S138912862100116X},
author = {Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng},
keywords = {Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward},
abstract = {Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.}
}
@article{ARFANUZZAMAN2021100127,
title = {Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia},
journal = {Environmental and Sustainability Indicators},
volume = {11},
pages = {100127},
year = {2021},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2021.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2665972721000283},
author = {Md. Arfanuzzaman},
keywords = {Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation},
abstract = {Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.}
}
@article{GHASEMAGHAEI201938,
title = {Can big data improve firm decision quality? The role of data quality and data diagnosticity},
journal = {Decision Support Systems},
volume = {120},
pages = {38-49},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300466},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data utilization, Data quality, Decision quality, Data diagnosticity},
abstract = {Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.}
}
@article{CARRA2020300,
title = {Data-driven ICU management: Using Big Data and algorithms to improve outcomes},
journal = {Journal of Critical Care},
volume = {60},
pages = {300-304},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306791},
author = {Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt},
keywords = {Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit},
abstract = {The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.}
}
@article{SHAMIM2021106777,
title = {Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106777},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106777},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100100X},
author = {Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah},
keywords = {Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality},
abstract = {Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.}
}
@article{LEON2021100253,
title = {Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data},
journal = {Big Data Research},
volume = {26},
pages = {100253},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000708},
author = {Ana León and Óscar Pastor},
keywords = {Big Data, Genomics, Computer science, Theory and methods},
abstract = {The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.}
}
@article{GU2021132,
title = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {132-147},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001246},
author = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
keywords = {Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data},
abstract = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.}
}
@article{TALHA2019916,
title = {Big Data: Trade-off between Data Quality and Data Security},
journal = {Procedia Computer Science},
volume = {151},
pages = {916-922},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305915},
author = {M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI},
keywords = {Big Data, Data Quality, Data Security, Trade-off between Quality, Security},
abstract = {The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.}
}
@article{HUANG2021101712,
title = {Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101712},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101712},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001198},
author = {Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang},
keywords = {Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata},
abstract = {The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.}
}
@article{MOSTEFAOUI2022,
title = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
journal = {Future Generation Computer Systems},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
author = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{YANG202223,
title = {Big data and reference intervals},
journal = {Clinica Chimica Acta},
volume = {527},
pages = {23-32},
year = {2022},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0009898122000018},
author = {Dan Yang and Zihan Su and Min Zhao},
keywords = {Indirect method, Reference interval, Data pre-processing, Verification, Big data},
abstract = {Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{RHAHLA2021102896,
title = {Guidelines for GDPR compliance in Big Data systems},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102896},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102896},
url = {https://www.sciencedirect.com/science/article/pii/S221421262100123X},
author = {Mouna Rhahla and Sahar Allegue and Takoua Abdellatif},
keywords = {The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security},
abstract = {The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.}
}
@article{LI2021241,
title = {AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making},
journal = {Environmental Science & Policy},
volume = {125},
pages = {241-246},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121002471},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui},
abstract = {AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.}
}
@article{WILKIN2020100470,
title = {Big data prioritization in SCM decision-making: Its role and performance implications},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100470},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100470},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300385},
author = {Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan},
keywords = {Big data, Big data availability, Big data prioritization, Supply chain management, Performance},
abstract = {Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.}
}
@article{WEERASINGHE2022121222,
title = {Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121222},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121222},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006557},
author = {Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin},
keywords = {Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory},
abstract = {The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.}
}
@article{MISHRA20216864,
title = {An efficient approach for manufacturing process using Big data analytics},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6864-6866},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321037226},
author = {Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma},
keywords = {Manufacturing process, Bigdata, Structured data, Unstructured data},
abstract = {Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.}
}
@article{GODOY2021100079,
title = {Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture},
journal = {Energy and AI},
volume = {5},
pages = {100079},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000331},
author = {Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft},
keywords = {Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data},
abstract = {In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.}
}
@article{ARDAGNA2018548,
title = {Context-aware data quality assessment for big data},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {548-562},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329151},
author = {Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali},
keywords = {Data quality, Big data, Context-awareness, Data profiling, DQ assessment},
abstract = {Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.}
}
@article{PAL2020100869,
title = {Big data in biology: The hope and present-day challenges in it},
journal = {Gene Reports},
volume = {21},
pages = {100869},
year = {2020},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2020.100869},
url = {https://www.sciencedirect.com/science/article/pii/S2452014420302831},
author = {Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh},
keywords = {Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning},
abstract = {The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.}
}
@article{ULLAH2022103294,
title = {On the scalability of Big Data Cyber Security Analytics systems},
journal = {Journal of Network and Computer Applications},
volume = {198},
pages = {103294},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002897},
author = {Faheem Ullah and M. Ali Babar},
keywords = {Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark},
abstract = {Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.}
}
@article{ALAOUI2019803,
title = {The Impact of Big Data Quality on Sentiment Analysis Approaches},
journal = {Procedia Computer Science},
volume = {160},
pages = {803-810},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317077},
author = {Imane El Alaoui and Youssef Gahi},
keywords = {Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining},
abstract = {Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.}
}
@article{MUHEIDAT202215,
title = {Emerging Concepts Using Blockchain and Big Data},
journal = {Procedia Computer Science},
volume = {198},
pages = {15-22},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024455},
author = {Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh},
keywords = {Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare},
abstract = {Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.}
}
@article{TANG2022100289,
title = {Big Data in Forecasting Research: A Literature Review},
journal = {Big Data Research},
volume = {27},
pages = {100289},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621001064},
author = {Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang},
keywords = {Big data, Forecasting, Literature review, Prediction models, Information},
abstract = {With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.}
}
@article{FUGINI2021100192,
title = {A Big Data Analytics Architecture for Smart Cities and Smart Companies},
journal = {Big Data Research},
volume = {24},
pages = {100192},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000095},
author = {Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli},
keywords = {Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities},
abstract = {This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.}
}
@article{WEN2021295,
title = {Big data driven Internet of Things for credit evaluation and early warning in finance},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {295-307},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001977},
author = {Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan},
keywords = {Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven},
abstract = {The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.}
}
@article{SOUIFI2021857,
title = {From Big Data to Smart Data: Application to performance management},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {857-862},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.100},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008491},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Big Data, Smart Data, Performance Management},
abstract = {In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.}
}
@article{SHAHOUD2021100432,
title = {An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies},
journal = {Internet of Things},
volume = {16},
pages = {100432},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000767},
author = {Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer},
keywords = {Meta learning, Machine learning, Microservice, Web-based applications, Big data},
abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}
@article{SHARMA20215515,
title = {A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {5515-5519},
year = {2021},
note = {3rd International e-Conference on Frontiers in Mechanical Engineering and nanoTechnology},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.374},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321024263},
author = {Vikrant Sharma and Atul Kumar and Mukesh Kumar},
keywords = {Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)},
abstract = {Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.}
}
@article{CISNEROSCABRERA2021114858,
title = {Experimenting with big data computing for scaling data quality-aware query processing},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114858},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002992},
author = {Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris},
keywords = {Data quality-aware queries, Big data computing, Empirical evaluation},
abstract = {Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.}
}
@article{WANG2022663,
title = {A novel oscillation identification method for grid-connected renewable energy based on big data technology},
journal = {Energy Reports},
volume = {8},
pages = {663-671},
year = {2022},
note = {2021 International Conference on New Energy and Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722002682},
author = {Jian Wang},
keywords = {Oscillation identification, Big data, Evidence theory, Support vector machine},
abstract = {With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.}
}
@article{GEORGIADIS2022105640,
title = {Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105640},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105640},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001138},
author = {Georgios Georgiadis and Geert Poels},
keywords = {Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review},
abstract = {Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.}
}
@article{YIN2021102514,
title = {Integrating remote sensing and geospatial big data for urban land use mapping: A review},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102514},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102514},
url = {https://www.sciencedirect.com/science/article/pii/S030324342100221X},
author = {Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu},
keywords = {Integration methods, Urban functional zone classification, Urban management, Land use},
abstract = {Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.}
}
@article{SHAMIM2020120315,
title = {Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120315},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120315},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311410},
author = {Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia},
keywords = {Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets},
abstract = {This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.}
}
@article{ROSADO2021102155,
title = {MARISMA-BiDa pattern: Integrated risk analysis for big data},
journal = {Computers & Security},
volume = {102},
pages = {102155},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304284},
author = {David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina},
keywords = {Big data, Risk assessment, Risk analysis, Information security, Security standards},
abstract = {Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.}
}
@article{KWON2014387,
title = {Data quality management, data usage experience and acquisition intention of big data analytics},
journal = {International Journal of Information Management},
volume = {34},
number = {3},
pages = {387-394},
year = {2014},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214000127},
author = {Ohbyung Kwon and Namyeon Lee and Bongsik Shin},
keywords = {Big data analytics, Resource-based view, Data quality management, IT capability, Data usage},
abstract = {Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.}
}
@article{LIU2016134,
title = {Rethinking big data: A review on the data quality and usage issues},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {134-142},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002567},
author = {Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu},
keywords = {Big data, Data quality and error, Data ethnics, Spatial information sciences},
abstract = {The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.}
}
@article{YANG2021100234,
title = {Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data},
journal = {Big Data Research},
volume = {25},
pages = {100234},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000514},
author = {Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino},
keywords = {Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning},
abstract = {Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.}
}
@article{YAO20181,
title = {Big data quality prediction in the process industry: A distributed parallel modeling framework},
journal = {Journal of Process Control},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300660},
author = {Le Yao and Zhiqiang Ge},
keywords = {Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics},
abstract = {With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.}
}
@article{CORTEREAL2020103141,
title = {Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103141},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308662},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira},
keywords = {Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory},
abstract = {Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.}
}
@article{HORNG202222,
title = {Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view},
journal = {Journal of Hospitality and Tourism Management},
volume = {51},
pages = {22-38},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000389},
author = {Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu},
keywords = {Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy},
abstract = {To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.}
}
@article{ALI2021101600,
title = {Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101600},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101600},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000368},
author = {Hamza Ali and Ryad Titah},
keywords = {Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government},
abstract = {It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.}
}
@article{GUPTA2021120986,
title = {Big data and firm marketing performance: Findings from knowledge-based view},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120986},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120986},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004182},
author = {Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen},
keywords = {Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view},
abstract = {A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.}
}
@article{SINGH2021157,
title = {Big data, industry 4.0 and cyber-physical systems integration: A smart industry context},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {157-162},
year = {2021},
note = {2nd International Conference on Manufacturing Material Science and Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.170},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352639},
author = {Harpreet Singh},
keywords = {Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing},
abstract = {The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.}
}
@article{BAZZAZABKENAR2021101517,
title = {Big data analytics meets social media: A systematic review of techniques, open issues, and future directions},
journal = {Telematics and Informatics},
volume = {57},
pages = {101517},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101517},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301763},
author = {Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii},
keywords = {Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review},
abstract = {Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.}
}
@article{CECH20211947,
title = {Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {1947-1953},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00061f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008856},
author = {Nadja B. Cech and Marnix H. Medema and Jon Clardy},
abstract = {ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.}
}
@article{HAZEN201472,
title = {Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications},
journal = {International Journal of Production Economics},
volume = {154},
pages = {72-80},
year = {2014},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314001339},
author = {Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer},
keywords = {Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory},
abstract = {Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.}
}
@article{OUAFIQ2022102093,
title = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102093},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
author = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{JIN202024,
title = {Big Data in food safety- A review},
journal = {Current Opinion in Food Science},
volume = {36},
pages = {24-32},
year = {2020},
note = {Food Safety},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214799320301260},
author = {Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin},
abstract = {The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.}
}
@article{BENITEZHIDALGO2021107489,
title = {TITAN: A knowledge-based platform for Big Data workflow management},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107489},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007516},
author = {Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Big Data analytics, Semantics, Knowledge extraction},
abstract = {Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.}
}
@article{GOKALP2022103585,
title = {A process assessment model for big data analytics},
journal = {Computer Standards & Interfaces},
volume = {80},
pages = {103585},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103585},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000805},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren},
keywords = {Big data, Data analytics, Software development, Software process improvement, Software process assessment},
abstract = {Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.}
}
@article{YANG2021107550,
title = {Optimal timing of big data application in a two-period decision model with new product sales},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107550},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107550},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100454X},
author = {Lei Yang and Anqian Jiang and Jiahua Zhang},
keywords = {Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare},
abstract = {We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.}
}
@article{RAUT2021103368,
title = {Big data analytics: Implementation challenges in Indian manufacturing supply chains},
journal = {Computers in Industry},
volume = {125},
pages = {103368},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103368},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306023},
author = {Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede},
keywords = {Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis},
abstract = {Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.}
}
@article{SILVA2019532,
title = {Risk Analysis of Using Big Data in Computer Sciences},
journal = {Procedia Computer Science},
volume = {160},
pages = {532-537},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317521},
author = {Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {Data management, data quality, decision making, data analysis},
abstract = {Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.}
}
@article{ZHANG2022101626,
title = {Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101626},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101626},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000629},
author = {Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui},
keywords = {Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues},
abstract = {Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@article{KOZIARA202184,
title = {Introduction to Big Data in trauma and orthopaedics},
journal = {Orthopaedics and Trauma},
volume = {35},
number = {2},
pages = {84-89},
year = {2021},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2021.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S187713272100004X},
author = {Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley},
keywords = {Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy},
abstract = {The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.}
}
@incollection{SEBASTIANCOLEMAN20223,
title = {Chapter 1 - The Importance of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {3-30},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000018},
author = {Laura Sebastian-Coleman},
keywords = {Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization},
abstract = {This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.}
}
@article{LEI2021101570,
title = {Modelling and analysis of big data platform group adoption behaviour based on social network analysis},
journal = {Technology in Society},
volume = {65},
pages = {101570},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101570},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21000452},
author = {Zhimei Lei and Yandan Chen and Ming K. Lim},
keywords = {Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis},
abstract = {Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.}
}
@article{YOUSSEF2022102827,
title = {Cross-national differences in big data analytics adoption in the retail industry},
journal = {Journal of Retailing and Consumer Services},
volume = {64},
pages = {102827},
year = {2022},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102827},
url = {https://www.sciencedirect.com/science/article/pii/S0969698921003933},
author = {Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag},
keywords = {Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry},
abstract = {Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.}
}
@article{LACAM2021100406,
title = {Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop},
journal = {The Journal of High Technology Management Research},
volume = {32},
number = {1},
pages = {100406},
year = {2021},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2021.100406},
url = {https://www.sciencedirect.com/science/article/pii/S1047831021000031},
author = {Jean-Sébastien Lacam and David Salvetat},
keywords = {Big data, Smart data, Volume, Velocity, Variety, Automotive distribution},
abstract = {This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.}
}
@article{LEEPOST2019113135,
title = {Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development},
journal = {Decision Support Systems},
volume = {126},
pages = {113135},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301642},
author = {Anita Lee-Post and Ram Pakath},
keywords = {Data quality, Big data, Secondary data, Numerical data, Quality threshold},
abstract = {An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.}
}
@article{DEEPA2022209,
title = {A survey on blockchain for big data: Approaches, opportunities, and future directions},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {209-226},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000243},
author = {N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana},
keywords = {Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security},
abstract = {Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.}
}
@article{KASTOUNI2020,
title = {Big data analytics in telecommunications: Governance, architecture and use cases},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
author = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{GHALLAB2020131,
title = {Detection outliers on internet of things using big data technology},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {3},
pages = {131-138},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301616},
author = {Haitham Ghallab and Hanan Fahmy and Mona Nasr},
keywords = {Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs},
abstract = {Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.}
}
@article{BERGIER2021102864,
title = {Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?},
journal = {Autoimmunity Reviews},
volume = {20},
number = {8},
pages = {102864},
year = {2021},
issn = {1568-9972},
doi = {https://doi.org/10.1016/j.autrev.2021.102864},
url = {https://www.sciencedirect.com/science/article/pii/S1568997221001361},
author = {Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud},
keywords = {Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine},
abstract = {The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.}
}
@article{PICCAROZZI20221746,
title = {The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies},
journal = {Procedia Computer Science},
volume = {200},
pages = {1746-1755},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.375},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003842},
author = {Michela Piccarozzi and Barbara Aquilani},
keywords = {Big Data, Covid-19, systematic literature review, management},
abstract = {2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.}
}
@article{RIDZUAN2022685,
title = {Diagnostic analysis for outlier detection in big data analytics},
journal = {Procedia Computer Science},
volume = {197},
pages = {685-692},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024133},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {Big data, data quality, outlier, Sustainable Development Goals},
abstract = {Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.}
}
@article{SFAXI2020101862,
title = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101862},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101862},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
author = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
keywords = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
abstract = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@article{GOH2022,
title = {Are batch effects still relevant in the age of big data?},
journal = {Trends in Biotechnology},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922000361},
author = {Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong},
keywords = {artificial intelligence, batch effect, machine learning, RNA sequencing, single cell},
abstract = {Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.}
}
@incollection{BIRKIN2020303,
title = {Big Data},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-311},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10616-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510616X},
author = {Mark Birkin},
keywords = {Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information},
abstract = {Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.}
}
@article{LI2021102064,
title = {Big data driven vehicle battery management method: A novel cyber-physical system perspective},
journal = {Journal of Energy Storage},
volume = {33},
pages = {102064},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.102064},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20318971},
author = {Shuangqi Li and Pengfei Zhao},
keywords = {Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning},
abstract = {The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.}
}
@article{CAISSIE2022100925,
title = {Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium},
journal = {Advances in Radiation Oncology},
pages = {100925},
year = {2022},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2022.100925},
url = {https://www.sciencedirect.com/science/article/pii/S245210942200032X},
author = {Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo},
abstract = {Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.}
}
@article{ARDAGNA2021107215,
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107215},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107215},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002081},
author = {Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert},
keywords = {Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy},
abstract = {We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.}
}
@article{SURBAKTI2020103146,
title = {Factors influencing effective use of big data: A research framework},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103146},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308649},
author = {Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq},
keywords = {Big data, Effective use, Factors, Framework},
abstract = {Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.}
}
@article{RIDZUAN2019731,
title = {A Review on Data Cleansing Methods for Big Data},
journal = {Procedia Computer Science},
volume = {161},
pages = {731-738},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318885},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {data cleansing, big data, data quality},
abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.}
}
@article{LI2020100608,
title = {Network analysis of big data research in tourism},
journal = {Tourism Management Perspectives},
volume = {33},
pages = {100608},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2019.100608},
url = {https://www.sciencedirect.com/science/article/pii/S2211973619301400},
author = {Xin Li and Rob Law},
keywords = {Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends},
abstract = {This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.}
}
@article{BRINCH2021539,
title = {Firm-level capabilities towards big data value creation},
journal = {Journal of Business Research},
volume = {131},
pages = {539-548},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320304859},
author = {Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba}},
keywords = {Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities},
abstract = {Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.}
}
@article{YE2021106293,
title = {Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106293},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003679},
author = {Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang},
keywords = {Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things},
abstract = {Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.}
}
@article{SELLAMI2020102732,
title = {On the use of big data frameworks for big service composition},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102732},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102732},
url = {https://www.sciencedirect.com/science/article/pii/S108480452030206X},
author = {Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid},
keywords = {Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark},
abstract = {Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.}
}
@incollection{BUDNITZ2021665,
title = {Transport Modes and Big Data},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {665-670},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10601-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106013},
author = {Hannah D Budnitz and Emmanouil Tranos and Lee Chapman},
keywords = {Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics},
abstract = {Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.}
}
@article{LI2022101021,
title = {A review of industrial big data for decision making in intelligent manufacturing},
journal = {Engineering Science and Technology, an International Journal},
volume = {29},
pages = {101021},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001336},
author = {Chunquan Li and Yaqiong Chen and Yuling Shang},
keywords = {Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making},
abstract = {Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.}
}
@article{KONG2020123142,
title = {A systematic review of big data-based urban sustainability research: State-of-the-science and future directions},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {123142},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620331875},
author = {Lingqiang Kong and Zhifeng Liu and Jianguo Wu},
keywords = {Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning},
abstract = {The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.}
}
@article{LIU202053,
title = {Semantic-aware data quality assessment for image big data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {53-65},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302304},
author = {Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu},
keywords = {Semantic-aware, Quality assessment, Image big data, IDSTH, SHR},
abstract = {Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@article{CORALLO2022102331,
title = {Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102331},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102331},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000205},
author = {Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi},
keywords = {Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection},
abstract = {Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.}
}
@article{NIKOLOV2021100440,
title = {Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers},
journal = {Internet of Things},
volume = {16},
pages = {100440},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000834},
author = {Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman},
keywords = {Big data workflows, Internet of Things, Domain-specific languages, Software containers},
abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.}
}
@article{SHEN2022102529,
title = {Personal big data pricing method based on differential privacy},
journal = {Computers & Security},
volume = {113},
pages = {102529},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102529},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003539},
author = {Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang},
keywords = {Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation},
abstract = {Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.}
}
@article{JOSE2022100081,
title = {Integrating big data and blockchain to manage energy smart grid - TOTEM framework},
journal = {Blockchain: Research and Applications},
pages = {100081},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000227},
author = {Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong},
keywords = {Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT},
abstract = {The demand for electricity is increasing exponentially day by day especially with the arrival of electric vehicles. In smart community neighborhood project, it demands electricity should be produced at the household or community level and sell or buy according to the demands. Since the actors can produce, sell and also buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways such as machine learning for analysing the household data for customer demand and peak hour for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensure data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows the users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here in this article, we will show the importance of TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.}
}
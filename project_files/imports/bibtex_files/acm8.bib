@inproceedings{10.1145/3414274.3414509,
author = {Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang},
title = {Multidimensional Data Mining on the Early Scientific Talents of China},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414509},
doi = {10.1145/3414274.3414509},
abstract = {The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {239–246},
numpages = {8},
keywords = {Cultivation of high-level talents, Data-mining, Education},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3211954.3211955,
author = {Seabolt, Ed and Kandogan, Eser and Roth, Mary},
title = {Contextual Intelligence for Unified Data Governance},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211955},
doi = {10.1145/3211954.3211955},
abstract = {Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {2},
numpages = {9},
keywords = {Context, Data Governance, Analytics, Graph},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@inproceedings{10.1145/2912160.2912179,
author = {Hu, Yanhua and Bai, Xianyang and Sun, Shuyang},
title = {Readiness Assessment of Open Government Data Programs: A Case of Shenzhen},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912179},
doi = {10.1145/2912160.2912179},
abstract = {More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {97–103},
numpages = {7},
keywords = {Open government data, Open data, Readiness assessment},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/2851613.2851757,
author = {Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson},
title = {Retrospective, Relevance, and Trends of SAC Requirements Engineering Track},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851757},
doi = {10.1145/2851613.2851757},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1264–1269},
numpages = {6},
keywords = {retrospective, requirements engineering, symposium on applied computing, relevance, scoping study, SAC, trends, systematic mapping study},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3284103.3284114,
author = {Wang, Rongxiao and Chen, Bin and Wang, Yiduo and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang},
title = {The Air Contaminant Dispersion Prediction by the Integration of the Neural Network and AermodSystem},
year = {2018},
isbn = {9781450360449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284103.3284114},
doi = {10.1145/3284103.3284114},
abstract = {Air pollution caused by industrial production has become a serious problem for public health. This challenging problem promotes the development of the research in the air contaminant dispersion (ADS) prediction, for the management of the emission and leak accident. However, conventional ADS models can hardly meet the requirement of both accuracy and efficiency. The data model, like the artificial neural network (ANN) provides a feasible way of forecasting the dispersion with high accuracy and efficiency. However, the construction of the ANN for prediction needs plenty of data, which is impractical to obtain in most emission cases. To address this problem, an ADS simulation software AermodSystem is applied to build the simulated dispersion scenarios and provide synthetic dataset for the model training and test. Based on the synthetic data set, the ANN prediction model is established, and evaluated on the test set, as well as the Gaussian model. Further, these two models are served as the forward dispersion model and combined with the Particle Swarm Optimization (PSO) for source estimation. The results verify the effectiveness of the proposed model and indicate that the ANN together with the AermodSystem as the data generator is feasible in the air contaminant dispersion forecast and the source estimation of a particular case.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
articleno = {11},
numpages = {5},
keywords = {Source estimation, Artificial neural network, Particle Swarm Optimization, AermodSystem, Atmospheric dispersion prediction},
location = {Seattle, WA, USA},
series = {Safety and Resilience'18}
}

@article{10.1145/1462571.1462573,
author = {Agrawal, Rakesh and Ailamaki, Anastasia and Bernstein, Philip A. and Brewer, Eric A. and Carey, Michael J. and Chaudhuri, Surajit and Doan, AnHai and Florescu, Daniela and Franklin, Michael J. and Garcia-Molina, Hector and Gehrke, Johannes and Gruenwald, Le and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Korth, Hank F. and Kossmann, Donald and Madden, Samuel and Magoulas, Roger and Ooi, Beng Chin and O'Reilly, Tim and Ramakrishnan, Raghu and Sarawagi, Sunita and Stonebraker, Michael and Szalay, Alexander S. and Weikum, Gerhard},
title = {The Claremont Report on Database Research},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/1462571.1462573},
doi = {10.1145/1462571.1462573},
abstract = {In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.},
journal = {SIGMOD Rec.},
month = {sep},
pages = {9–19},
numpages = {11}
}

@article{10.1145/3131780,
author = {Lukyanenko, Roman and Samuel, Binny M.},
title = {Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131780},
doi = {10.1145/3131780},
abstract = {Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {14},
numpages = {15},
keywords = {database design, Conceptual modeling}
}

@inproceedings{10.1145/3512850.3512861,
author = {Mei, Songzhu and Liu, Cong and Wang, Qinglin and Su, Huayou},
title = {Model Provenance Management in MLOps Pipeline},
year = {2022},
isbn = {9781450395717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512850.3512861},
doi = {10.1145/3512850.3512861},
booktitle = {2022 The 8th International Conference on Computing and Data Engineering},
pages = {45–50},
numpages = {6},
keywords = {MLOps, Model provenance, Machine learning engineering, Artificial Intelligence},
location = {Bangkok, Thailand},
series = {ICCDE 2022}
}

@article{10.1145/3446679,
author = {Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.},
title = {Mobility Trace Analysis for Intelligent Vehicular Networks: Methods, Models, and Applications},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446679},
doi = {10.1145/3446679},
abstract = {Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {49},
numpages = {38},
keywords = {Vehicular networks, mobility, data mining, data analysis, vanet, topology, routing, survey}
}

@inproceedings{10.1109/MSR.2019.00031,
author = {Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris},
title = {World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00031},
doi = {10.1109/MSR.2019.00031},
abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {143–154},
numpages = {12},
keywords = {software mining, software supply chain, software ecosystem},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inbook{10.1145/3368089.3417039,
author = {Nguyen-Duc, Anh and Abrahamsson, Pekka},
title = {Continuous Experimentation on Artificial Intelligence Software: A Research Agenda},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417039},
abstract = {Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an “unknown unknown” problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1513–1516},
numpages = {4}
}

@article{10.1145/3458027,
author = {Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik},
title = {Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2692-1626},
url = {https://doi.org/10.1145/3458027},
doi = {10.1145/3458027},
abstract = {For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.},
journal = {Digital Threats},
month = {oct},
articleno = {6},
numpages = {22},
keywords = {knowledge graph, security, ontology, Cyber threat intelligence}
}

@inproceedings{10.1145/2345396.2345413,
author = {Mehta, R. Vasanth Kumar and Sankarasubramaniam, B. and Rajalakshmi, S.},
title = {An Algorithm for Fuzzy-Based Sentence-Level Document Clustering for Micro-Level Contradiction Analysis},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345413},
doi = {10.1145/2345396.2345413},
abstract = {Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {102–105},
numpages = {4},
keywords = {information-retrieval, document clustering, contradiction analysis},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/3207677.3277983,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Inconsistent Data Detection Based on Maximum Dependency Set},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277983},
doi = {10.1145/3207677.3277983},
abstract = {For1 the incomplete detection of inconsistent data by conditional functional dependency(CFDs), this paper proposes a dependency lifting algorithm (DLA) based on maximum dependency set (MDS), which detects inconsistent data in data set by acquiring recessive conditional functional dependencies (RCFDs) in CFDs. Presenting the dynamic domain adjustment, setting forward and backward pointers of numerical change to improve the enumeration process in original algorithm, the applicability of the algorithm to the continuous attributes is raised too. Then, this paper provides the algorithm flow and pseudo code of dynamic domain adjustment and the DLA, analyses the convergence and time complexity of them. Finally, the validity of the DLA is verified by comparing the detection accuracy and time-cost.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {63},
numpages = {8},
keywords = {Conditional functional dependency (CFDs), dynamic domain adjustment, maximum dependency set (MDS), inconsistent data},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3328519.3329133,
author = {Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael},
title = {Towards an End-to-End Human-Centric Data Cleaning Framework},
year = {2019},
isbn = {9781450367912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328519.3329133},
doi = {10.1145/3328519.3329133},
abstract = {Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {1},
numpages = {7},
location = {Amsterdam, Netherlands},
series = {HILDA'19}
}

@inproceedings{10.1145/3170427.3170632,
author = {Thelisson, Eva and Sharma, Kshitij and Salam, Hanan and Dignum, Virginia},
title = {The General Data Protection Regulation: An Opportunity for the HCI Community?},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170632},
doi = {10.1145/3170427.3170632},
abstract = {With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community.This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions.The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR.There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community.This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals...) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {responsible innovation, design, soft law, codes of conduct, labeling, privacy, quality standards},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.5555/2346696.2346710,
author = {Wang, BingQiang and See, Simon},
title = {Accelerate High Throughput Analysis for Genome Sequencing with GPU},
year = {2012},
isbn = {9781450316446},
publisher = {A*STAR Computational Resource Centre},
address = {SGP},
booktitle = {Proceedings of the ATIP/A*CRC Workshop on Accelerator Technologies for High-Performance Computing: Does Asia Lead the Way?},
articleno = {11},
numpages = {46},
location = {Buona Vista, Singapore},
series = {ATIP '12}
}

@inproceedings{10.1145/3381991.3395603,
author = {Momen, Nurul and Bock, Sven and Fritsch, Lothar},
title = {Accept - Maybe - Decline: Introducing Partial Consent for the Permission-Based Access Control Model of Android},
year = {2020},
isbn = {9781450375689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381991.3395603},
doi = {10.1145/3381991.3395603},
abstract = {The consent to personal data sharing is an integral part of modern access control models on smart devices. This paper examines the possibility of registering conditional consent which could potentially increase trust in data sharing. We introduce an indecisive state of consenting to policies that will enable consumers to evaluate data services before fully committing to their data sharing policies. We address technical, regulatory, social, individual and economic perspectives for inclusion of partial consent within an access control mechanism. Then, we look into the possibilities to integrate it within the access control model of Android by introducing an additional button in the interface--Maybe. This article also presents a design for such implementation and demonstrates feasibility by showcasing a prototype built on Android platform. Our effort is exploratory and aims to shed light on the probable research direction.},
booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},
pages = {71–80},
numpages = {10},
keywords = {data protection, partial consent, access control, privacy},
location = {Barcelona, Spain},
series = {SACMAT '20}
}

@inbook{10.1145/3486635.3491073,
author = {Rao, Jinmeng and Gao, Song and Zhu, Xiaojin},
title = {VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450391207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486635.3491073},
abstract = {Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {43–46},
numpages = {4}
}

@article{10.1109/TNET.2019.2944984,
author = {Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun},
title = {Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2944984},
doi = {10.1109/TNET.2019.2944984},
abstract = {Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2294–2307},
numpages = {14}
}

@article{10.1145/2590989.2590995,
author = {Naumann, Felix},
title = {Data Profiling Revisited},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2590995},
doi = {10.1145/2590989.2590995},
abstract = {Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {40–49},
numpages = {10}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {82},
numpages = {47},
keywords = {orchestration, deep learning, machine learning, IoT}
}

@inproceedings{10.1145/2729104.2729129,
author = {Lipuntsov, Yuri P.},
title = {Three Types of Data Exchange in the Open Government Information Projects},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729129},
doi = {10.1145/2729104.2729129},
abstract = {Open government as a new system of public administration requires a qualitatively new level of information support for digital interactions of agencies, as well as between government and citizens, experts, and businesses. This article examines three categories of government counterparties, interactions with which are based on different principles: community of interest; subject areas; a loosely coupled environment. The public sector has now many information projects, and most of them deal with the data exchange, data delivery from and to the environment. The understanding of an environment is various for different projects. The categorization of projects depends on nature and the level of intellectual interaction with the external environment needed for the correct definition of project objectives, assessment of effectiveness. With the growing number of users and the transition to an open world, semantic principles are becoming more significant. Given the shift from systems integration to the semantic method, the role of subject matter expert is growing substantially.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {88–94},
numpages = {7},
keywords = {Shared Environment, Core Component, Linked Data, Open Government, Data Standardization, Open Data},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

@inproceedings{10.1145/3308558.3313683,
author = {Altenburger, Kristen M. and Ho, Daniel E.},
title = {Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer Reviews},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313683},
doi = {10.1145/3308558.3313683},
abstract = {Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.},
booktitle = {The World Wide Web Conference},
pages = {2543–2550},
numpages = {8},
keywords = {food safety, consumer reviews, replication, Yelp, regulation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3424771.3424821,
author = {Zimmermann, Olaf and Pautasso, Cesare and L\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko},
title = {Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424821},
doi = {10.1145/3424771.3424821},
abstract = {Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {11},
numpages = {25},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1145/3170427.3170636,
author = {Russell, Daniel M. and Convertino, Gregorio and Kittur, Aniket and Pirolli, Peter and Watkins, Elizabeth Anne},
title = {Sensemaking in a Senseless World: 2018 Workshop Abstract},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170636},
doi = {10.1145/3170427.3170636},
abstract = {Sensemaking is a common activity in the analysis of a large or complex amount of information. This active area of HCI research asks how DO people come to understand such difficult sets of information? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. In the second part of the workshop we will work to synthesize a cross-disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {collaborative work, information understanding, sensemaking},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3428757.3429972,
author = {Draheim, Dirk},
title = {On Architecture of E-Government Ecosystems: From e-Services to e-Participation: [IiWAS'2020 Keynote]},
year = {2020},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429972},
doi = {10.1145/3428757.3429972},
abstract = {The "digital transformation" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. "Form ever follows function." Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {3–10},
numpages = {8},
keywords = {Fiware, data governance, public key infrastructures, data exchange layers, consent management, X-Road, persistent messaging, e-government, e-governance, digital transformation, GAIA-X},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1145/3009973,
author = {Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie},
title = {Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3009973},
doi = {10.1145/3009973},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {nov},
articleno = {17},
numpages = {23},
keywords = {Visualization, visual analytics, exploratory analysis, intelligent visual interfaces, selection bias}
}

@inproceedings{10.1145/2896377.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901461},
doi = {10.1145/2896377.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {249–260},
numpages = {12},
keywords = {mechanism design, strategic data subjects, game theory, incentive mechanism, differential privacy},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1145/2964791.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2964791.2901461},
doi = {10.1145/2964791.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {249–260},
numpages = {12},
keywords = {strategic data subjects, game theory, mechanism design, incentive mechanism, differential privacy}
}

@inproceedings{10.1145/3291078.3291083,
author = {Zhicheng, Dai and Feng, Liu},
title = {Evaluation of the Smart Campus Information Portal},
year = {2018},
isbn = {9781450365772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291078.3291083},
doi = {10.1145/3291078.3291083},
abstract = {As the internet wave swept the world, "Internet plus education" came into being. Smart campus design and construction has since become a research hotspot. The Campus Information Portal (CIP) plays an increasingly important role in the management of smart campuses. That is why, conducting a comprehensive evaluation study on the construction level of campus information portals is necessary. By combining CIP's own characteristics and incorporating intelligent needs, a comprehensive evaluation index system for CIP was developed. An Analytic Hierarchy Process (AHP) was used to determine index weights, while a Fuzzy Comprehensive Evaluation (FCE) was used to calculate the quantitative scores of the evaluation objects. We selected 10 representative Chinese universities for a comprehensive CIP evaluation and experimental analysis. We analyze the final results of the study, evaluate the validity of our process and methods and finally provide guidance for the construction of a smart campus information portal.},
booktitle = {Proceedings of the 2018 2nd International Conference on Education and E-Learning},
pages = {73–79},
numpages = {7},
keywords = {Index system, Comprehensive evaluation, Smart campus, Campus information portal},
location = {Bali, Indonesia},
series = {ICEEL 2018}
}

@inproceedings{10.1145/3488838.3488873,
author = {Cai, Hongxia and Tan, Qiqi},
title = {Blockchain-Based Data Control for Complex Product Assembly Collaboration Process},
year = {2021},
isbn = {9781450384094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488838.3488873},
doi = {10.1145/3488838.3488873},
abstract = {The collaborative development model of complex products brings the challenge to the data interaction management. There are many manufacturers and suppliers involved in the whole life cycle of the assembly process, which makes it difficult to ensure the data security and traceability. The Hyperledger-Fabric architecture in blockchain technology has modular design, pluggable architecture, complete authority control and security, which can well solve the data security and traceability management in the collaborative development of complex products. Therefore, this paper proposes a framework based on the Hyperledger-Fabric architecture of blockchain for the whole life cycle data management of complex products. We also demonstrate the effectiveness of our proposed new framework integrating blockchain technology through the case of quality data control during the aircraft final assembly collaborative process.},
booktitle = {2021 The 3rd World Symposium on Software Engineering},
pages = {205–209},
numpages = {5},
keywords = {Blockchain, Complex product assembly, Collaborative Production},
location = {Xiamen, China},
series = {WSSE 2021}
}

@inproceedings{10.1145/3336499.3338012,
author = {Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru},
title = {Modelling and Linking Company Data in the EuBusinessGraph Platform},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338012},
doi = {10.1145/3336499.3338012},
abstract = {In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {12},
numpages = {6},
keywords = {Company data, Entity Matching, RDF, Record Linkage},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3132218.3132226,
author = {Esnaola-Gonzalez, Iker and Berm\'{u}dez, Jes\'{u}s and Fern\'{a}ndez, Izaskun and Fern\'{a}ndez, Santiago and Arnaiz, Aitor},
title = {Towards a Semantic Outlier Detection Framework in Wireless Sensor Networks},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132226},
doi = {10.1145/3132218.3132226},
abstract = {Outlier detection in the preprocessing phase of Knowledge Discovery in Databases (KDD) processes has been a widely researched topic for many years. However, identifying the potential outlier cause still remains an unsolved challenge even though it could be very helpful for determining what actions to take after detecting it. Furthermore, conventional outlier detection methods might still overlook outliers in certain complex contexts. In this article, Semantic Technologies are used to contribute overcoming these problems by proposing the SemOD (Semantic Outlier Detection) Framework. This framework guides the data-scientist towards the detection of certain types of outliers in WSNs (Wireless Sensor Network). Feasibility of the approach has been tested in outdoor temperature sensors and results show that the proposed approach is generic enough to apply it to different sensors, even improving the accuracy, specificity and sensitivity of outlier detection as well as spotting their potential cause.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {152–159},
numpages = {8},
keywords = {Wireless Sensor Network, Knowledge Discovery in Databases, Outlier Detection, Semantic Technologies},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@inproceedings{10.1145/2856767.2856779,
author = {Gotz, David and Sun, Shun and Cao, Nan},
title = {Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856779},
doi = {10.1145/2856767.2856779},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {85–95},
numpages = {11},
keywords = {intelligent visual interfaces, exploratory analysis, visualization, visual analytics},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@inproceedings{10.1145/3423455.3430316,
author = {Seto, Toshikazu and Sekimoto, Yoshihide and Asahi, Kosuke and Endo, Takahiro},
title = {Constructing a Digital City on a Web-3D Platform: Simultaneous and Consistent Generation of Metadata and Tile Data from a Multi-Source Raw Dataset},
year = {2020},
isbn = {9781450381659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423455.3430316},
doi = {10.1145/3423455.3430316},
abstract = {In this study, we develop a platform that can display approximately 20 types of data via a web browser to realize a digital twin of a wider area, including a detailed reading display of block units and individual three-dimensional point cloud data (point cloud) of a city. Using actual data, we examine if the data model and visualization design correspond with the zoom level. Owing to the comparative examination of the wide-area display performance and the map representation design in a JavaScript-based open-source library, we were able to develop a platform with light architecture and an easily customizable display. Furthermore, prototyping, based on Mapbox GL JS and Deck.GL, and the display of spatiotemporal flow layers, such as background maps, point cloud data in many places, dozens of layer display types, and the General Transit Feed Specification (GTFS) allowed for the seamless transition from the local government to the wide-area display in the prefecture unit in approximately 10-20 s.It is recommended that this digital smart city platform should be standardized by other local governments, especially in areas where higher-order data visualization is yet to advance. To display this digital city in a lightweight environment, we consider the digital data situation of local governments in Japan. It is necessary to define the visualized design for each zoom level according to the characteristics of the data. We then arranged the display model of each zoom level for 20 types of urban infrastructure data related to the digital smart city by referring to the style schema of the tile form. Through these tasks, we organized the commonality and optimization of data models and formats.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
pages = {1–9},
numpages = {9},
keywords = {infrastructure 3D tiles, Mapbox GL JS, digital city, Deck.GL},
location = {Seattle, Washington},
series = {ARIC '20}
}

@inbook{10.1145/3448016.3457542,
author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
title = {AI Meets Database: AI4DB and DB4AI},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457542},
abstract = {Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2859–2866},
numpages = {8}
}

@article{10.1145/3511666,
author = {Schmeck, Hartmut and Monti, Antonello and Hagenmeyer, Veit},
title = {Energy Informatics: Key Elements for Tomorrow's Energy System},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3511666},
doi = {10.1145/3511666},
journal = {Commun. ACM},
month = {mar},
pages = {58–63},
numpages = {6}
}

@inproceedings{10.1145/3473714.3473788,
author = {Zhang, Jiamin},
title = {Potential Energy Saving Estimation for Retrofit Building with ASHRAE-Great Energy Predictor III Using Machine Learning},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473788},
doi = {10.1145/3473714.3473788},
abstract = {Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.},
booktitle = {Proceedings of the 2021 International Conference on Control and Intelligent Robotics},
pages = {425–429},
numpages = {5},
keywords = {energy saving, green architecture, machine learning, retrofit building},
location = {Guangzhou, China},
series = {ICCIR 2021}
}

@article{10.1145/3415212,
author = {Feger, Sebastian S. and Wozniak, Pawe\l{} W. and Lischke, Lars and Schmidt, Albrecht},
title = { 'Yes, I Comply!': Motivations and Practices around Research Data Management and Reuse across Scientific Fields},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415212},
doi = {10.1145/3415212},
abstract = {As science becomes increasingly data-intensive, the requirements for comprehensive Research Data Management (RDM) grow. This often overwhelms scientists, requiring more workload and training. The failure to conduct effective RDM leads to producing research artefacts that cannot be reproduced or reused. Past research placed high value on supporting data science workers, but focused mainly on data production, collection, processing, and sensemaking. In order to understand practices and needs of data science workers in relation to documentation, preservation, sharing, and reuse, we conducted a cross-domain study with 15 scientists and data managers from diverse scientific domains. We identified five core concepts which describe requirements, drivers, and boundaries in the development of commitment for RDM, essential for generating reproducible research artefacts: Practice, Adoption, Barriers, Education, and Impact. Based on those concepts, we introduce a stage-based model of personal RDM commitment evolution. The model can be used to drive the design of future systems that support a transition to open science. We discuss infrastructure, policies, and motivations involved at the stages and transitions in the model. Our work supports designers in understanding the constraints and challenges involved in designing for reproducibility in an age of data-driven science.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {141},
numpages = {26},
keywords = {reproducibility, human data interventions, research data management, data-processing science, reuse, motivation}
}

@article{10.1145/3057266,
author = {Perera, Charith and Qin, Yongrui and Estrella, Julio C. and Reiff-Marganiec, Stephan and Vasilakos, Athanasios V.},
title = {Fog Computing for Sustainable Smart Cities: A Survey},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057266},
doi = {10.1145/3057266},
abstract = {The Internet of Things (IoT) aims to connect billions of smart objects to the Internet, which can bring a promising future to smart cities. These objects are expected to generate large amounts of data and send the data to the cloud for further processing, especially for knowledge discovery, in order that appropriate actions can be taken. However, in reality sensing all possible data items captured by a smart object and then sending the complete captured data to the cloud is less useful. Further, such an approach would also lead to resource wastage (e.g., network, storage, etc.). The Fog (Edge) computing paradigm has been proposed to counterpart the weakness by pushing processes of knowledge discovery using data analytics to the edges. However, edge devices have limited computational capabilities. Due to inherited strengths and weaknesses, neither Cloud computing nor Fog computing paradigm addresses these challenges alone. Therefore, both paradigms need to work together in order to build a sustainable IoT infrastructure for smart cities. In this article, we review existing approaches that have been proposed to tackle the challenges in the Fog computing domain. Specifically, we describe several inspiring use case scenarios of Fog computing, identify ten key characteristics and common features of Fog computing, and compare more than 30 existing research efforts in this domain. Based on our review, we further identify several major functionalities that ideal Fog computing platforms should support and a number of open challenges toward implementing them, to shed light on future research directions on realizing Fog computing for building sustainable smart cities.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {32},
numpages = {43},
keywords = {smart cities, fog computing, Internet of things, sustainability}
}

@article{10.1007/s00779-019-01217-0,
author = {Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang},
title = {Data Collection Scheme with Minimum Cost and Location of Emotional Recognition Edge Devices},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01217-0},
doi = {10.1007/s00779-019-01217-0},
abstract = {This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {595–606},
numpages = {12},
keywords = {Data acquisition, Location, Data collection, Edge devices, Emotional recognition, Collection cost}
}

@article{10.1145/2857274.2886105,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision-Making: A View from Computational Journalism},
year = {2015},
issue_date = {November-December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {9},
issn = {1542-7730},
url = {https://doi.org/10.1145/2857274.2886105},
doi = {10.1145/2857274.2886105},
abstract = {Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.},
journal = {Queue},
month = {nov},
pages = {126–149},
numpages = {24}
}

@inproceedings{10.1145/2702123.2702528,
author = {Mauriello, Matthew Louis and Norooz, Leyla and Froehlich, Jon E.},
title = {Understanding the Role of Thermography in Energy Auditing: Current Practices and the Potential for Automated Solutions},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702528},
doi = {10.1145/2702123.2702528},
abstract = {The building sector accounts for 41% of primary energy consumption in the US, contributing an increasing portion of the country's carbon dioxide emissions. With recent sensor improvements and falling costs, auditors are increasingly using thermography-infrared (IR) cameras-to detect thermal defects and analyze building efficiency. Research in automated thermography has grown commensurately, aimed at reducing manual labor and improving thermal models. Though promising, we could find no prior work exploring the professional auditor's perspectives of thermography or reactions to emerging automation. To address this gap, we present results from two studies: a semi-structured interview with 10 professional energy auditors, which includes design probes of five automated thermography scenarios, and an observational case study of a residential audit. We report on common perspectives, concerns, and benefits related to thermography and summarize reactions to our automated scenarios. Our findings have implications for thermography tool designers as well as researchers working on automated solutions in robotics, computer science, and engineering.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1993–2002},
numpages = {10},
keywords = {sustainable hci, energy audits, human-robotic interaction, formative inquiry, robotics, thermography, design probes},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/2740908.2742563,
author = {Deng, Alex},
title = {Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742563},
doi = {10.1145/2740908.2742563},
abstract = {As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {923–928},
numpages = {6},
keywords = {multiple testing, bayesian statistics, prior, optional stopping, empirical bayes, controlled experiments, objective bayes, a/b testing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2565585.2565608,
author = {Welbourne, Evan and Wu, Pang and Bao, Xuan and Munguia-Tapia, Emmanuel},
title = {Crowdsourced Mobile Data Collection: Lessons Learned from a New Study Methodology},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565608},
doi = {10.1145/2565585.2565608},
abstract = {In this paper we explore a scalable data collection methodology that simultaneously achieves low cost and a high degree of control. We use popular online crowdsourcing platforms to recruit 63 subjects for a 90-day data collection that resulted in over 75K hours of data. The total cost of data collection was dramatically lower than for alternative methodologies, with total subject compensation under $3.5K US, and a total of less than 10 hours/week spent by researchers managing the study. At the same time, our methodology enhances control and enables richer study protocols by allowing direct contact with subjects. We were able to conduct surveys, exchange messages, and debug remotely with feedback from subjects. In addition to reporting on study details, we also discuss interesting findings and offer lessons learned.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {2},
numpages = {6},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inbook{10.1145/3447404.3447415,
author = {McMenemy, David},
title = {Ethical Issues in Machine Learning},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447415},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {195–196},
numpages = {2}
}

@article{10.1145/2844110,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision Making},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2844110},
doi = {10.1145/2844110},
abstract = {A view from computational journalism.},
journal = {Commun. ACM},
month = {jan},
pages = {56–62},
numpages = {7}
}

@inproceedings{10.1145/3325112.3325243,
author = {Chen, Tao and Ran, Longya and Gao, Xian},
title = {AI Innovation for Advancing Public Service: The Case of China's First Administrative Approval Bureau},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325243},
doi = {10.1145/3325112.3325243},
abstract = {The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {100–108},
numpages = {9},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@article{10.1145/2756547,
author = {Shekhar, Shashi and Feiner, Steven K. and Aref, Walid G.},
title = {Spatial Computing},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2756547},
doi = {10.1145/2756547},
abstract = {Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.},
journal = {Commun. ACM},
month = {dec},
pages = {72–81},
numpages = {10}
}

@inproceedings{10.1145/2666310.2666471,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing: Efficiently Parsing Billions of Addresses on MapReduce},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666471},
doi = {10.1145/2666310.2666471},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {433–436},
numpages = {4},
keywords = {record linkage, large-scale data, address parsing},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@article{10.1145/3527546.3527555,
author = {Ahlers, Dirk and Wilde, Erik and Spaniol, Marc and Baeza-Yates, Ricardo and Alonso, Omar},
title = {Report on the 11th International Workshop on Location and the Web (LocWeb 2021) and the 11th Temporal Web Analytics Workshop (TempWeb2021) at WWW2021},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527555},
doi = {10.1145/3527546.3527555},
abstract = {LocWeb and TempWeb 2021 were the eleventh events in their workshop series and took place co-located on 12th April 2021 in conjunction with The Web Conference WWW 2021. They were intended to be held in Ljubljana, Slovenia as a potentially hybrid event, but due to the pandemic, were fully moved online.LocWeb and TempWeb were held as one colocated session with a merged programme and shared topics to explore similarities and introduce attendees to the two related and complementary areas. LocWeb 2021 explored the intersection of location-based analytics and Web architecture with a focus on on Web-scale services and location-aware information access. TempWeb 2021 discussed temporal analytics at a Web scale with experts from science and industry.Date: 12 April, 2021.Websites: https://dhere.de/locweb/locweb2021 and http://temporalweb.net/.},
journal = {SIGIR Forum},
month = {mar},
articleno = {6},
numpages = {7}
}

@inproceedings{10.1145/3091478.3091521,
author = {Xu, Jiejun and Xie, Daniel and Lu, Tsai-Ching and Cafeo, John},
title = {EDSV: Emerging Defect Surveillance for Vehicles},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091521},
doi = {10.1145/3091478.3091521},
abstract = {We present early findings on building a proof of concept for an automated system to identify emerging trends regarding vehicle defects. The proposed system functions by continuously collecting and monitoring publicly available data from several heterogeneous channels ranging from online social media to vehicle enthusiast forums and consumer reporting sites. By mining the collected data, the system would provide real-time detection of ongoing consumer issues with vehicles. In addition, our system has special emphasis on detecting early signals prior to the widespread knowledge of the general public. One of the system components involves estimating a baseline statistical distribution governing the frequency of observing specific types of vehicle defective complaints from our data sources and subsequently identifying irregular deviations from this distribution. A web interface is made available to visualize descriptive statistics derived from various channels, with the intent to provide timely insights for human analysts.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {219–222},
numpages = {4},
keywords = {measurement, user generated content, business intelligence, quality management, online social media},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@inproceedings{10.1145/2663713.2664430,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing with Massive Synthetic Training Data Generation},
year = {2014},
isbn = {9781450314596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663713.2664430},
doi = {10.1145/2663713.2664430},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches of synthetic training data generation to build robust models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses.},
booktitle = {Proceedings of the 4th International Workshop on Location and the Web},
pages = {33–36},
numpages = {4},
keywords = {address parsing, large-scale data, record linkage},
location = {Shanghai, China},
series = {LocWeb '14}
}

@article{10.1145/3352020.3352033,
author = {Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise},
title = {When the Power of the Crowd Meets the Intelligence of the Middleware: The Mobile Phone Sensing Case},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3352020.3352033},
doi = {10.1145/3352020.3352033},
abstract = {The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {85–90},
numpages = {6}
}

@article{10.1145/2795403.2795412,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Report on the Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14)},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2795403.2795412},
doi = {10.1145/2795403.2795412},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remained to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side---the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues---and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization---are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects---how to not creep out users.There was a strong feeling that we made substantial progress. Specifically, the discussion contributed to our understanding of the way forward. First, for notable (head, shoulder, but not tail) entities in semantic search we have reached the level of quality at minimal costs allowing for deployment in major web search engines---the dream has become a reality. Second, entity detection is moving fast into domain specific, personal, and business domains, and has become a vital component for a range of applications. Third, semantic web has exchanged logic for machine learning approaches, and machine learning is the natural unification of semantic web and information retrieval approaches.},
journal = {SIGIR Forum},
month = {jun},
pages = {27–34},
numpages = {8}
}

@inproceedings{10.1145/2187980.2188029,
author = {Soldatos, John and Draief, Moez and Macdonald, Craig and Ounis, Iadh},
title = {Multimedia Search over Integrated Social and Sensor Networks},
year = {2012},
isbn = {9781450312301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187980.2188029},
doi = {10.1145/2187980.2188029},
abstract = {This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {283–286},
numpages = {4},
keywords = {multimedia, sensors, search engine},
location = {Lyon, France},
series = {WWW '12 Companion}
}

@inproceedings{10.1145/3102254.3102268,
author = {Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.},
title = {An Innovative Majority Voting Mechanism in Interactive Social Network Clustering},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102268},
doi = {10.1145/3102254.3102268},
abstract = {We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {social networks, graph clustering, majority, vote},
location = {Amantea, Italy},
series = {WIMS '17}
}

@article{10.1145/3480968,
author = {Tahir, Madiha and Halim, Zahid and Rahman, Atta Ur and Waqas, Muhammad and Tu, Shanshan and Chen, Sheng and Han, Zhu},
title = {Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3480968},
doi = {10.1145/3480968},
abstract = {The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {61},
numpages = {24},
keywords = {data-driven decision-making, affective states, pattern recognition, machine learning, Affective computing}
}

@inproceedings{10.1145/3014087.3014112,
author = {Nikiforov, Alexander and Singireja, Anastasija},
title = {Open Data and Crowdsourcing Perspectives for Smart City in the United States and Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014112},
doi = {10.1145/3014087.3014112},
abstract = {In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {171–177},
numpages = {7},
keywords = {smart city, open innovations, open data, government 2.0, e-government, crowdsourcing, civic issue tracker},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/2951894.2951901,
author = {Fan, Liju and Flood, Mark D.},
title = {An Ontology of Form PF},
year = {2016},
isbn = {9781450344074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951894.2951901},
doi = {10.1145/2951894.2951901},
abstract = {Form PF, mandated by the 2010 Dodd-Frank Act, is financial regulators' primary source for supervisory data on the risk exposures of hedge funds. Investment advisers use the Securities and Exchange Commission's (SEC) Form ADV to register, and then submit Form PF to report on each of the funds that they advise. These forms embed significant internal structure that is amenable to knowledge representation via formal ontologies, which would facilitate key tasks, such as data integration and the assurance of data integrity. We argue that ontologies should be a core and integral component of information management for financial regulatory reporting. We have tested the approach by designing, developing, and integrating ontologies in OWL/RDF in prototype to consistently describe Form ADV and Form PF with precise semantics. Preliminary results indicate that this technique is feasible in practice for data search and analysis, and will yield useful functionality. We also outline directions for future research.},
booktitle = {Proceedings of the Second International Workshop on Data Science for Macro-Modeling},
articleno = {9},
numpages = {6},
keywords = {investment advisers, data integrity, data integration, hedge funds, supervisory data, Financial regulation, knowledge representation, ontologies},
location = {San Francisco, CA, USA},
series = {DSMM'16}
}

@inproceedings{10.1145/3510858.3510863,
author = {Sun, Fangyu},
title = {Manufacturing Audit Quality Analysis Model Based on Data Mining Technology},
year = {2021},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510863},
doi = {10.1145/3510858.3510863},
abstract = {As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {1–6},
numpages = {6},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3209978.3210194,
author = {Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210194},
doi = {10.1145/3209978.3210194},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1415–1418},
numpages = {4},
keywords = {natural language processing, text mining, bibliometrics, information retrieval, information extraction, citation analysis, digital libraries},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3107091.3107093,
author = {Truong, Hong-Linh and Berardinelli, Luca},
title = {Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107093},
doi = {10.1145/3107091.3107093},
abstract = { Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures. },
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {5–8},
numpages = {4},
keywords = {elasticity, testing, Cloud, MBT, uncertainty, IoT, MDE},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

@inproceedings{10.1145/3269206.3271798,
author = {Kuo, Yu-Hsuan and Li, Zhenhui and Kifer, Daniel},
title = {Detecting Outliers in Data with Correlated Measures},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271798},
doi = {10.1145/3269206.3271798},
abstract = {Advances in sensor technology have enabled the collection of large-scale datasets. Such datasets can be extremely noisy and often contain a significant amount of outliers that result from sensor malfunction or human operation faults. In order to utilize such data for real-world applications, it is critical to detect outliers so that models built from these datasets will not be skewed by outliers. In this paper, we propose a new outlier detection method that utilizes the correlations in the data (e.g., taxi trip distance vs. trip time). Different from existing outlier detection methods, we build a robust regression model that explicitly models the outliers and detects outliers simultaneously with the model fitting. We validate our approach on real-world datasets against methods specifically designed for each dataset as well as the state of the art outlier detectors. Our outlier detection method achieves better performances, demonstrating the robustness and generality of our method. Last, we report interesting case studies on some outliers that result from atypical events.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {287–296},
numpages = {10},
keywords = {robust regression, contextual outlier detection},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3132847.3132894,
author = {Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai},
title = {Destination-Aware Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132894},
doi = {10.1145/3132847.3132894},
abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {spatial task assignment, user mobility, spatial crowdsourcing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1109/TASLP.2015.2512041,
author = {Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan},
title = {An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512041},
doi = {10.1109/TASLP.2015.2512041},
abstract = {Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {432–444},
numpages = {13},
keywords = {topic model, sentiment classification, cross-language}
}

@article{10.1145/3436239,
author = {Liu, Zhicheng and Zhang, Yang and Huang, Ruihong and Chen, Zhiwei and Song, Shaoxu and Wang, Jianmin},
title = {EXPERIENCE: Algorithms and Case Study for Explaining Repairs with Uniform Profiles over IoT Data},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3436239},
doi = {10.1145/3436239},
abstract = {IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {18},
numpages = {17},
keywords = {time series, outlier repairs, Outlier explanation, data profiling}
}

@article{10.1145/3310230,
author = {Fan, Wenfei},
title = {Dependencies for Graphs: Challenges and Opportunities},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3310230},
doi = {10.1145/3310230},
abstract = {What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {5},
numpages = {12},
keywords = {Dependencies, graphs, validation, dependency discovery, satisfiability, error detection, certain fixes, implication}
}

@inproceedings{10.1145/1710035.1710077,
author = {Liu, Jing and Cho, Sungchol and Han, Sunyoung and Kim, Keecheon and Ha, YoungGuk and Choe, Jongwon and Kamolphiwong, Sinchai and Choo, Hyunseung and Shin, Yongtae and Kim, Chinchol},
title = {Establishment and Traffic Measurement of Overlay Multicast Testbed in KOREN, THaiREN and TEIN2},
year = {2009},
isbn = {9781605585369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1710035.1710077},
doi = {10.1145/1710035.1710077},
abstract = {Nowadays not only many of research works with various international networks are increasing more and more but also commercial works are increasing with different international networks. In this paper, we have constructed the overlay multicast testbed with KOREN and TEIN2 network, and then we also analyze and research many works with the data got from the testbed experiments, and research works for speed of transmission and transmission security when the data is forwarded to several various international network. We work out the process of problem based on several data of experiments. We analyze these problems and propose the research way to other researchers in overlay multicast area, and we also provide these useful results to other researchers in this area.},
booktitle = {Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems},
articleno = {42},
numpages = {7},
keywords = {KOREN, UniNet, overlay multicast, overlay, multicast, measurement, TEIN2},
location = {Nice, France},
series = {Mobility '09}
}

@inproceedings{10.1145/3209415.3209459,
author = {Millard, Jeremy and Thomasen, Louise and Pastrovic, Goran and Cvetkovic, Bojan},
title = {A Roadmap for E-Participation and Open Government: Empirical Evidence from the Western Balkans},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209459},
doi = {10.1145/3209415.3209459},
abstract = {This paper describes why and how a conceptual framework for e-participation and open government has been developed and applied to six aspirant EU countries in the Western Balkans. It provides a rationale and background, and then examines the main academic and other relevant sources used. This is followed by an overview of the conceptual framework and a description of its main elements. Finally, the paper examines international data on e-participation covering the Western Balkan countries, uses this to examine the results of applying the conceptual framework in each country, and then provides conclusions and recommendations.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {191–198},
numpages = {8},
keywords = {Transparency, Open government, Collaboration, Participation, E-Participation, E-government, Policy},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/3208159.3208187,
author = {Gavrilova, Marina L.},
title = {Machine Learning for Social Behavior Understanding},
year = {2018},
isbn = {9781450364010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208159.3208187},
doi = {10.1145/3208159.3208187},
abstract = {Human brain has an ability to perform a massive processing of auxiliary information such as visual cues, cognitive and social interactions, contextual and spatio-temporal data. Similarly to a human brain, social behavioral cues can aid the reliable decision-making of a biometric security system. Being an integral part of human behavior, social interactions are likely to possess unique behavioral patterns. This state-of-the-art review paper discusses an emerging person recognition approach based on the in-depth analysis of individuals' social behavior in order to enhance the performance of a traditional biometric system. The social behavioral information can be mined from their offline or online interactions, and can be identified as a set of Social Behavioral Biometric (SBB) features. These features could be used on their own or further combined with other behavioral and physiological patters, and classification can be enhanced by the use of machine learning approaches. An overview of open problems and challenges as well as applications of studying social behavior in various domains concludes this paper.},
booktitle = {Proceedings of Computer Graphics International 2018},
pages = {247–252},
numpages = {6},
keywords = {Human behavior recognition, online networks, machine learning, virtual worlds, social behavioral biometrics, decision-making},
location = {Bintan, Island, Indonesia},
series = {CGI 2018}
}

@inproceedings{10.1145/3243082.3264607,
author = {Costa, Daniel G.},
title = {On the Development of Visual Sensors with Raspberry Pi},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3264607},
doi = {10.1145/3243082.3264607},
abstract = {The increasing interest for Internet of Things (IoT) technologies has brought a lot of attention to microelectronics and sensors development. With the availability of affordable embedded platforms for countless applications, it is possible to develop low-cost programmable sensors to provide different types of data, benefiting applications in the IoT world. When cameras can be integrated to such development platforms, visual sensors can be easily created, supporting monitoring and controls functions based on the processing of images and videos. In this context, some of the most relevant details concerning the development of visual sensors with the Raspberry Pi platform are described herein, bringing fundamentals for the creation of highly programmable visual sensors.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {19–22},
numpages = {4},
keywords = {Raspberry Pi, Visual sensors, Wireless sensor networks, Internet of things, Camera},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inbook{10.1145/3480001.3480017,
author = {Jin, Ying and Gao, Ming and Yu, Jixiang},
title = {A Transformer Based Sales Prediction of Smart Container in New Retail Era},
year = {2021},
isbn = {9781450390163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480001.3480017},
abstract = {With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.},
booktitle = {2021 5th International Conference on Deep Learning Technologies (ICDLT)},
pages = {46–53},
numpages = {8}
}

@inproceedings{10.1145/2494091.2499576,
author = {Blunck, Henrik and Bouvin, Niels Olof and Franke, Tobias and Gr\o{}nb\ae{}k, Kaj and Kjaergaard, Mikkel B. and Lukowicz, Paul and W\"{u}stenberg, Markus},
title = {On Heterogeneity in Mobile Sensing Applications Aiming at Representative Data Collection},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499576},
doi = {10.1145/2494091.2499576},
abstract = {Gathering representative data using mobile sensing to answer research questions is becoming increasingly popular, driven by growing ubiquity and sensing capabilities of mobile devices. However, there are pitfalls along this path, which introduce heterogeneity in the gathered data, and which are rooted in the diversity of the involved device platforms, hardware, software versions and participants. Thus, we, as a research community, need to establish good practices and methodologies for addressing this issue in order to help ensure that, e.g., scientific results and policy changes based on collective, mobile sensed data are valid. In this paper, we aim to inform researchers and developers about mobile sensing data heterogeneity and ways to combat it. We do so via distilling a vocabulary of underlying causes, and via describing their effects on mobile sensing---building on experiences from three projects within citizen science, crowd awareness and trajectory tracking.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1087–1098},
numpages = {12},
keywords = {mobile sensing, data collection, representativeness, heterogeneity},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@article{10.1145/3484945,
author = {Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman},
title = {A Novel Hybrid Approach for Multi-Dimensional Data Anonymization for Apache Spark},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3484945},
doi = {10.1145/3484945},
abstract = {Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.},
journal = {ACM Trans. Priv. Secur.},
month = {nov},
articleno = {5},
numpages = {25},
keywords = {data anonymization, Spark, resilient distributed dataset (RDD), multi-dimensional data, Mondrian}
}

@inbook{10.1145/3310205.3310208,
title = {Outlier Detection},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310208},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3085504.3085505,
author = {Gorenflo, Christian and Golab, Lukasz and Keshav, Srinivasan},
title = {Managing Sensor Data Streams: Lessons Learned from the WeBike Project},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085505},
doi = {10.1145/3085504.3085505},
abstract = {We present insights on data management resulting from a field deployment of approximately 30 sensor-equipped electric bicycles (e-bikes) at the University of Waterloo. The trial has been in operation for the last two-and-a-half years, and we have collected and analyzed more than 150 gigabytes of data. We discuss best practices for the entire data management process, spanning data collection, extract-transform-load, data cleaning, and choosing a suitable data management ecosystem. We also comment on how our experiences will inform the design of a future large-scale field trial involving several thousand fully-instrumented e-bikes.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {1},
numpages = {11},
keywords = {Time series data management, Data management for the Internet of Things (IoT), Data feed management},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@article{10.1145/3522592,
author = {Cai, Jianghui and Yang, Yuqing and Yang, Haifeng and Zhao, Xujun and Hao, Jing},
title = {ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522592},
doi = {10.1145/3522592},
abstract = {The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, A two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this paper. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS are verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
keywords = {influence space, ranking factor, data pre-processing scheme, data representation, noise identification}
}

@article{10.1109/TCBB.2016.2535251,
author = {Ma, Tianle and Zhang, Aidong},
title = {Omics Informatics: From Scattered Individual Software Tools to Integrated Workflow Management Systems},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2535251},
doi = {10.1109/TCBB.2016.2535251},
abstract = {Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {926–946},
numpages = {21}
}

@inproceedings{10.1145/3469213.3470272,
author = {Hu, Yerong and He, Xiangzhen and Zhang, Yihao and Zeng, Jia and Yang, Huaiyuan and Zhou, Shuaihang},
title = {Research and Application of Digital Collection Method of Human Movement},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470272},
doi = {10.1145/3469213.3470272},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {71},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3003733.3003761,
author = {Gatziolis, Kleanthis and Boucouvalas, Anthony C.},
title = {User Profile Extraction Engine},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003761},
doi = {10.1145/3003733.3003761},
abstract = {The Internet is overwhelmed by a huge amount of information every day and every user has different interests from another. It is therefore important that this information is filtered and sorted according to their preferences. Thus, the profiling systems exploit particularities and preferences of each user and finally they can be studied or used by other applications or humans. This paper analyzes the methods of collecting data (data gathering), and the ways in which this information can be used - filtered so as to create knowledge. A user profile extraction engine is presented and analyzed.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {41},
numpages = {6},
keywords = {Mobile shopping, E-Commerce, Retailing, e-Shopping, User Profiling},
location = {Patras, Greece},
series = {PCI '16}
}

@article{10.1109/TCBB.2019.2903804,
author = {Chen, Yiyuan and Wang, Yufeng and Cao, Liang and Jin, Qun},
title = {CCFS: A Confidence-Based Cost-Effective Feature Selection Scheme for Healthcare Data Classification},
year = {2021},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2903804},
doi = {10.1109/TCBB.2019.2903804},
abstract = {Feature selection (FS) is one of the fundamental data processing techniques in various machine learning algorithms, especially for classification of healthcare data. However, it is a challenging issue due to the large search space. Binary Particle Swarm Optimization (BPSO) is an efficient evolutionary computation technique, and has been widely used in FS. In this paper, we proposed a Confidence-based and Cost-effective feature selection (CCFS) method using BPSO to improve the performance of healthcare data classification. Specifically, first, CCFS improves search effectiveness by developing a new updating mechanism that designs the feature confidence to explicitly take into account the fine-grained impact of each dimension in the particle on the classification performance. The feature confidence is composed of two measurements: the correlation between feature and categories, and historically selected frequency of each feature. Second, considering the fact that the acquisition costs of different features are naturally different, especially for medical data, and should be fully taken into account in practical applications, besides the classification performance, the feature cost and the feature reduction ratio are comprehensively incorporated into the design of fitness function. The proposed method has been verified in various UCI public datasets and compared with various benchmark schemes. The thoroughly experimental results show the effectiveness of the proposed method, in terms of accuracy and feature selection cost.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {may},
pages = {902–911},
numpages = {10}
}

@article{10.1145/3502736,
author = {Varde, Aparna S.},
title = {Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3502736},
doi = {10.1145/3502736},
abstract = {Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {86},
numpages = {52},
keywords = {estimation, scientific applications, Applied research, classification, graphical data mining, machine learning, domain knowledge, clustering, predictive analytics}
}

@article{10.1145/3485875,
author = {Yang, Qiang},
title = {Toward Responsible AI: An Overview of Federated Learning for User-Centered Privacy-Preserving Computing},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3485875},
doi = {10.1145/3485875},
abstract = {With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {oct},
articleno = {32},
numpages = {22},
keywords = {data security, decentralized AI, Federated learning, machine learning, privacy-preserving computing, responsible AI, blockchain, user privacy}
}

@inbook{10.1145/3313831.3376485,
author = {Gathani, Sneha and Lim, Peter and Battle, Leilani},
title = {Debugging Database Queries: A Survey of Tools, Techniques, and Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376485},
abstract = {Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16}
}

@article{10.1145/3434173,
author = {Hockenhull, Michael and Cohn, Marisa Leavitt},
title = {Speculative Data Work &amp; Dashboards: Designing Alternative Data Visions},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434173},
doi = {10.1145/3434173},
abstract = {This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {264},
numpages = {31},
keywords = {business intelligence, ethnography, speculative design, data work, data visualization}
}

@inproceedings{10.1145/3400903.3400908,
author = {Schuler, Robert and Czajkowski, Karl and D'Arcy, Mike and Tangmunarunkit, Hongsuda and Kesselman, Carl},
title = {Towards Co-Evolution of Data-Centric Ecosystems},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400908},
doi = {10.1145/3400903.3400908},
abstract = {Database evolution is a notoriously difficult task, and it is exacerbated by the necessity to evolve database-dependent applications. As science becomes increasingly dependent on sophisticated data management, the need to evolve an array of database-driven systems will only intensify. In this paper, we present an architecture for data-centric ecosystems that allows the components to seamlessly co-evolve by centralizing the models and mappings at the data service and pushing model-adaptive interactions to the database clients. Boundary objects fill the gap where applications are unable to adapt and need a stable interface to interact with the components of the ecosystem. Finally, evolution of the ecosystem is enabled via integrated schema modification and model management operations. We present use cases from actual experiences that demonstrate the utility of our approach.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {4},
numpages = {12},
keywords = {model management, application-database co-evolution, software ecosystems, schema evolution},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

@inproceedings{10.1145/3384544.3384611,
author = {Hartner, Raphael and Mezhuyev, Vitaliy and Tschandl, Martin and Bischof, Christian},
title = {Digital Shop Floor Management: A Practical Framework For Implementation},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384611},
doi = {10.1145/3384544.3384611},
abstract = {In the context of manufacturing, shop floor management (SFM) is employed to ensure efficient production operations and workflows. Advanced technologies and methods can be used to improve the SFM and achieve close to real-time responsiveness. Even though there is a number of research available for the digitalized SFM (DSFM), a supportive framework for implementation purposes was not considered yet. Consequently, this paper utilizes concepts from related disciplines and research areas to derive an architectural framework for a DSFM. This particular architecture is then implemented to ensure its practicability and foster the understanding of challenges and opportunities. The proposed multi-layer framework and supportive methods can be employed by manufacturing companies to implement a DSFM focused on interoperability, security and low-latency.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {41–45},
numpages = {5},
keywords = {Shop Floor Management, Lean Production, Cloud Computing, Smart Production, Middleware, Fog Computing, Mist Computing, Retrofitting, Industry 4.0},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@inbook{10.1145/3447404.3447406,
author = {Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark D.},
title = {Introduction},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447406},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {1–13},
numpages = {13}
}

@inbook{10.1145/3368089.3417055,
author = {Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.},
title = {Towards Intelligent Incident Management: Why We Need It and How We Make It},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417055},
abstract = {The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1487–1497},
numpages = {11}
}

@inproceedings{10.1145/3437800.3439203,
author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
title = {High Performance Computing Education: Current Challenges and Future Directions},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439203},
doi = {10.1145/3437800.3439203},
abstract = {High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd).  Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future.},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {51–74},
numpages = {24},
keywords = {iticse working group, contemporary computing education, hpc education, high performance computing, computer science education, high-performance computing curricula},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}

@inproceedings{10.1145/3357492.3358628,
author = {Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin},
title = {Privacy and Information Protection for a New Generation of City Services},
year = {2019},
isbn = {9781450369787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357492.3358628},
doi = {10.1145/3357492.3358628},
abstract = {This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.},
booktitle = {Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {5},
numpages = {6},
keywords = {Digital equity, government services, Digital Inclusion, Privacy, Automatic decision systems},
location = {Portland, OR, USA},
series = {SCC '19}
}

@inproceedings{10.1145/3340531.3412105,
author = {Ranbaduge, Thilina and Schnell, Rainer},
title = {Securing Bloom Filters for Privacy-Preserving Record Linkage},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412105},
doi = {10.1145/3340531.3412105},
abstract = {Privacy-preserving record linkage (PPRL) facilitates the matching of records that correspond to the same real-world entities across different databases while preserving the privacy of the individuals in these databases. A Bloom filter (BF) is a space efficient probabilistic data structure that is becoming popular in PPRL as an efficient privacy technique to encode sensitive information in records while still enabling approximate similarity computations between attribute values. However, BF encoding is susceptible to privacy attacks which can re-identify the values that are being encoded. In this paper we propose two novel techniques that can be applied on BF encoding to improve privacy against attacks. Our techniques use neighbouring bits in a BF to generate new bit values. An empirical study on large real databases shows that our techniques provide high security against privacy attacks, and achieve better similarity computation accuracy and linkage quality compared to other privacy improvements that can be applied on BF encoding.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2185–2188},
numpages = {4},
keywords = {sliding window, hardening, random sampling, perturbation, xor},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3233347.3233373,
author = {Hagemann, Simon and Stark, Rainer},
title = {Automated Body-in-White Production System Design: Data-Based Generation of Production System Configurations},
year = {2018},
isbn = {9781450364720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233347.3233373},
doi = {10.1145/3233347.3233373},
abstract = {Design processes for production systems (PS) in the automotive body-in-white (BIW) sector tie up tremendous resources. Current challenges like the continuous increase of product variants and product complexity have direct impact on the required planning effort in production system design (PSD), which is currently increasing significantly. Analysis of these design processes have revealed a high potential for process automatization. In order to achieve this, suitable methods are required as well as a data basis of reasonable quality. Both methods and data basis are deeply investigated in this paper. The investigations' results create a solid basis for further research in the young field of automated BIW PSD.},
booktitle = {Proceedings of the 4th International Conference on Frontiers of Educational Technologies},
pages = {192–196},
numpages = {5},
keywords = {knowledge-engineering, automotive, process automatization, production system design, optimization, body-in-white, ruled-based algorithms, artificial intelligence, data mining},
location = {Moscow, Russian Federation},
series = {ICFET '18}
}

@article{10.1145/3502288,
author = {Barth, Susanne and Ionita, Dan and Hartel, Pieter},
title = {Understanding Online Privacy—A Systematic Review of Privacy Visualizations and Privacy by Design Guidelines},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3502288},
doi = {10.1145/3502288},
abstract = {Privacy visualizations help users understand the privacy implications of using an online service. Privacy by Design guidelines provide generally accepted privacy standards for developers of online services. To obtain a comprehensive understanding of online privacy, we review established approaches, distill a unified list of 15 privacy attributes and rank them based on perceived importance by users and privacy experts. We then discuss similarities, explain notable differences, and examine trends in terms of the attributes covered. Finally, we show how our results provide a foundation for user-centric privacy visualizations, inspire best practices for developers, and give structure to privacy policies.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {63},
numpages = {37},
keywords = {privacy factors, privacy labels, Privacy attributes, privacy icons, privacy by design}
}

@inproceedings{10.1145/3209281.3209326,
author = {Haak, Elise and Ubacht, Jolien and Van den Homberg, Marc and Cunningham, Scott and Van den Walle, Bartel},
title = {A Framework for Strengthening Data Ecosystems to Serve Humanitarian Purposes},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209326},
doi = {10.1145/3209281.3209326},
abstract = {The incidence of natural disasters worldwide is increasing. As a result, a growing number of people is in need of humanitarian support, for which limited resources are available. This requires an effective and efficient prioritization of the most vulnerable people in the preparedness phase, and the most affected people in the response phase of humanitarian action. Data-driven models have the potential to support this prioritization process. However, the applications of these models in a country requires a certain level of data preparedness. To achieve this level of data preparedness on a large scale we need to know how to facilitate, stimulate and coordinate data-sharing between humanitarian actors. We use a data ecosystem perspective to develop success criteria for establishing a "humanitarian data ecosystem". We first present the development of a general framework with data ecosystem governance success criteria based on a systematic literature review. Subsequently, the applicability of this framework in the humanitarian sector is assessed through a case study on the "Community Risk Assessment and Prioritization toolbox" developed by the Netherlands Red Cross. The empirical evidence led to the adaption the framework to the specific criteria that need to be addressed when aiming to establish a successful humanitarian data ecosystem.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {85},
numpages = {9},
keywords = {humanitarian sector, data preparedness, data ecosystem, governance, framework},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3488560.3498421,
author = {Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos},
title = { 'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498421},
doi = {10.1145/3488560.3498421},
abstract = {The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {48–56},
numpages = {9},
keywords = {tip of the tongue known item retrieval, known item retrieval},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3325112.3325222,
author = {S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil},
title = {Opening Privacy Sensitive Microdata Sets in Light of GDPR},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325222},
doi = {10.1145/3325112.3325222},
abstract = {To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {314–323},
numpages = {10},
keywords = {Criminal justice data, GDPR, Data protection, Justice domain data, Microdata, Open data, Privacy},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/3428502.3428514,
author = {Papadopoulos, Theodoros and Charalabidis, Yannis},
title = {What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428514},
doi = {10.1145/3428502.3428514},
abstract = {The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {100–111},
numpages = {12},
keywords = {machine learning, AI strategies, NLP, topic modelling, Automated Text Analysis, document similarity},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/2600821.2600847,
author = {Moazeni, Ramin and Link, Daniel and Boehm, Barry},
title = {COCOMO II Parameters and IDPD: Bilateral Relevances},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600847},
doi = {10.1145/2600821.2600847},
abstract = { The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed. },
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {20–24},
numpages = {5},
keywords = {incremental development, scale factors, Parametric cost estimation, cost drivers, IDPD},
location = {Nanjing, China},
series = {ICSSP 2014}
}


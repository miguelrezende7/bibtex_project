@article{SHIROSAKIMARCALDESOUZA2021160,
title = {Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”},
journal = {Sustainable Production and Consumption},
volume = {26},
pages = {160-171},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2020.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S2352550920303286},
author = {Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}},
keywords = {Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors},
abstract = {The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.}
}
@article{CAUDAI20215762,
title = {AI applications in functional genomics},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {5762-5790},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2001037021004311},
author = {Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo},
keywords = {Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning},
abstract = {We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.}
}
@incollection{KRISHNAN202099,
title = {5 - Pharmacy industry applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {99-111},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000053},
author = {Krish Krishnan},
keywords = {Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests},
abstract = {One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.}
}
@article{MOON2018304,
title = {Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm},
journal = {Computers and Electronics in Agriculture},
volume = {154},
pages = {304-313},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918303697},
author = {Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son},
keywords = {Smart farm, Lossy compression, IoT, Signal processing, Data fidelity},
abstract = {As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.}
}
@article{XU2022105396,
title = {An overview of visualization and visual analytics applications in water resources management},
journal = {Environmental Modelling & Software},
pages = {105396},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105396},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001025},
author = {Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal},
abstract = {Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of water resources management applications.}
}
@article{LI2020124178,
title = {Forecasting crude oil price with multilingual search engine data},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {551},
pages = {124178},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.124178},
url = {https://www.sciencedirect.com/science/article/pii/S037843712030025X},
author = {Jingjing Li and Ling Tang and Shouyang Wang},
keywords = {Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence},
abstract = {In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.}
}
@article{KAMPKER2018120,
title = {Enabling Data Analytics in Large Scale Manufacturing},
journal = {Procedia Manufacturing},
volume = {24},
pages = {120-127},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305341},
author = {Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil},
keywords = {Automotive, Manufacturing, Data Analytics, Big Data, Optimization},
abstract = {Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.}
}
@article{HE2019320,
title = {Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {108},
pages = {320-339},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18312543},
author = {Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen},
keywords = {Big data, Floating car data, Urban road network, Traffic congestion, Road intersection},
abstract = {Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.}
}
@article{JANSSEN2020101493,
title = {Data governance: Organizing data for trustworthy Artificial Intelligence},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101493},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101493},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302719},
author = {Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski},
keywords = {Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks},
abstract = {The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.}
}
@article{ZHANG2020102659,
title = {Design and application of a personal credit information sharing platform based on consortium blockchain},
journal = {Journal of Information Security and Applications},
volume = {55},
pages = {102659},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102659},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620308139},
author = {Jing Zhang and Rong Tan and Chunhua Su and Wen Si},
keywords = {Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting},
abstract = {The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.}
}
@article{MOZZONI2022402,
title = {Transfer’s monitoring in bus transit services by Automatic Vehicle Location data},
journal = {Transportation Research Procedia},
volume = {60},
pages = {402-409},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009534},
author = {Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino},
keywords = {Big Data, Transfer diagnosis, Automatic Vehicle Location Data},
abstract = {Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.}
}
@incollection{MILOSEVIC201639,
title = {Chapter 2 - Real-Time Analytics},
editor = {Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi},
booktitle = {Big Data},
publisher = {Morgan Kaufmann},
pages = {39-61},
year = {2016},
isbn = {978-0-12-805394-2},
doi = {https://doi.org/10.1016/B978-0-12-805394-2.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053942000027},
author = {Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi},
keywords = {Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm},
abstract = {Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.}
}
@article{TAGLIAFERRI201873,
title = {A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {73-78},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518300621},
author = {Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni},
keywords = {Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management},
abstract = {The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.}
}
@incollection{KATARINA202283,
title = {Chapter 6 - Innovative technologies in precision healthcare},
editor = {Debmalya Barh},
booktitle = {Biotechnology in Healthcare},
publisher = {Academic Press},
pages = {83-102},
year = {2022},
isbn = {978-0-323-89837-9},
doi = {https://doi.org/10.1016/B978-0-323-89837-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898379000164},
author = {Šoltýs Katarína and Kľoc Marek and Rabajdová Miroslava and Mareková Mária},
keywords = {Precision healthcare, biotechnology, emerging technologies, big data},
abstract = {Precision medicine is the intersection of data science, analytics, and biomedicine in creating a healthy learning system that conducts research in the context of clinical care while optimizing the tools and information used to provide better outcomes for patients. Emerging technologies represent a novel, innovative, and fast-evolving trend within a particular field. Among the latest trends as virtual reality, robotics, wearable, and implantable sensors, and removable tattoos, together with nanotechnologies, 3D printing, and others are considered. In addition, new advanced computing technologies including artificial intelligence, machine learning (ML), big data mining, and cloud computing form an integral part of personalized healthcare. Personalized medicine is not necessarily the same as precision medicine. From the point of view of technology development, precision medicine is an intermediate step to personalized medicine, which will be much more complex and will require even more data. There is a big challenge to combine multiomics approaches in analysis, as we can see in bioinformatics that there are a lot of techniques in one area. Analysis of more than one area such as the genome, transcriptome even microbiome, starts exponentially grown on science field. The integration of multiomics data analysis and machine learning can have led to the discovery of new biomarkers, and improve of differential diagnostics of latent diseases. In this chapter, we describe the use of emerging technologies as well as bioengineering and ML for precision healthcare.}
}
@article{JIANG2021172,
title = {Data consistency method of heterogeneous power IOT based on hybrid model},
journal = {ISA Transactions},
volume = {117},
pages = {172-179},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821000665},
author = {Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li},
keywords = {Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method},
abstract = {The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.}
}
@article{PATONAI2021e00203,
title = {Integrating trophic data from the literature: The Danube River food web},
journal = {Food Webs},
volume = {28},
pages = {e00203},
year = {2021},
issn = {2352-2496},
doi = {https://doi.org/10.1016/j.fooweb.2021.e00203},
url = {https://www.sciencedirect.com/science/article/pii/S2352249621000161},
author = {Katalin Patonai and Ferenc Jordán},
keywords = {Aggregation, Danube River, Food web, Incomplete data, Taxonomy},
abstract = {In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.}
}
@article{MATTIOLI2022453,
title = {Information Quality: the cornerstone for AI-based Industry 4.0},
journal = {Procedia Computer Science},
volume = {201},
pages = {453-460},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004720},
author = {Juliette Mattioli and Pierre-Olivier Robic and Emeric Jesson},
keywords = {Industry 4.0, Data-driven AI, Knowledge-based AI, Data Quality, Information Quality},
abstract = {AI becomes a key enabler for Industry 4.0. Data / information quality become a real cornerstone on the overall process from user expectation to products / systems / solutions in a consistent perspective in order to ensure quality of the manufacturing production. This paper highlights some key characteristics in terms of information quality required to implement an effective AI based monitoring framework, in order to achieve operational excellence in Industry.}
}
@article{AGANY20201704,
title = {Assessment of vector-host-pathogen relationships using data mining and machine learning},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {1704-1721},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020303202},
author = {Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba},
keywords = {Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease},
abstract = {Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.}
}
@article{LEE201820,
title = {Industrial Artificial Intelligence for industry 4.0-based manufacturing systems},
journal = {Manufacturing Letters},
volume = {18},
pages = {20-23},
year = {2018},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213846318301081},
author = {Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare},
keywords = {Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems},
abstract = {The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.}
}
@incollection{KRISHNAN202085,
title = {4 - Scientific research applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {85-97},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000041},
author = {Krish Krishnan},
keywords = {CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model},
abstract = {Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.}
}
@article{LEAL2021100172,
title = {Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production},
journal = {Big Data Research},
volume = {24},
pages = {100172},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100172},
url = {https://www.sciencedirect.com/science/article/pii/S221457962030040X},
author = {Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier},
keywords = {ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts},
abstract = {Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.}
}
@incollection{NASSEHI2022317,
title = {Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {317-348},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000026},
author = {Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu},
keywords = {Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0},
abstract = {With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.}
}
@article{KARIM2016214,
title = {Maintenance Analytics – The New Know in Maintenance},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {214-219},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324612},
author = {Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar},
keywords = {big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support},
abstract = {Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.}
}
@article{SETER201959,
title = {The data driven transport research train is leaving the station. Consultants all aboard?},
journal = {Transport Policy},
volume = {80},
pages = {59-69},
year = {2019},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X17305589},
author = {Hanne Seter and Petter Arnesen and Odd André Hjelkrem},
abstract = {This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.}
}
@article{HE201714946,
title = {Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14946-14951},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2546},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334717},
author = {Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat},
keywords = {Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis},
abstract = {Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.}
}
@article{LEE2020157,
title = {Machine learning for enterprises: Applications, algorithm selection, and challenges},
journal = {Business Horizons},
volume = {63},
number = {2},
pages = {157-170},
year = {2020},
note = {ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301521},
author = {In Lee and Yong Jae Shin},
keywords = {Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities},
abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.}
}
@article{BOHNSACK2019799,
title = {What the hack? A growth hacking taxonomy and practical applications for firms},
journal = {Business Horizons},
volume = {62},
number = {6},
pages = {799-818},
year = {2019},
note = {Digital Transformation & Disruption},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301247},
author = {René Bohnsack and Meike Malena Liesner},
keywords = {Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data},
abstract = {As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.}
}
@article{EHRING2021163,
title = {SMART standards - concept for the automated transfer of standard contents into a machine-actionable form},
journal = {Procedia CIRP},
volume = {100},
pages = {163-168},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004868},
author = {Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah},
keywords = {SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg},
abstract = {Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as "connectivity of machines", "artificial intelligence", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps "extraction", "modeling", “modification”, "fusion and storage", "provision" and "application", to digitalize existing content, is presented and discussed.}
}
@article{RAJ2021103107,
title = {A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103107},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103107},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001284},
author = {Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato},
abstract = {There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.}
}
@article{ABRAHAM2019424,
title = {Data governance: A conceptual framework, structured review, and research agenda},
journal = {International Journal of Information Management},
volume = {49},
pages = {424-438},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219300787},
author = {Rene Abraham and Johannes Schneider and Jan {vom Brocke}},
keywords = {Data governance, Information governance, Conceptual framework, Literature review, Research agenda},
abstract = {Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.}
}
@article{HAMILTON2020103926,
title = {Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development},
journal = {Food Quality and Preference},
volume = {83},
pages = {103926},
year = {2020},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2020.103926},
url = {https://www.sciencedirect.com/science/article/pii/S0950329319308304},
author = {Leah M. Hamilton and Jacob Lahne},
keywords = {Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning},
abstract = {As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.}
}
@article{RIKHARDSSON201837,
title = {Business intelligence & analytics in management accounting research: Status and future focus},
journal = {International Journal of Accounting Information Systems},
volume = {29},
pages = {37-58},
year = {2018},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300616},
author = {Pall Rikhardsson and Ogan Yigitbasioglu},
keywords = {Business intelligence, Management accounting, Big data, Analytics},
abstract = {Executives see technology, data and analytics as a transforming force in business. Many organizations are therefore implementing business intelligence & analytics (BI&A) technologies to support reporting and decision-making. Traditionally, management accounting is the primary support for decision-making and control in an organization. As such, it has clear links to and can benefit from applying BI&A technologies. This indicates an interesting research area for accounting and AIS researchers. However, a review of the literature in top accounting and information systems journals indicates that to date, little research has focused on this link. This article reviews the literature, points to several research gaps and proposes a framework for studying the relationship between BI&A and management accounting.}
}
@article{QIU2020115,
title = {Research on Cost Management Optimization of Financial Sharing Center Based on RPA},
journal = {Procedia Computer Science},
volume = {166},
pages = {115-119},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301538},
author = {Yu Lian Qiu and Guo Fang Xiao},
keywords = {RPA, Financial shared service center, Cost management, Process optimization, Big data},
abstract = {With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, "Cloud Purchasing Platform" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.}
}
@article{CHOUVARDA201522,
title = {Connected health and integrated care: Toward new models for chronic disease management},
journal = {Maturitas},
volume = {82},
number = {1},
pages = {22-27},
year = {2015},
note = {PERSONALIZED HEALTHCARE FOR MIDLIFE AND BEYOND},
issn = {0378-5122},
doi = {https://doi.org/10.1016/j.maturitas.2015.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0378512215006052},
author = {Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras},
keywords = {Connected health, Integrated care, Personal health system, Electronic health},
abstract = {The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.}
}
@article{STEVENS201515,
title = {Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems},
journal = {Spatial and Spatio-temporal Epidemiology},
volume = {13},
pages = {15-29},
year = {2015},
issn = {1877-5845},
doi = {https://doi.org/10.1016/j.sste.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877584515000179},
author = {Kim B. Stevens and Dirk U. Pfeiffer},
keywords = {Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information},
abstract = {During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.}
}
@article{POWELL2022100261,
title = {Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100261},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000595},
author = {Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov},
keywords = {Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains},
abstract = {The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.}
}
@article{LIANG201887,
title = {Application and research of global grid database design based on geographic information},
journal = {Global Energy Interconnection},
volume = {1},
number = {1},
pages = {87-95},
year = {2018},
issn = {2096-5117},
doi = {https://doi.org/10.14171/j.2096-5117.gei.2018.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2096511718300112},
author = {Xuming Liang},
keywords = {Big data collection, Geographic information, Grid database, Data mining},
abstract = {Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.}
}
@article{DEY20191317,
title = {Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {11},
pages = {1317-1335},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0735109719302360},
author = {Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick},
keywords = {artificial intelligence, cardiovascular imaging, deep learning, machine learning},
abstract = {Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.}
}
@incollection{WANG2018247,
title = {Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior},
editor = {Junfeng Wang},
booktitle = {Safety Theory and Control Technology of High-Speed Train Operation},
publisher = {Academic Press},
pages = {247-266},
year = {2018},
isbn = {978-0-12-813304-0},
doi = {https://doi.org/10.1016/B978-0-12-813304-0.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133040000086},
author = {Junfeng Wang},
keywords = {High-speed railway, big data, train running status, monitoring, early warning},
abstract = {In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.}
}
@article{LAIFA2021981,
title = {Train delay prediction in Tunisian railway through LightGBM model},
journal = {Procedia Computer Science},
volume = {192},
pages = {981-990},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015891},
author = {Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}},
keywords = {Delay prediction, Data Analysis, Machine learning, LightGBM},
abstract = {Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.}
}
@article{SAVOLAINEN202095,
title = {Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {95-100},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301592},
author = {P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh},
keywords = {Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance},
abstract = {Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.}
}
@article{HURTER2014207,
title = {Interactive image-based information visualization for aircraft trajectory analysis},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {47},
pages = {207-227},
year = {2014},
note = {Special Issue: Emerging Technologies Special Issue of ICTIS 2013 – Guest Editors: Liping Fu and Ming Zhong and Special Issue: Visualization & Visual Analytics in Transportation – Guest Editors: Patricia S. Hu and Michael L. Pack},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2014.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X14000710},
author = {C. Hurter and S. Conversy and D. Gianazza and A.C. Telea},
keywords = {Trajectory manipulation, Air traffic control, Image based techniques, Information visualization},
abstract = {Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.}
}
@incollection{20151,
title = {Chapter One - Introduction},
editor = {Olivier Curé and Guillaume Blin},
booktitle = {RDF Database Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-8},
year = {2015},
isbn = {978-0-12-799957-9},
doi = {https://doi.org/10.1016/B978-0-12-799957-9.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780127999579000018},
keywords = {RDF, Web of Data, Semantic Web, Big Data, Data management},
abstract = {This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.}
}
@article{LAU2019357,
title = {A survey of data fusion in smart city applications},
journal = {Information Fusion},
volume = {52},
pages = {357-374},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519300326},
author = {Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan},
keywords = {Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification},
abstract = {The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.}
}
@article{KOURTESIS2014307,
title = {Semantic-based QoS management in cloud systems: Current status and future challenges},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {307-323},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1300232X},
author = {Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis},
keywords = {Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data},
abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.}
}
@article{SCHLEICH20183,
title = {Geometrical Variations Management 4.0: towards next Generation Geometry Assurance},
journal = {Procedia CIRP},
volume = {75},
pages = {3-10},
year = {2018},
note = {The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305948},
author = {Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack},
keywords = {Industry 4.0, Digital Twin, Geometry Assurance},
abstract = {Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.}
}
@article{LOPEZROBLES201922,
title = {30 years of intelligence models in management and business: A bibliometric review},
journal = {International Journal of Information Management},
volume = {48},
pages = {22-38},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121730244X},
author = {J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo},
keywords = {Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis},
abstract = {The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.}
}
@article{ROJO2019160,
title = {Near-ground effect of height on pollen exposure},
journal = {Environmental Research},
volume = {174},
pages = {160-169},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119302439},
author = {Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters},
keywords = {Height, Pollen, Aerobiology, Monitoring network, Big data},
abstract = {The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.}
}
@article{MIA2022100238,
title = {A privacy-preserving National Clinical Data Warehouse: Architecture and analysis},
journal = {Smart Health},
volume = {23},
pages = {100238},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000544},
author = {Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed},
keywords = {Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports},
abstract = {A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.}
}
@article{NYOMANKUTHAKRISNAWIJAYA2022106813,
title = {Data analytics platforms for agricultural systems: A systematic literature review},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106813},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106813},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001302},
author = {Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol},
keywords = {Data analytics platforms, Agriculture, Systematic literature review, Big Data},
abstract = {With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.}
}
@article{FAIEQ2017151,
title = {C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities},
journal = {Procedia Computer Science},
volume = {110},
pages = {151-158},
year = {2017},
note = {14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.06.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917312486},
author = {Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani},
keywords = {Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City},
abstract = {The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.}
}
@article{LI20191234,
title = {Internet of Things to network smart devices for ecosystem monitoring},
journal = {Science Bulletin},
volume = {64},
number = {17},
pages = {1234-1245},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319304013},
author = {Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou},
keywords = {Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device},
abstract = {Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.}
}
@article{CHEN2020104344,
title = {Robust Bayesian networks for low-quality data modeling and process monitoring applications},
journal = {Control Engineering Practice},
volume = {97},
pages = {104344},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2020.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0967066120300289},
author = {Guangjie Chen and Zhiqiang Ge},
keywords = {Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis},
abstract = {In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.}
}
@article{VIAL2019113133,
title = {Reflections on quality requirements for digital trace data in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113133},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301629},
author = {Gregory Vial},
keywords = {Digital trace data, Data quality, GitHub},
abstract = {In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.}
}
@article{NABATI2017160,
title = {Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy},
journal = {Procedia CIRP},
volume = {59},
pages = {160-165},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630960X},
author = {Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben},
keywords = {Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making},
abstract = {Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.}
}
@article{ZARKOWSKY2021260,
title = {Artificial intelligence's role in vascular surgery decision-making},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {260-267},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895796721000624},
author = {Devin S. Zarkowsky and David P. Stonko},
abstract = {ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.}
}
@article{AMUTHABALA2019233,
title = {Robust analysis and optimization of a novel efficient quality assurance model in data warehousing},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {233-244},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618318470},
author = {P. Amuthabala and R. Santhosh},
keywords = {Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning},
abstract = {The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.}
}
@article{EHWERHEMUEPHA2022108120,
title = {Cerner real-world data (CRWD) - A de-identified multicenter electronic health records database},
journal = {Data in Brief},
volume = {42},
pages = {108120},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108120},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922003304},
author = {Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster},
keywords = {Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records (EHR), HealtheIntent, HealtheDataLab™, Cerner learning Health Network (LHN)},
abstract = {Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter electronic health records. Cerner Corporation secured appropriate data use agreements and permissions from more than 100 health systems in the United States contributing to the database as of March 2022. A subset of the database was extracted to include data from only patients with SARS-CoV-2 infections and is referred to as the Cerner COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9 billion, 486 million, and 11.5 billion records in the condition, medication, procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19 Dataset consists of 130.1 million encounters from 3.8 million patients. The size and longitudinal nature of CRWD can be leveraged for advanced analytics and artificial intelligence in medical research across all specialties and is a rich source of novel discoveries on a wide range of conditions including but not limited to COVID-19.}
}
@article{HARRISON2020102672,
title = {New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {117},
pages = {102672},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20305878},
author = {Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson},
keywords = {Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data},
abstract = {High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.}
}
@article{MICHAILIDOU2022101953,
title = {EQUALITY: Quality-aware intensive analytics on the edge},
journal = {Information Systems},
volume = {105},
pages = {101953},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101953},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001496},
author = {Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas},
keywords = {Fog computing, Optimization, Sensors, Data quality},
abstract = {Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.}
}
@incollection{BOREK201423,
title = {Chapter 2 - Enterprise Information Management},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {23-38},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405547600002X},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
keywords = {EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM},
abstract = {This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.}
}
@article{VILLAHENRIKSEN202060,
title = {Internet of Things in arable farming: Implementation, applications, challenges and potential},
journal = {Biosystems Engineering},
volume = {191},
pages = {60-84},
year = {2020},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020300039},
author = {Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen},
keywords = {Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning},
abstract = {The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.}
}
@article{LIOUTAS2021103023,
title = {Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us},
journal = {Agricultural Systems},
volume = {187},
pages = {103023},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308842},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari},
keywords = {Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience},
abstract = {The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.}
}
@article{ANDRADE2019102352,
title = {Cognitive security: A comprehensive study of cognitive science in cybersecurity},
journal = {Journal of Information Security and Applications},
volume = {48},
pages = {102352},
year = {2019},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2214212618307804},
author = {Roberto O Andrade and Sang Guun Yoo},
keywords = {Cognitive security, Cognitive science, Situation awareness, Cyber operations},
abstract = {Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.}
}
@article{OMRI202023,
title = {Industrial data management strategy towards an SME-oriented PHM},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {23-36},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300467},
author = {N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni},
keywords = {Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy},
abstract = {The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.}
}
@article{TSENG2021108244,
title = {Smart product service system hierarchical model in banking industry under uncertainties},
journal = {International Journal of Production Economics},
volume = {240},
pages = {108244},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002206},
author = {Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud},
keywords = {Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory},
abstract = {This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.}
}
@article{SHARMA2018103,
title = {Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {103-120},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918311311},
author = {Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran},
keywords = {Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming},
abstract = {The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.}
}
@incollection{BRAVOMERODIO2021191,
title = {Chapter Four - Translational biomarkers in the era of precision medicine},
editor = {Gregory S. Makowski},
series = {Advances in Clinical Chemistry},
publisher = {Elsevier},
volume = {102},
pages = {191-232},
year = {2021},
issn = {0065-2423},
doi = {https://doi.org/10.1016/bs.acc.2020.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065242320300913},
author = {Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos},
keywords = {Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials},
abstract = {In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.}
}
@article{KRISTOFFERSEN2021120957,
title = {Towards a business analytics capability for the circular economy},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120957},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120957},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003899},
author = {Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li},
keywords = {Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews},
abstract = {Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.}
}
@article{MONTANS2019845,
title = {Data-driven modeling and learning in science and engineering},
journal = {Comptes Rendus Mécanique},
volume = {347},
number = {11},
pages = {845-855},
year = {2019},
note = {Data-Based Engineering Science and Technology},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1631072119301809},
author = {Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz},
keywords = {Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data},
abstract = {In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.}
}
@article{RUMSON2019598,
title = {Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk},
journal = {Science of The Total Environment},
volume = {661},
pages = {598-612},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.01.114},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719301317},
author = {Alexander G. Rumson and Stephen H. Hallett},
keywords = {Risk analytics, Adaptation, Remote sensing, Big Data},
abstract = {Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.}
}
@article{SINGH20191147,
title = {A Systemic Cybercrime Stakeholders Architectural Model},
journal = {Procedia Computer Science},
volume = {161},
pages = {1147-1155},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.227},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319386},
author = {Manmeet Mahinderjit Singh and Anizah Abu Bakar},
keywords = {Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic},
abstract = {The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.}
}
@article{ALASHHAB2021100059,
title = {Impact of coronavirus pandemic crisis on technologies and cloud computing applications},
journal = {Journal of Electronic Science and Technology},
volume = {19},
number = {1},
pages = {100059},
year = {2021},
note = {Special Section on In Silico Research on Microbiology and Public Health},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2020.100059},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X20300665},
author = {Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a}},
keywords = {Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home},
abstract = {In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.}
}
@article{LEE2020101426,
title = {Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science},
journal = {The Leadership Quarterly},
pages = {101426},
year = {2020},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2020.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1048984320300539},
author = {Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene},
keywords = {Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects},
abstract = {Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.}
}
@article{WANG2020119852,
title = {Safety informatics as a new, promising and sustainable area of safety science in the information age},
journal = {Journal of Cleaner Production},
volume = {252},
pages = {119852},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119852},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619347225},
author = {Bing Wang and Chao Wu},
keywords = {Safety science, Information science, Safety information, Safety informatics, Safety 4.0},
abstract = {Safety is a central dimension in contemporary debates on human health, loss prevention, environmental protection, sustainability, and cleaner production. In the information age, especially in the era of big data, safety information is an essential strategy for safety, and safety informatics has become a major research interest and a popular issue in the field of safety science. In recent years, safety informatics—a new area of safety science—has received increasing attention, developing greatly with successful research on the subject. The three key purposes of this paper are: (i) to analyze the historical development of safety informatics, (ii) to review the research progress of safety informatics, and (iii) to review limitations and propose future directions in the field of safety informatics. First, the development process of safety informatics is divided into four typical stages: (i) the embryonic stage (1940–1980), (ii) the initial stage (1980–1990), (iii) the formation stage (1990–2010), and (iv) the deepening stage (2010–present). Then, a review of safety informatics research is provided from seven aspects, including: (i) the discipline construction of safety informatics, (ii) theoretical safety information model, (iii) accident causation model from a safety information perspective, (iv) safety management based on safety information, (v) safety big data, (vi) safety intelligence, and (vii) safety information technology. Finally, limitations and future research directions in the safety informatics area are briefly discussed.}
}
@article{FAN201575,
title = {Temporal knowledge discovery in big BAS data for building energy management},
journal = {Energy and Buildings},
volume = {109},
pages = {75-89},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815302991},
author = {Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang},
keywords = {Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management},
abstract = {With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.}
}
@article{SALIM2020106964,
title = {Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey},
journal = {Building and Environment},
volume = {183},
pages = {106964},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106964},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320303231},
author = {Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan},
keywords = {Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities},
abstract = {The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.}
}
@article{MAO2021103052,
title = {Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models},
journal = {iScience},
volume = {24},
number = {9},
pages = {103052},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103052},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221010208},
author = {Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang},
keywords = {Data analysis in structural biology, Machine learning, Structural biology},
abstract = {Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.}
}
@article{HUSEIEN2022100116,
title = {A review on 5G technology for smart energy management and smart buildings in Singapore},
journal = {Energy and AI},
volume = {7},
pages = {100116},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000653},
author = {Ghasan Fahim Huseien and Kwok Wei Shah},
keywords = {5G technology, Sustainability, Smart building, Facilities management, Build environment},
abstract = {Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{BURNS2022420,
title = {Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World},
journal = {Clinical Therapeutics},
volume = {44},
number = {3},
pages = {420-437},
year = {2022},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149291822000170},
author = {Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell},
keywords = {Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making},
abstract = {Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.}
}
@article{MINET2017126,
title = {Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach},
journal = {Computers and Electronics in Agriculture},
volume = {142},
pages = {126-138},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917300479},
author = {Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny},
keywords = {Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection},
abstract = {Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.}
}
@article{HE2021102867,
title = {State-of-health estimation based on real data of electric vehicles concerning user behavior},
journal = {Journal of Energy Storage},
volume = {41},
pages = {102867},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102867},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21005892},
author = {Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan},
keywords = {Electric vehicles, SOH, User behavior, LWLR, LSTM},
abstract = {State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.}
}
@article{JIANG2020101505,
title = {Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101505},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302386},
author = {Huaxiong Jiang and Stan Geertman and Patrick Witte},
keywords = {Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context},
abstract = {Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.}
}
@article{AMARATUNGA2020100027,
title = {Uses and opportunities for machine learning in hypertension research},
journal = {International Journal of Cardiology Hypertension},
volume = {5},
pages = {100027},
year = {2020},
issn = {2590-0862},
doi = {https://doi.org/10.1016/j.ijchy.2020.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2590086220300045},
author = {Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis},
keywords = {Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network},
abstract = {Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.}
}
@article{PERERA2016512,
title = {Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {23},
pages = {512-517},
year = {2016},
note = {10th IFAC Conference on Control Applications in Marine SystemsCAMS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.487},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316320778},
author = {Lokukaluge P. Perera and Brage Mo},
keywords = {Principal Component Analysis, Big Data, Marine Engine Operations, Ship Performance Monitoring, Structured Data},
abstract = {Abstract:
Marine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression/expansion types of applications as a big data solution in shipping.}
}
@article{GACUTAN2022150742,
title = {Continental patterns in marine debris revealed by a decade of citizen science},
journal = {Science of The Total Environment},
volume = {807},
pages = {150742},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150742},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721058204},
author = {Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark},
keywords = {Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris},
abstract = {Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.}
}
@article{CHE2023513,
title = {Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data},
journal = {Journal of Environmental Sciences},
volume = {125},
pages = {513-523},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222001061},
author = {Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau},
keywords = {Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area},
abstract = {Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.}
}
@article{YAO202154,
title = {Application of artificial intelligence in renal disease},
journal = {Clinical eHealth},
volume = {4},
pages = {54-61},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2588914121000083},
author = {Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang},
keywords = {Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology},
abstract = {Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.}
}
@article{KAMAL2016191,
title = {A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {191-206},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304119},
author = {Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi},
keywords = {MapReduce, K-nearest neighbor, Big data, DNA (deoxyribonucleic acid), Computational biology, Imbalance data},
abstract = {Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.}
}
@article{YEBENES2019614,
title = {Towards a Data Governance Framework for Third Generation Platforms},
journal = {Procedia Computer Science},
volume = {151},
pages = {614-621},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.082},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305447},
author = {Juan Yebenes and Marta Zorrilla},
keywords = {Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data},
abstract = {The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.}
}
@article{LAKIND2019302,
title = {ExpoQual: Evaluating measured and modeled human exposure data},
journal = {Environmental Research},
volume = {171},
pages = {302-312},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119300465},
author = {Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman},
keywords = {ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty},
abstract = {Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).}
}
@article{SUN2022191,
title = {Advances in optical phenotyping of cereal crops},
journal = {Trends in Plant Science},
volume = {27},
number = {2},
pages = {191-208},
year = {2022},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1360138521002028},
author = {Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen},
keywords = {cereal crops, high-throughput phenotyping, optical sensors, traits},
abstract = {Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.}
}
@article{CHOW2017455,
title = {Internet-based computer technology on radiotherapy},
journal = {Reports of Practical Oncology & Radiotherapy},
volume = {22},
number = {6},
pages = {455-462},
year = {2017},
issn = {1507-1367},
doi = {https://doi.org/10.1016/j.rpor.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1507136716301602},
author = {James C.L. Chow},
keywords = {Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data},
abstract = {Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.}
}
@article{GRANELL20161,
title = {Future Internet technologies for environmental applications},
journal = {Environmental Modelling & Software},
volume = {78},
pages = {1-15},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215301298},
author = {Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon},
keywords = {Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking},
abstract = {This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.}
}
@article{YE2019936,
title = {Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model},
journal = {Science of The Total Environment},
volume = {658},
pages = {936-946},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.12.276},
url = {https://www.sciencedirect.com/science/article/pii/S0048969718351489},
author = {Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia},
keywords = {Points of interest, Population, Random forests, Nighttime light, China},
abstract = {Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.}
}
@incollection{CINNAMON2020121,
title = {Humanitarian Mapping},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {121-128},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10559-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105591},
author = {Jonathan Cinnamon},
keywords = {Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping},
abstract = {Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.}
}
@article{MOSAVI2022503,
title = {Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities},
journal = {Procedia Computer Science},
volume = {201},
pages = {503-510},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004781},
author = {Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais},
keywords = {Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0},
abstract = {The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.}
}
@incollection{LEUNG2021197,
title = {Chapter 13 - A support vector machine–based voice disorders detection using human voice signal},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {197-208},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000140},
author = {Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos}},
keywords = {Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders},
abstract = {Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.}
}
@article{SHARMA2021101624,
title = {Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy},
journal = {Government Information Quarterly},
pages = {101624},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000605},
author = {Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar},
keywords = {Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies},
abstract = {The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.}
}
@article{MACIAS2022,
title = {Nowcasting food inflation with a massive amount of online prices},
journal = {International Journal of Forecasting},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S016920702200036X},
author = {Paweł Macias and Damian Stelmasiak and Karol Szafranek},
keywords = {Inflation nowcasting, Online prices, Big data, Nowcasting competition, Web scraping},
abstract = {The consensus in the literature on providing accurate inflation forecasts underlines the importance of precise nowcasts. In this paper, we focus on this issue by employing a unique, extensive dataset of online food and non-alcoholic beverages prices gathered automatically from the webpages of major online retailers in Poland since 2009. We perform a real-time nowcasting experiment by using a highly disaggregated framework among popular, simple univariate approaches. We demonstrate that pure estimates of online price changes are already effective in nowcasting food inflation, but accounting for online food prices in a simple, recursively optimized model delivers further gains in the nowcast accuracy. Our framework outperforms various other approaches, including judgmental methods, traditional benchmarks, and model combinations. After the outbreak of the COVID-19 pandemic, its nowcasting quality has improved compared to other approaches and remained comparable with judgmental nowcasts. We also show that nowcast accuracy increases with the volume of online data, but their quality and relevance are essential for providing accurate in-sample fit and out-of-sample nowcasts. We conclude that online prices can markedly aid the decision-making process at central banks.}
}
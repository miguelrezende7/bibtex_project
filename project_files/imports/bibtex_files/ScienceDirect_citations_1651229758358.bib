@incollection{ZEITOUNI2020159,
title = {Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {159-171},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000187},
author = {Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov},
keywords = {Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture},
abstract = {In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.}
}
@article{ECKARDT2020406,
title = {Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas},
journal = {Nursing Outlook},
volume = {68},
number = {4},
pages = {406-416},
year = {2020},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0029655419306931},
author = {Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker},
keywords = {Precision health, Big data and Data analytics, Determinants of health, Global health, Opioid use disorder research},
abstract = {Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.}
}
@article{ZHANG2020100715,
title = {Knowledge mapping of tourism demand forecasting research},
journal = {Tourism Management Perspectives},
volume = {35},
pages = {100715},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2020.100715},
url = {https://www.sciencedirect.com/science/article/pii/S2211973620300829},
author = {Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei},
keywords = {Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic},
abstract = {Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.}
}
@article{SHAH2019562,
title = {An Internet-of-things Enabled Smart Manufacturing Testbed},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {562-567},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.122},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319302083},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring},
abstract = {The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.}
}
@article{PASIDIS2019301,
title = {Congestion by accident? A two-way relationship for highways in England},
journal = {Journal of Transport Geography},
volume = {76},
pages = {301-314},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300704},
author = {Ilias Pasidis},
keywords = {Accidents, Traffic congestion, Big data, Highways, England},
abstract = {This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.}
}
@incollection{BERMAN20181,
title = {1 - Introduction},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1-13},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000017},
author = {Jules J. Berman},
keywords = {Big data definition, Small data, Data filtering, Data reduction},
abstract = {Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.}
}
@article{ZHANG2018149,
title = {Product features characterization and customers’ preferences prediction based on purchasing data},
journal = {CIRP Annals},
volume = {67},
number = {1},
pages = {149-152},
year = {2018},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0007850618300441},
author = {Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong},
keywords = {Design, Product development, Big data},
abstract = {Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.}
}
@incollection{MCGILVRAY2021253,
title = {Chapter 5 - Structuring Your Project},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {253-267},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000013},
author = {Danette McGilvray},
keywords = {Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach},
abstract = {It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.}
}
@article{CAPPIELLO2022101874,
title = {Assessing and improving measurability of process performance indicators based on quality of logs},
journal = {Information Systems},
volume = {103},
pages = {101874},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101874},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000995},
author = {Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim},
keywords = {Business process, Event log, Data quality assessment, Data quality improvement},
abstract = {The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.}
}
@article{DETRE2018541,
title = {Handling veracity in multi-criteria decision-making: A multi-dimensional approach},
journal = {Information Sciences},
volume = {460-461},
pages = {541-554},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517309337},
author = {Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer},
keywords = {Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling},
abstract = {Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.}
}
@article{SAYAD2019130,
title = {Predictive modeling of wildfires: A new dataset and machine learning approach},
journal = {Fire Safety Journal},
volume = {104},
pages = {130-146},
year = {2019},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0379711218303941},
author = {Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}},
keywords = {Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence},
abstract = {Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.}
}
@incollection{DEMCHENKO201721,
title = {Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications},
editor = {Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu},
booktitle = {Big Data Analytics for Sensor-Network Collected Intelligence},
publisher = {Academic Press},
pages = {21-62},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-809393-1},
doi = {https://doi.org/10.1016/B978-0-12-809393-1.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093931000027},
author = {Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis},
keywords = {Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform},
abstract = {This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.}
}
@article{KARAFILI201852,
title = {An argumentation reasoning approach for data processing},
journal = {Computers in Industry},
volume = {94},
pages = {52-61},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S016636151730338X},
author = {Erisa Karafili and Konstantina Spanaki and Emil C. Lupu},
keywords = {Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios},
abstract = {Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.}
}
@article{GHORBANIAN2020276,
title = {Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {276-288},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302008},
author = {Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou},
keywords = {Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran},
abstract = {Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.}
}
@incollection{SEBASTIANCOLEMAN202269,
title = {Chapter 4 - The Data Challenge: The Mechanics of Meaning},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {69-92},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000043},
author = {Laura Sebastian-Coleman},
keywords = {History of data, statistics, scientific data, organizational data, relational data, characteristics of data},
abstract = {This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.}
}
@article{ROBERTSON2020214,
title = {An integrated environmental analytics system (IDEAS) based on a DGGS},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {162},
pages = {214-228},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300502},
author = {Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts},
keywords = {DGGS, Data model, Big data, Spatial data, Analytics, Environment},
abstract = {Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.}
}
@article{GUNDLA2016460,
title = {Creating NoSQL Biological Databases with Ontologies for Query Relaxation},
journal = {Procedia Computer Science},
volume = {91},
pages = {460-469},
year = {2016},
note = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916313138},
author = {Naresh Kumar Gundla and Zhengxin Chen},
keywords = {NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph},
abstract = {The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.}
}
@article{MAHDAVINEJAD2018161,
title = {Machine learning for internet of things data analysis: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {161-175},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S235286481730247X},
author = {Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth},
keywords = {Machine learning, Internet of Things, Smart data, Smart City},
abstract = {Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.}
}
@article{ALTURJMAN2020357,
title = {Intelligence and security in big 5G-oriented IoNT: An overview},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {357-368},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301074},
author = {Fadi Al-Turjman},
keywords = {IoNT, Security, Big data, Design factors},
abstract = {Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.}
}
@article{XIE202272,
title = {Real-World Data for Healthcare Research in China: Call for Actions},
journal = {Value in Health Regional Issues},
volume = {27},
pages = {72-81},
year = {2022},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212109921000765},
author = {Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu},
keywords = {administrative claims, data access, electronic health records, real-world data},
abstract = {Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.}
}
@article{PAIS2019100194,
title = {An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine},
journal = {Informatics in Medicine Unlocked},
volume = {16},
pages = {100194},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300851},
author = {Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles},
keywords = {MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing},
abstract = {Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.}
}
@article{JEONG2019358,
title = {Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS)},
journal = {Environmental Research},
volume = {172},
pages = {358-366},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0013935118306388},
author = {Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha},
keywords = {Ko-CHENS, Children, Environment, Cohort profile, Birth cohort},
abstract = {The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.}
}
@article{AYVAZ2021114598,
title = {Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114598},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000397},
author = {Serkan Ayvaz and Koray Alpay},
keywords = {Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data},
abstract = {In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.}
}
@article{FERREIRA2021757,
title = {How do data scientists and managers influence machine learning value creation?},
journal = {Procedia Computer Science},
volume = {181},
pages = {757-764},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002714},
author = {Humberto Ferreira and Pedro Ruivo and Carolina Reis},
keywords = {machine learning, business value, data scientists, managers, people factor},
abstract = {Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.}
}
@article{POLYVYANYY2019345,
title = {A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain},
journal = {Safety Science},
volume = {118},
pages = {345-354},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316230},
author = {Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}},
keywords = {Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents},
abstract = {The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.}
}
@article{SOUIFI2022103666,
title = {Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions},
journal = {Computers in Industry},
volume = {140},
pages = {103666},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200063X},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling},
abstract = {For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.}
}
@article{MORAN201942,
title = {Curious Feature Selection},
journal = {Information Sciences},
volume = {485},
pages = {42-54},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519301100},
author = {Michal Moran and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection},
abstract = {In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.}
}
@article{FRONZETTICOLLADON2019113075,
title = {Using social network and semantic analysis to analyze online travel forums and forecast tourism demand},
journal = {Decision Support Systems},
volume = {123},
pages = {113075},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113075},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301046},
author = {Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella},
keywords = {Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data},
abstract = {Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.}
}
@article{GE20201883,
title = {Developing the Quality Model for Collaborative Open Data},
journal = {Procedia Computer Science},
volume = {176},
pages = {1883-1892},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032130X},
author = {Mouzhi Ge and Włodzimierz Lewoniewski},
keywords = {Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model},
abstract = {Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.}
}
@incollection{BERANGER201697,
title = {2 - Ethical Development of the Medical Datasphere},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {97-166},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500026},
author = {Jérôme Béranger},
keywords = {Algorithmic ethics, Architecture, Complex data, Ethical data mining, Ethical issues, Ethical-technical guidance, Evaluation, Medical datasphere, Neo-Platonic modeling},
abstract = {Abstract:
As Lucy Suchmann observed in 2011, through Lévi-Strauss, “…we are our tools…” and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.}
}
@article{XU2021100860,
title = {Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers},
journal = {Tourism Management Perspectives},
volume = {39},
pages = {100860},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000738},
author = {Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang},
keywords = {Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers},
abstract = {Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.}
}
@article{AUFAURE2016100,
title = {From Business Intelligence to semantic data stream management},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {100-107},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003635},
author = {Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian},
keywords = {Data stream, Linked Data, Business Intelligence, Stream reasoning},
abstract = {The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.}
}
@article{LU201968,
title = {Oil and Gas 4.0 era: A systematic review and outlook},
journal = {Computers in Industry},
volume = {111},
pages = {68-90},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519302064},
author = {Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang},
keywords = {Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization},
abstract = {Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.}
}
@article{SHAO2022102736,
title = {IoT data visualization for business intelligence in corporate finance},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102736},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102736},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002181},
author = {Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam},
keywords = {IoT, Data visualization, Business intelligence, Corporate finance},
abstract = {Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.}
}
@article{KAYABAY2022121264,
title = {Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121264},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121264},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006983},
author = {Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit},
keywords = {Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data},
abstract = {Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.}
}
@article{WANG202151,
title = {The national multi-center artificial intelligent myopia prevention and control project},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {51-55},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000085},
author = {Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin},
keywords = {Myopia prevention and control, Artificial intelligent, National multicenter project},
abstract = {In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.}
}
@article{SMIDT20211018,
title = {The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic},
journal = {Procedia Computer Science},
volume = {181},
pages = {1018-1026},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003306},
author = {Hermanus J Smidt and Osden Jokonya},
keywords = {COVID 19, tracking, society, technology, privacy},
abstract = {Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.}
}
@article{ESCOBAR2020103378,
title = {Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary},
journal = {Computer Standards & Interfaces},
volume = {68},
pages = {103378},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.103378},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919300480},
author = {Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral},
keywords = {Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data},
abstract = {Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.}
}
@incollection{GUDIVADA201731,
title = {Chapter 2 - Data Analytics: Fundamentals},
editor = {Mashrur Chowdhury and Amy Apon and Kakan Dey},
booktitle = {Data Analytics for Intelligent Transportation Systems},
publisher = {Elsevier},
pages = {31-67},
year = {2017},
isbn = {978-0-12-809715-1},
doi = {https://doi.org/10.1016/B978-0-12-809715-1.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809715100002X},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, model building},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.}
}
@article{LARSON2016700,
title = {A review and future direction of agile, business intelligence, analytics and data science},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {700-710},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121630233X},
author = {Deanne Larson and Victor Chang},
keywords = {Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data},
abstract = {Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.}
}
@incollection{HOVENGA2022209,
title = {Chapter 9 - Quality data, design, implementation, and governance},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {209-237},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000136},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain},
abstract = {Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.}
}
@incollection{SVAB2021274,
title = {Complexity of Patient Data in Primary Care Practice},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {274-282},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11590-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115909},
author = {Igor Švab},
keywords = {Big data, Digital health, Family medicine, Genomics, Primary care},
abstract = {The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.}
}
@article{CHENG2021102938,
title = {Construction of a service quality scale for the online food delivery industry},
journal = {International Journal of Hospitality Management},
volume = {95},
pages = {102938},
year = {2021},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2021.102938},
url = {https://www.sciencedirect.com/science/article/pii/S0278431921000815},
author = {Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen},
keywords = {Online food delivery, Service quality, Big data analytic, OFD service quality scale},
abstract = {The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.}
}
@article{VIDGEN2017626,
title = {Management challenges in creating value from business analytics},
journal = {European Journal of Operational Research},
volume = {261},
number = {2},
pages = {626-639},
year = {2017},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717301455},
author = {Richard Vidgen and Sarah Shaw and David B. Grant},
keywords = {Analytics, Delphi, Management challenges, Value creation, Ecosystem},
abstract = {The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.}
}
@incollection{KRISHNAN2020157,
title = {9 - Governance},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {157-174},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000090},
author = {Krish Krishnan},
keywords = {Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata},
abstract = {Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.}
}
@article{CORRALESGARAY201977,
title = {Knowledge areas, themes and future research on open data: A co-word analysis},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {77-87},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303216},
author = {Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín},
keywords = {Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends},
abstract = {This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.}
}
@article{LI20161,
title = {A snail shell process model for knowledge discovery via data analytics},
journal = {Decision Support Systems},
volume = {91},
pages = {1-12},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301233},
author = {Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson},
keywords = {Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making},
abstract = {The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.}
}
@incollection{FARRE2022197,
title = {Chapter 7 - Data-driven policy evaluation},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {197-225},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000026},
author = {Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré},
keywords = {Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation},
abstract = {Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.}
}
@article{ZHANG2020104512,
title = {Blockchain-based life cycle assessment: An implementation framework and system architecture},
journal = {Resources, Conservation and Recycling},
volume = {152},
pages = {104512},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919304185},
author = {Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh},
keywords = {Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence},
abstract = {Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.}
}
@article{BUTTERWORTH2018257,
title = {The ICO and artificial intelligence: The role of fairness in the GDPR framework},
journal = {Computer Law & Security Review},
volume = {34},
number = {2},
pages = {257-268},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S026736491830044X},
author = {Michael Butterworth},
keywords = {Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics},
abstract = {The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.}
}
@article{APPELBAUM201729,
title = {Impact of business analytics and enterprise systems on managerial accounting},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {29-44},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1467089517300490},
author = {Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan},
keywords = {Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence},
abstract = {The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.}
}
@article{ESCOBAR2021748,
title = {Quality 4.0 — Green, Black and Master Black Belt Curricula},
journal = {Procedia Manufacturing},
volume = {53},
pages = {748-759},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.085},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001086},
author = {Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez},
keywords = {Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data},
abstract = {Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.}
}
@article{CHEN20151331,
title = {Towards Integrated Study of Data Management and Data Mining},
journal = {Procedia Computer Science},
volume = {55},
pages = {1331-1339},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015926},
author = {Zhengxin Chen},
keywords = {Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)},
abstract = {From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory}
}
@article{RAGUSEO2021103451,
title = {Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103451},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103451},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000252},
author = {Elisabetta Raguseo and Federico Pigni and Claudio Vitari},
keywords = {Streams of big data, Process efficiency, Product effectiveness, Competitive advantage},
abstract = {Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.}
}
@article{SILVA2015289,
title = {Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps},
journal = {Transportation Research Procedia},
volume = {11},
pages = {289-296},
year = {2015},
note = {Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515003166},
author = {João de Abreu e Silva and Mark Davis},
keywords = {web based surveys, smartphone surveys, respondent interaction, respondent burden},
abstract = {Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.}
}
@article{OZKAN2019208,
title = {Criminology in the age of data explosion: New directions},
journal = {The Social Science Journal},
volume = {56},
number = {2},
pages = {208-219},
year = {2019},
issn = {0362-3319},
doi = {https://doi.org/10.1016/j.soscij.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0362331918301514},
author = {Turgut Ozkan},
keywords = {Social science, Big data, Crime, Social media, Data-driven social science},
abstract = {This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.}
}
@article{ANJUM2018326,
title = {Privacy preserving data by conceptualizing smart cities using MIDR-Angelization},
journal = {Sustainable Cities and Society},
volume = {40},
pages = {326-334},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717314646},
author = {Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq},
keywords = {Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city},
abstract = {Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.}
}
@article{REIS2015238,
title = {Integrating modelling and smart sensors for environmental and human health},
journal = {Environmental Modelling & Software},
volume = {74},
pages = {238-246},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S136481521500167X},
author = {Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly},
keywords = {Integrated modelling, Environmental sensors, Population health, Environmental health, Big data},
abstract = {Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.}
}
@article{WU2020116388,
title = {Impact factors of the real-world fuel consumption rate of light duty vehicles in China},
journal = {Energy},
volume = {190},
pages = {116388},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116388},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219320833},
author = {Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang},
keywords = {Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China},
abstract = {Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.}
}
@article{KUMAR202185,
title = {Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach},
journal = {Procedia CIRP},
volume = {98},
pages = {85-90},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000330},
author = {Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan},
keywords = {Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis},
abstract = {Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.}
}
@incollection{KHAN2022,
title = {Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000590},
author = {Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam},
keywords = {NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification},
abstract = {With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.}
}
@incollection{GROOT2017127,
title = {Chapter 5 - Data Management Tools and Techniques},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {127-177},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000053},
author = {Martijn Groot},
keywords = {data management technology, databases, big data, financial analytics, IT management},
abstract = {In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{OIEN20211334,
title = {An approach to data structuring and predictive analysis in discrete manufacturing},
journal = {Procedia CIRP},
volume = {104},
pages = {1334-1338},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.224},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011227},
author = {Christian Dalheim Øien and Sebastian Dransfeld},
keywords = {Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems},
abstract = {In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.}
}
@article{LUO2021197,
title = {Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation},
journal = {Computer Communications},
volume = {180},
pages = {197-209},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003637},
author = {Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo},
keywords = {Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system},
abstract = {Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.}
}
@article{GUO2021107627,
title = {Automated pressure transient analysis: A cloud-based approach},
journal = {Journal of Petroleum Science and Engineering},
volume = {196},
pages = {107627},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520306951},
author = {Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed},
keywords = {Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application},
abstract = {Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.}
}
@article{ZHANG202124,
title = {Thinking on the informatization development of China's healthcare system in the post-COVID-19 era},
journal = {Intelligent Medicine},
volume = {1},
number = {1},
pages = {24-28},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000097},
author = {Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu},
keywords = {Coronavirus disease 2019, Healthcare system, Informatization},
abstract = {With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.}
}
@article{WANG2018440,
title = {Research on the Theory and Method of Grid Data Asset Management},
journal = {Procedia Computer Science},
volume = {139},
pages = {440-447},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319288},
author = {Jun Wang and Yun-si Li and Wei Song and Ai-hua Li},
keywords = {big data, grid data asset, asset management, data governance},
abstract = {In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.}
}
@article{AKTER201985,
title = {Analytics-based decision-making for service systems: A qualitative study and agenda for future research},
journal = {International Journal of Information Management},
volume = {48},
pages = {85-95},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218312696},
author = {Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos},
keywords = {Big data analytics, Decision-making, Service systems},
abstract = {While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.}
}
@article{SKARPATHIOTAKI2022100274,
title = {Cross-Industry Process Standardization for Text Analytics},
journal = {Big Data Research},
volume = {27},
pages = {100274},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000915},
author = {Christina G. Skarpathiotaki and Konstantinos E. Psannis},
keywords = {Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes},
abstract = {We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.}
}
@article{YAISH20142168,
title = {Multi-tenant Elastic Extension Tables Data Management},
journal = {Procedia Computer Science},
volume = {29},
pages = {2168-2181},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914003792},
author = {Haitham Yaish and Madhu Goyal and George Feuerlicht},
keywords = {Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.},
abstract = {Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.}
}
@article{ANG20161,
title = {Big Sensor Data Applications in Urban Environments},
journal = {Big Data Research},
volume = {4},
pages = {1-12},
year = {2016},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615300241},
author = {Li-Minn Ang and Kah Phooi Seng},
keywords = {Big data, Sensor-based systems, Survey, Application, Challenges},
abstract = {The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.}
}
@article{LIU2021589,
title = {Toward intelligent wireless communications: Deep learning - based physical layer technologies},
journal = {Digital Communications and Networks},
volume = {7},
number = {4},
pages = {589-597},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000742},
author = {Siqi Liu and Tianyu Wang and Shaowei Wang},
keywords = {Data-driven, Deep learning, Physical layer, Wireless communications},
abstract = {Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.}
}
@article{ARIMURA2020100212,
title = {Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100212},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301238},
author = {Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada},
keywords = {Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data},
abstract = {At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.}
}
@article{NG2017939,
title = {A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning},
journal = {Procedia Engineering},
volume = {196},
pages = {939-947},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817331569},
author = {S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu},
keywords = {Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure},
abstract = {In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.}
}
@article{WANG201714,
title = {GSA: Genome Sequence Archive*},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {15},
number = {1},
pages = {14-18},
year = {2017},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022917300025},
author = {Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao},
keywords = {Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC},
abstract = {With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.}
}
@incollection{FORTSON2021185,
title = {Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science},
editor = {Amy Paige Kaminski},
booktitle = {Space Science and Public Engagement},
publisher = {Elsevier},
pages = {185-219},
year = {2021},
isbn = {978-0-12-817390-9},
doi = {https://doi.org/10.1016/B978-0-12-817390-9.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173909000099},
author = {Lucy Fortson},
keywords = {Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement},
abstract = {The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.}
}
@article{DEMOULIN2020103120,
title = {Acceptance of text-mining systems: The signaling role of information quality},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103120},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308765},
author = {Nathalie T.M. Demoulin and Kristof Coussement},
keywords = {Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support},
abstract = {The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.}
}
@article{CHRISTOPOULOS201570,
title = {Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013},
journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
volume = {128},
pages = {70-83},
year = {2015},
issn = {1364-6826},
doi = {https://doi.org/10.1016/j.jastp.2015.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364682615000577},
author = {Demetris T. Christopoulos},
keywords = {Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis},
abstract = {Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.}
}
@article{LIN2018293,
title = {DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {293-302},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17317247},
author = {Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu},
keywords = {Cloud computing, Reputation mechanism, Trustworthiness, Data veracity},
abstract = {Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.}
}
@article{NASCIMENTO202097,
title = {Estimating record linkage costs in distributed environments},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {97-106},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302756},
author = {Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre},
keywords = {Record linkage, Theoretical model, Data quality, Cloud computing},
abstract = {Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.}
}
@article{SOLANKI2019476,
title = {Towards a knowledge driven framework for bridging the gap between software and data engineering},
journal = {Journal of Systems and Software},
volume = {149},
pages = {476-484},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302772},
author = {Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan},
keywords = {Ontologies, Data engineering, Software engineering, Alignment, Integration},
abstract = {In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.}
}
@article{XIANG20191180,
title = {Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform},
journal = {Journal of Cleaner Production},
volume = {220},
pages = {1180-1193},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.310},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619303373},
author = {Zehua Xiang and Minli Xu},
keywords = {Big data marketing, Differential game, Closed-loop supply chain, Internet service platform},
abstract = {In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.}
}
@article{ESKANDARITORBAGHAN2022106543,
title = {Understanding the potential of emerging digital technologies for improving road safety},
journal = {Accident Analysis & Prevention},
volume = {166},
pages = {106543},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106543},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005741},
author = {Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund},
keywords = {Safety, Road, Transport, Digital technology, Information},
abstract = {Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.}
}
@article{TSAI2021105421,
title = {Sustainable supply chain management trends in world regions: A data-driven analysis},
journal = {Resources, Conservation and Recycling},
volume = {167},
pages = {105421},
year = {2021},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2021.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0921344921000288},
author = {Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu},
keywords = {Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory},
abstract = {This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.}
}
@article{KATARIA2019101429,
title = {Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {33},
number = {4},
pages = {101429},
year = {2019},
note = {How to Investigate: Very Early Inflammatory Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2019.101429},
url = {https://www.sciencedirect.com/science/article/pii/S1521694219300981},
author = {Suchitra Kataria and Vinod Ravindran},
keywords = {Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health},
abstract = {Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.}
}
@article{MOLL2019100833,
title = {The role of internet-related technologies in shaping the work of accountants: New directions for accounting research},
journal = {The British Accounting Review},
volume = {51},
number = {6},
pages = {100833},
year = {2019},
note = {Innovative Governance and Sustainable Pathways in a Disruptive Environment},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890838919300459},
author = {Jodie Moll and Ogan Yigitbasioglu},
keywords = {Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence},
abstract = {This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.}
}
@article{MERHI2021121180,
title = {Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121180},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121180},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006132},
author = {Mohammad I. Merhi},
keywords = {Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP},
abstract = {This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.}
}
@article{CINNAMON2016253,
title = {Evidence and future potential of mobile phone data for disease disaster management},
journal = {Geoforum},
volume = {75},
pages = {253-264},
year = {2016},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2016.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016718516301981},
author = {Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger},
keywords = {Mobile phone, Call detail records, SMS, Disaster, Disease, Big data},
abstract = {Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.}
}
@article{RIESENER2019304,
title = {Framework for defining information quality based on data attributes within the digital shadow using LDA},
journal = {Procedia CIRP},
volume = {83},
pages = {304-310},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119304366},
author = {Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes},
keywords = {Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)},
abstract = {The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.}
}
@article{NEWHART2019498,
title = {Data-driven performance analyses of wastewater treatment plants: A review},
journal = {Water Research},
volume = {157},
pages = {498-513},
year = {2019},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419302490},
author = {Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath},
keywords = {Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring},
abstract = {Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.}
}
@incollection{MOISE2015279,
title = {Chapter 12 - Terabyte-Scale Image Similarity Search},
editor = {Venu Govindaraju and Vijay V. Raghavan and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {33},
pages = {279-301},
year = {2015},
booktitle = {Big Data Analytics},
issn = {0169-7161},
doi = {https://doi.org/10.1016/B978-0-444-63492-4.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634924000125},
author = {Diana Moise and Denis Shestakov},
keywords = {Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves},
abstract = {While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.}
}
@article{MUNINGER2022140,
title = {Social media use: A review of innovation management practices},
journal = {Journal of Business Research},
volume = {143},
pages = {140-156},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000510},
author = {Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi},
keywords = {Social media, Innovation, Systematic review, Framework and research agenda},
abstract = {The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.}
}
@article{LLAVE2018516,
title = {Data lakes in business intelligence: reporting from the trenches},
journal = {Procedia Computer Science},
volume = {138},
pages = {516-524},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317046},
author = {Marilex Rea Llave},
keywords = {Business intelligence, big data, data lake, BI architecture},
abstract = {The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.}
}
@article{HASSANI2021121111,
title = {The science of statistics versus data science: What is the future?},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121111},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121111},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005448},
author = {Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen},
keywords = {Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism},
abstract = {The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.}
}
@article{CAO2020107850,
title = {Online investigation of vibration serviceability limitations using smartphones},
journal = {Measurement},
volume = {162},
pages = {107850},
year = {2020},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120303882},
author = {Lei Cao and Jun Chen},
keywords = {Vibration serviceability, Online sampling, Big data, Data cleaning},
abstract = {Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.}
}
@article{MATHEUS2020101284,
title = {Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101284},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300303},
author = {Ricardo Matheus and Marijn Janssen and Devender Maheshwari},
keywords = {Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making},
abstract = {Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.}
}
@incollection{PEZOULAS202019,
title = {Chapter 2 - Types and sources of medical and other related data},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {19-65},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000025},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data},
abstract = {This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.}
}
@article{GAHA2021216,
title = {Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals},
journal = {Procedia Manufacturing},
volume = {54},
pages = {216-221},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001694},
author = {Raoudha Gaha and Alexandre Durupt and Benoit Eynard},
keywords = {Digital Twin, CMM, inspection, Model-based-defintion, Digital thread},
abstract = {The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.}
}
@article{AHMAD2022112128,
title = {Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm},
journal = {Renewable and Sustainable Energy Reviews},
volume = {160},
pages = {112128},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112128},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122000569},
author = {Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb},
keywords = {Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response},
abstract = {The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.}
}
@article{FOSSOWAMBA2015234,
title = {How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study},
journal = {International Journal of Production Economics},
volume = {165},
pages = {234-246},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004253},
author = {Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou},
keywords = {‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review},
abstract = {Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.}
}
@article{LAU2019209,
title = {Pitfalls in big data analysis: next-generation technologies, last-generation data},
journal = {Diagnostic Microbiology and Infectious Disease},
volume = {94},
number = {2},
pages = {209-210},
year = {2019},
issn = {0732-8893},
doi = {https://doi.org/10.1016/j.diagmicrobio.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732889318306710},
author = {Susanna K.P. Lau and Patrick C.Y. Woo}
}
@article{GENENDERFELTHEIMER2018112,
title = {Visualizing High Dimensional and Big Data},
journal = {Procedia Computer Science},
volume = {140},
pages = {112-121},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319811},
author = {Amy Genender-Feltheimer},
keywords = {Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data},
abstract = {The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.}
}
@article{CHENG20181,
title = {Data and knowledge mining with big data towards smart production},
journal = {Journal of Industrial Information Integration},
volume = {9},
pages = {1-13},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300584},
author = {Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao},
keywords = {Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery},
abstract = {Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.}
}
@article{PAPANAGNOU2018343,
title = {Coping with demand volatility in retail pharmacies with the aid of big data exploration},
journal = {Computers & Operations Research},
volume = {98},
pages = {343-354},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817302162},
author = {Christos I. Papanagnou and Omeiza Matthews-Amune},
keywords = {Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty},
abstract = {Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.}
}
@article{PHILIPCHEN2014314,
title = {Data-intensive applications, challenges, techniques and technologies: A survey on Big Data},
journal = {Information Sciences},
volume = {275},
pages = {314-347},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346},
author = {C.L. {Philip Chen} and Chun-Yang Zhang},
keywords = {Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing},
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.}
}
@incollection{CARNICERO2019121,
title = {Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {121-131},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000083},
author = {Javier Carnicero and David Rojas},
keywords = {Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability},
abstract = {Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.}
}
@article{LIN2018220,
title = {An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks},
journal = {Future Generation Computer Systems},
volume = {82},
pages = {220-234},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313262},
author = {Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao},
keywords = {Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata},
abstract = {Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.}
}
@article{CANO201736,
title = {Perspectives on Big Data applications of health information},
journal = {Current Opinion in Systems Biology},
volume = {3},
pages = {36-42},
year = {2017},
note = {• Mathematical modelling • Mathematical modelling, Dynamics of brain activity at the systems level • Clinical and translational systems biology},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300409},
author = {Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca},
keywords = {Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting},
abstract = {Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.}
}
@article{BIVAND201887,
title = {Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {87-91},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300579},
author = {Roger Bivand and Konstantin Krivoruchko},
keywords = {Change of support, Sampling design, Data transformation, Prediction standard error},
abstract = {Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.}
}
@article{DALLAVALLE201876,
title = {Social media big data integration: A new approach based on calibration},
journal = {Expert Systems with Applications},
volume = {111},
pages = {76-90},
year = {2018},
note = {Big Data Analytics for Business Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417308667},
author = {Luciana {Dalla Valle} and Ron Kenett},
keywords = {Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques},
abstract = {In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.}
}
@article{THADURI2015457,
title = {Railway Assets: A Potential Domain for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {53},
pages = {457-467},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018268},
author = {Adithya Thaduri and Diego Galar and Uday Kumar},
keywords = {Big Data, Railways, Maintenance, Transportation},
abstract = {Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.}
}
@article{VETRO2021101619,
title = {A data quality approach to the identification of discrimination risk in automated decision making systems},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101619},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101619},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000551},
author = {Antonio Vetrò and Marco Torchiano and Mariachiara Mecati},
keywords = {Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance},
abstract = {Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.}
}
@article{BALA2017114,
title = {A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments},
journal = {Data & Knowledge Engineering},
volume = {111},
pages = {114-136},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300611},
author = {Mahfoud Bala and Omar Boussaid and Zaia Alimazighi},
keywords = {Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce},
abstract = {Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.}
}
@article{KUM2015127,
title = {Using big data for evidence based governance in child welfare},
journal = {Children and Youth Services Review},
volume = {58},
pages = {127-136},
year = {2015},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0190740915300591},
author = {Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan},
keywords = {Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data},
abstract = {Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.}
}
@article{PERRONS2015117,
title = {Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”},
journal = {Energy Policy},
volume = {81},
pages = {117-121},
year = {2015},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2015.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0301421515000932},
author = {Robert K. Perrons and Jesse W. Jensen},
keywords = {Big data, Oil and gas, Information technologies, Data},
abstract = {The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.}
}
@article{YANG2019755,
title = {How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications},
journal = {Transport Reviews},
volume = {39},
number = {6},
pages = {755-773},
year = {2019},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2019.1649315},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722001568},
author = {Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li},
keywords = {AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data},
abstract = {ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.}
}
@article{SHIN2015311,
title = {Ecological views of big data: Perspectives and issues},
journal = {Telematics and Informatics},
volume = {32},
number = {2},
pages = {311-320},
year = {2015},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0736585314000665},
author = {Dong-Hee Shin and Min Jae Choi},
keywords = {Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development},
abstract = {From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.}
}
@article{MEHMOOD20151107,
title = {Big Data Logistics: A health-care Transport Capacity Sharing Model},
journal = {Procedia Computer Science},
volume = {64},
pages = {1107-1114},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027015},
author = {Rashid Mehmood and Gary Graham},
keywords = {future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources},
abstract = {The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.}
}
@article{RAIKOV2016147,
title = {Big Data Refining on the Base of Cognitive Modeling},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {32},
pages = {147-152},
year = {2016},
note = {Cyber-Physical & Human-Systems CPHS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316328774},
author = {Alexander N. Raikov and Z. Avdeeva and A. Ermakov},
keywords = {data refining, cognitive modeling, Big Data, intellectual agents, networked expertise},
abstract = {Abstract:
In conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.}
}
@incollection{2022vii,
title = {In praise of Meeting the Challenges of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {vii-ix},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500016X}
}
@article{HUI2019S90,
title = {PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS},
journal = {Value in Health},
volume = {22},
pages = {S90},
year = {2019},
note = {ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.04.303},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519304954},
author = {Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li}
}
@article{RICHTER201537,
title = {Medicinal chemistry in the era of big data},
journal = {Drug Discovery Today: Technologies},
volume = {14},
pages = {37-41},
year = {2015},
note = {From Chemistry to Biology Database Curation},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1740674915000141},
author = {Lars Richter and Gerhard F. Ecker},
abstract = {In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.}
}
@article{HUANG2018165,
title = {A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {165-176},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311927},
author = {Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu},
keywords = {Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment},
abstract = {While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).}
}
@incollection{KRESS201911,
title = {Big Data for Ecological Models},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {11-20},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105573},
author = {Marin M. Kress},
keywords = {Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media},
abstract = {The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.}
}
@article{KOZJEK2018209,
title = {Big data analytics for operations management in engineer-to-order manufacturing},
journal = {Procedia CIRP},
volume = {72},
pages = {209-214},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118302531},
author = {Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala},
keywords = {Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data},
abstract = {Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.}
}
@article{CHEN2016184,
title = {On Big Data and Hydroinformatics},
journal = {Procedia Engineering},
volume = {154},
pages = {184-191},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.443},
url = {https://www.sciencedirect.com/science/article/pii/S187770581631832X},
author = {Yiheng Chen and Dawei Han},
keywords = {Big data, Hydroinformatics},
abstract = {Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.}
}
@article{ZAMAN2017537,
title = {Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry},
journal = {Procedia Engineering},
volume = {194},
pages = {537-544},
year = {2017},
note = {10th International Conference on Marine Technology, MARTEC 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817333386},
author = {Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman},
keywords = {Carbon emission, data-oriented, MRV, big data},
abstract = {Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.}
}
@article{REHMAN2016917,
title = {Big data reduction framework for value creation in sustainable enterprises},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part A},
pages = {917-928},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216303097},
author = {Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah},
keywords = {Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model},
abstract = {Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.}
}
@article{DAMIANI20181,
title = {Large databases (Big Data) and evidence-based medicine},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {1-2},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518301961},
author = {Andrea Damiani and Graziano Onder and Vincenzo Valentini}
}
@article{KOBUSINSKA20181321,
title = {Big Data fingerprinting information analytics for sustainability},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1321-1337},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.061},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329965},
author = {Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński},
keywords = {Big Data, Fingerprinting, Web tracking, Security, Analytics},
abstract = {Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.}
}
@article{LIU201797,
title = {Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions},
journal = {Computers & Security},
volume = {69},
pages = {97-113},
year = {2017},
note = {Security Data Science and Cyber Threat Management},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301778},
author = {Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao},
keywords = {Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query},
abstract = {With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.}
}
@article{OLMEDILLA201679,
title = {Harvesting Big Data in social science: A methodological approach for collecting online user-generated content},
journal = {Computer Standards & Interfaces},
volume = {46},
pages = {79-87},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916300034},
author = {M. Olmedilla and M.R. Martínez-Torres and S.L. Toral},
keywords = {Big Data, User-generated content, e-Social science, Computing, Data gathering},
abstract = {Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.}
}
@article{JANKE2016227,
title = {Exploring the Potential of Predictive Analytics and Big Data in Emergency Care},
journal = {Annals of Emergency Medicine},
volume = {67},
number = {2},
pages = {227-236},
year = {2016},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2015.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0196064415005302},
author = {Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy},
abstract = {Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.}
}
@article{OPRESNIK2015174,
title = {The value of Big Data in servitization},
journal = {International Journal of Production Economics},
volume = {165},
pages = {174-184},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004307},
author = {David Opresnik and Marco Taisch},
keywords = {Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information},
abstract = {Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.}
}
@article{KACFAHEMANI201570,
title = {Understandable Big Data: A survey},
journal = {Computer Science Review},
volume = {17},
pages = {70-81},
year = {2015},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2015.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013715000064},
author = {Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle},
keywords = {Big data, Hadoop, Reasoning, Coreference resolution, Entity linking, Information extraction, Ontology alignment},
abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.}
}
@incollection{KRISHNAN2020175,
title = {10 - Building the big data application},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {175-197},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000107},
author = {Krish Krishnan},
keywords = {Big data, Business continuity, Research project, Software, Storyboard, User interface},
abstract = {This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.}
}
@article{ENGLEBRIGHT2016280,
title = {The Role of the Chief Nurse Executive in the Big Data Revolution},
journal = {Nurse Leader},
volume = {14},
number = {4},
pages = {280-284},
year = {2016},
issn = {1541-4612},
doi = {https://doi.org/10.1016/j.mnl.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1541461215300094},
author = {Jane Englebright and Barbara Caspers}
}
@incollection{AGOSTON201953,
title = {Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {53-75},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000046},
author = {Denes V. Agoston},
keywords = {Big Data, Artificial intelligence and machine learning in neurotrauma},
abstract = {Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.}
}
@article{BELLOORGAZ201645,
title = {Social big data: Recent achievements and new challenges},
journal = {Information Fusion},
volume = {28},
pages = {45-59},
year = {2016},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253515000780},
author = {Gema Bello-Orgaz and Jason J. Jung and David Camacho},
keywords = {Big data, Data mining, Social media, Social networks, Social-based frameworks and applications},
abstract = {Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.}
}
@article{KORTESNIEMI201890,
title = {The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession},
journal = {Physica Medica},
volume = {56},
pages = {90-93},
year = {2018},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718313152},
author = {Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis},
abstract = {Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.}
}
@article{TAN2015223,
title = {Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph},
journal = {International Journal of Production Economics},
volume = {165},
pages = {223-233},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004289},
author = {Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang},
keywords = {Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation},
abstract = {Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.}
}
@article{JAN2019275,
title = {Deep learning in big data Analytics: A comparative study},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {275-287},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315835},
author = {Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon},
keywords = {Big data, Deep learning, Deep belief networks, Convolutional Neural Networks},
abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.}
}
@incollection{VIDHYALAKSHMI20201,
title = {Chapter 1 - Medical big data mining and processing in e-health care},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar},
booktitle = {An Industrial IoT Approach for Pharmaceutical Industry Growth},
publisher = {Academic Press},
pages = {1-30},
year = {2020},
isbn = {978-0-12-821326-1},
doi = {https://doi.org/10.1016/B978-0-12-821326-1.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213261000012},
author = {A. Vidhyalakshmi and C. Priya},
keywords = {Big data, IoT, health care, telemedicine, WHO, image processing},
abstract = {Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.}
}
@article{HECKMAN202019,
title = {The Role of Physicians in the Era of Big Data},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {1},
pages = {19-21},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19312826},
author = {George A. Heckman and John P. Hirdes and Robert S. McKelvie}
}
@article{GOVINDAN2018343,
title = {Big data analytics and application for logistics and supply chain management},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {343-349},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1366554518302606},
author = {Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla},
keywords = {Big data analytics, Supply chain management, Logistics},
abstract = {This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.}
}
@article{MANTELERO2017584,
title = {Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework},
journal = {Computer Law & Security Review},
volume = {33},
number = {5},
pages = {584-602},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917301644},
author = {Alessandro Mantelero},
keywords = {Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms},
abstract = {In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.}
}
@article{REKHA2015295,
title = {Survey on Software Project Risks and Big Data Analytics},
journal = {Procedia Computer Science},
volume = {50},
pages = {295-300},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915005463},
author = {J.H. Rekha and R. Parvathi},
keywords = {software project, big data analytics, anlytics tools.},
abstract = {Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.}
}
@article{BERNASCONI2021100009,
title = {Data quality-aware genomic data integration},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100009},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000082},
author = {Anna Bernasconi},
keywords = {Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability},
abstract = {Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.}
}
@article{COLEMAN20151091,
title = {How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture},
journal = {American Journal of Ophthalmology},
volume = {160},
number = {6},
pages = {1091-1103.e3},
year = {2015},
issn = {0002-9394},
doi = {https://doi.org/10.1016/j.ajo.2015.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S000293941500598X},
author = {Anne Louise Coleman},
abstract = {Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.}
}
@article{SEMANJSKI201738,
title = {Spatial context mining approach for transport mode recognition from mobile sensed big data},
journal = {Computers, Environment and Urban Systems},
volume = {66},
pages = {38-52},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516304367},
author = {Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox},
keywords = {Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data},
abstract = {Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.}
}
@article{AKTER2016113,
title = {How to improve firm performance using big data analytics capability and business strategy alignment?},
journal = {International Journal of Production Economics},
volume = {182},
pages = {113-131},
year = {2016},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527316302110},
author = {Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe},
keywords = {Capabilities, Entanglement view, Big data analytics, Hierarchical modeling},
abstract = {The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.}
}
@article{LIU2020113381,
title = {Minimizing the data quality problem of information systems: A process-based method},
journal = {Decision Support Systems},
volume = {137},
pages = {113381},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113381},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301366},
author = {Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang},
keywords = {Data quality, Information system, Petri net, Optimization model, Process model},
abstract = {The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.}
}
@article{KUMAR2018428,
title = {A big data driven sustainable manufacturing framework for condition-based maintenance prediction},
journal = {Journal of Computational Science},
volume = {27},
pages = {428-439},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316305129},
author = {Ajay Kumar and Ravi Shankar and Lakshman S. Thakur},
keywords = {data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination},
abstract = {Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.}
}
@article{ZAIN2018140,
title = {Big Data Analytics based on PANFIS MapReduce},
journal = {Procedia Computer Science},
volume = {144},
pages = {140-152},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322233},
author = {Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad},
keywords = {Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy},
abstract = {In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.}
}
@article{RAMBUR2018176,
title = {A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses},
journal = {Journal of Professional Nursing},
volume = {34},
number = {3},
pages = {176-181},
year = {2018},
issn = {8755-7223},
doi = {https://doi.org/10.1016/j.profnurs.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S8755722316303283},
author = {Betty Rambur and Therese Fitzpatrick}
}
@article{KHENNOU201860,
title = {Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR},
journal = {Procedia Computer Science},
volume = {127},
pages = {60-68},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301091},
author = {Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui},
keywords = {Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners},
abstract = {Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.}
}
@article{ZHANG2015606,
title = {A System for Tender Price Evaluation of Construction Project Based on Big Data},
journal = {Procedia Engineering},
volume = {123},
pages = {606-614},
year = {2015},
note = {Selected papers from Creative Construction Conference 2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.10.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815032154},
author = {Yongcheng Zhang and Hanbin Luo and Yi He},
keywords = {System, Bid price evaluation, Construction project, Big data},
abstract = {Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.}
}
@article{JI2017187,
title = {Big data analytics based fault prediction for shop floor scheduling},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {187-194},
year = {2017},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612517300389},
author = {Wei Ji and Lihui Wang},
keywords = {Big data analytics, Fault prediction, Shop floor, Scheduling},
abstract = {The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.}
}
@article{LEFEVRE20181,
title = {Big data in forensic science and medicine},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {1-6},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X17301154},
author = {Thomas Lefèvre},
keywords = {Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality},
abstract = {In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.}
}
@article{YAHIA20181,
title = {Preface: Special Issue on Big Data},
journal = {Fuzzy Sets and Systems},
volume = {348},
pages = {1-3},
year = {2018},
note = {SI: Fuzzy Approaches to Big Data},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0165011418302987},
author = {Sadok Ben Yahia and Anne Laurent and Gabriella Pasi}
}
@article{NATIVI20151,
title = {Big Data challenges in building the Global Earth Observation System of Systems},
journal = {Environmental Modelling & Software},
volume = {68},
pages = {1-26},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000481},
author = {Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai},
keywords = {GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems},
abstract = {There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.}
}
@article{PURANIK2019838,
title = {The perils and pitfalls of big data analysis in medicine},
journal = {The Ocular Surface},
volume = {17},
number = {4},
pages = {838-839},
year = {2019},
issn = {1542-0124},
doi = {https://doi.org/10.1016/j.jtos.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1542012419301740},
author = {C.J. Puranik and Sreenivasa Rao and S. Chennamaneni}
}
@article{MAUDSLEY2018961,
title = {Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7},
pages = {961-975},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018300402},
author = {Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts},
keywords = {Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics},
abstract = {Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {0210-5691},
doi = {https://doi.org/10.1016/j.medin.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0210569118301827},
author = {Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{GAO2016952,
title = {A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era},
journal = {Chinese Journal of Chemical Engineering},
volume = {24},
number = {8},
pages = {952-962},
year = {2016},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2016.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S1004954116305134},
author = {Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang},
keywords = {Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data},
abstract = {Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.}
}
@article{DUNCAN2019127,
title = {Big data sharing and analysis to advance research in post-traumatic epilepsy},
journal = {Neurobiology of Disease},
volume = {123},
pages = {127-136},
year = {2019},
note = {Antiepileptogenesis following Traumatic Brain Injury},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2018.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0969996118301700},
author = {Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga},
keywords = {Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI},
abstract = {We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.}
}
@article{GARG2016940,
title = {Challenges and Techniques for Testing of Big Data},
journal = {Procedia Computer Science},
volume = {85},
pages = {940-948},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916306354},
author = {Naveen Garg and Sanjay Singla and Surender Jangra},
keywords = {Big Data, Testing, Verasity, Hadoop},
abstract = {Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.}
}
@article{TAGLANG201617,
title = {Use of “big data” in drug discovery and clinical trials},
journal = {Gynecologic Oncology},
volume = {141},
number = {1},
pages = {17-23},
year = {2016},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2016.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0090825816300464},
author = {Guillaume Taglang and David B. Jackson},
keywords = {Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers},
abstract = {Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.}
}
@incollection{LEI202029,
title = {2 - Fundamentals of big data in radio astronomy},
editor = {Linghe Kong and Tian Huang and Yongxin Zhu and Shenghua Yu},
booktitle = {Big Data in Astronomy},
publisher = {Elsevier},
pages = {29-58},
year = {2020},
isbn = {978-0-12-819084-5},
doi = {https://doi.org/10.1016/B978-0-12-819084-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190845000109},
author = {Jiale Lei and Linghe Kong},
keywords = {Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process},
abstract = {Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.}
}
@incollection{LOPES2017167,
title = {Chapter 10 - Big Data: A Practitioners Perspective},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-179},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000107},
author = {Darshan Lopes and Kevin Palmer and Fiona O'Sullivan},
keywords = {Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse},
abstract = {Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.}
}
@article{KIM2022100256,
title = {Organizational process maturity model for IoT data quality management},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100256},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000480},
author = {Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee},
keywords = {Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute},
abstract = {Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.}
}
@article{BROTHERS201884,
title = {Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery},
journal = {Biochemical Pharmacology},
volume = {152},
pages = {84-93},
year = {2018},
issn = {0006-2952},
doi = {https://doi.org/10.1016/j.bcp.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006295218301199},
author = {John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko},
keywords = {Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome},
abstract = {The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.}
}
@article{MENDESSAMPAIO20158304,
title = {DQ2S – A framework for data quality-aware information management},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8304-8326},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004522},
author = {Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio},
keywords = {Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data},
abstract = {This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.}
}
@article{PIRRACCHIO2019377,
title = {Big data and targeted machine learning in action to assist medical decision in the ICU},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {4},
pages = {377-384},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818302169},
author = {Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard},
abstract = {Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.}
}
@article{ALLES201644,
title = {Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors},
journal = {International Journal of Accounting Information Systems},
volume = {22},
pages = {44-59},
year = {2016},
note = {2015 Research Symposium on Information Integrity & Information Systems Assurance},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300811},
author = {Michael Alles and Glen L. Gray},
keywords = {Big Data, Auditing, Accounting information systems},
abstract = {With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.}
}
@article{LIU2021257,
title = {Massive-scale carbon pollution control and biological fusion under big data context},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {257-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000042},
author = {Yi Liu and Jie Xu and Weijie Yi},
keywords = {Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware},
abstract = {In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.}
}
@article{SARALADEVI2015596,
title = {Big Data and Hadoop-a Study in Security Perspective},
journal = {Procedia Computer Science},
volume = {50},
pages = {596-601},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.091},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500592X},
author = {B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan},
keywords = {Big data ;Hadoop ;HDFS ;Security},
abstract = {Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.}
}
@article{YU201621,
title = {Single-cell Transcriptome Study as Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {14},
number = {1},
pages = {21-30},
year = {2016},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022916000437},
author = {Pingjian Yu and Wei Lin},
keywords = {Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization},
abstract = {The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.}
}
@article{KOZIEL2021116057,
title = {Investments in data quality: Evaluating impacts of faulty data on asset management in power systems},
journal = {Applied Energy},
volume = {281},
pages = {116057},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920314896},
author = {Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh},
keywords = {Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off},
abstract = {Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.}
}
@article{ASSUNCAO20153,
title = {Big Data computing and clouds: Trends and future directions},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {3-15},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514001452},
author = {Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya},
keywords = {Big Data, Cloud computing, Analytics, Data management},
abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.}
}
@article{WOLFERT201769,
title = {Big Data in Smart Farming – A review},
journal = {Agricultural Systems},
volume = {153},
pages = {69-80},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2017.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16303754},
author = {Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt},
keywords = {Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling},
abstract = {Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.}
}
@article{HUANG20181413,
title = {Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1413-1423},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17324500},
author = {Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma},
keywords = {Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network},
abstract = {With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.}
}
@article{ZHU2018107,
title = {Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data},
journal = {Annual Reviews in Control},
volume = {46},
pages = {107-133},
year = {2018},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578818301056},
author = {Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao},
keywords = {Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics},
abstract = {Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.}
}
@article{HASHEM2016748,
title = {The role of big data in smart city},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {748-758},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216302778},
author = {Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma},
keywords = {Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing},
abstract = {The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{GIL201761,
title = {Big Data. New approaches of modelling and management},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {61-63},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917301022},
author = {David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo},
abstract = {Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.}
}
@article{BUDHIRAJA2016241,
title = {The Role of Big Data in the Management of Sleep-Disordered Breathing},
journal = {Sleep Medicine Clinics},
volume = {11},
number = {2},
pages = {241-255},
year = {2016},
note = {Novel Approaches to the Management of Sleep-Disordered Breathing},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X1630008X},
author = {Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline},
keywords = {Sleep-disordered breathing, Big data, Management, Sleep apnea}
}
@article{PAPADOPOULOS20171108,
title = {The role of Big Data in explaining disaster resilience in supply chains for sustainability},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1108-1118},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301275},
author = {Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba},
keywords = {Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis},
abstract = {The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.}
}
@article{FANG2022104070,
title = {BIM-integrated portfolio-based strategic asset data quality management},
journal = {Automation in Construction},
volume = {134},
pages = {104070},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104070},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005215},
author = {Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian},
keywords = {Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management},
abstract = {A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.}
}
@article{HE201835,
title = {Statistical process monitoring as a big data analytics tool for smart manufacturing},
journal = {Journal of Process Control},
volume = {67},
pages = {35-43},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417301257},
author = {Q. Peter He and Jin Wang},
keywords = {Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things},
abstract = {With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.}
}
@article{HAMMER2017715,
title = {Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure},
journal = {Procedia CIRP},
volume = {63},
pages = {715-720},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117302408},
author = {Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer},
keywords = {Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing},
abstract = {The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.}
}
@incollection{CHESSELL201733,
title = {Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {33-48},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805467300003X},
author = {Mandy Chessell and Dan Wolfson and Tim Vincent},
keywords = {Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains},
abstract = {Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.}
}
@incollection{GONCALVESPINHO2021155,
title = {Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {155-165},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822884500009X},
author = {Manuel Gonçalves-Pinho and Alberto Freitas},
keywords = {Administrative database, Mental Health, Secondary data, Psychiatry, Research design},
abstract = {Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@incollection{SEBASTIANCOLEMAN202231,
title = {Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {31-45},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500002X},
author = {Laura Sebastian-Coleman},
keywords = {Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy},
abstract = {This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.}
}
@article{LIU2019242,
title = {How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables},
journal = {Procedia Computer Science},
volume = {155},
pages = {242-249},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919309494},
author = {Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning},
keywords = {Data analytics, wearables, sleep quality, statistical methods},
abstract = {Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.}
}
@article{CLARK2016443,
title = {Big data and ophthalmic research},
journal = {Survey of Ophthalmology},
volume = {61},
number = {4},
pages = {443-465},
year = {2016},
issn = {0039-6257},
doi = {https://doi.org/10.1016/j.survophthal.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039625716000023},
author = {Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens},
keywords = {data linkage, clinical registry, health services research, ophthalmic epidemiology, big data},
abstract = {Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.}
}
@article{ZERBINO2018818,
title = {Big Data-enabled Customer Relationship Management: A holistic approach},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {818-846},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300067},
author = {Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno},
keywords = {Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree},
abstract = {This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.}
}
@incollection{JOINER201895,
title = {Chapter 5 - Information Seeking With Big Data: Not Just the Facts},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {95-110},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000058},
author = {Ida Arlene Joiner},
keywords = {Big data, libraries, security, privacy, infrastructure, Hadoop},
abstract = {As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.}
}
@article{NGUYEN2018254,
title = {Big data analytics in supply chain management: A state-of-the-art literature review},
journal = {Computers & Operations Research},
volume = {98},
pages = {254-264},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817301685},
author = {Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin},
keywords = {Literature review, Big data, Big data analytics, Supply chain management, Research directions},
abstract = {The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.}
}
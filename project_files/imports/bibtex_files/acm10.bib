@inbook{10.1145/3447404.3447429,
author = {McMenemy, David},
title = {Ethics in Automotive User Interface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447429},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {431–432},
numpages = {2}
}

@inbook{10.1145/3447404.3447427,
author = {McMenemy, David},
title = {Ethics and Adaptive Touch Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447427},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {407–408},
numpages = {2}
}

@inbook{10.1145/3447404.3447413,
author = {McMenemy, David},
title = {Ethical Issues of Digital Signal Processing},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447413},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {141–142},
numpages = {2}
}

@inbook{10.1145/3447404.3447425,
author = {McMenemy, David},
title = {Ethics and Personal Context},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447425},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {377–378},
numpages = {2}
}

@article{10.1145/3533378,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {Machine learning applications, sofware deployment}
}

@inbook{10.1145/3447404.3447421,
author = {McMenemy, David},
title = {Ethical Issues in Probabilistic Text Entry},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447421},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {321–322},
numpages = {2}
}

@inproceedings{10.1145/3277139.3277179,
author = {Lu, Yi and Yin, Jun and Ge, Shilun},
title = {Analysis of Dynamic Complexity Feature of Information System Data Based on Visualization},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277179},
doi = {10.1145/3277139.3277179},
abstract = {The volume and complexity of the data in the information system continues to increase during the operation of the information system. The phenomenon leads to the fact that decision-makers are faced with data-rich and information-deficient situations. How to intuitively grasp the dynamic complexity of information system data has become a concern of scholars. Therefore, this paper collected and organized the system log data of two shipping companies, adopted multiple visualization forms, and combined the dynamic complexity measurement methods of information systems, focusing on the overall distribution status and development trend of the data dynamic complexity and internal information system and the influence of complexity of subsystems within the information system on overall data dynamic operation complexity, and then analyzed the overall feature of the dynamic complexity of information system data, and summarized the feature regulars of the complexity of the visualization method. The results show that the visual feature analysis of the dynamic complexity of information systems can improve the ability of enterprises to analyze and make decisions, and provide a basis for enterprise managers to develop corresponding management measures.},
booktitle = {Proceedings of the 2018 International Conference on Information Management &amp; Management Science},
pages = {206–215},
numpages = {10},
keywords = {visualization, feature analysis, management information system, dynamic complexity},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1145/3209281.3209317,
author = {Batubara, F. Rizal and Ubacht, Jolien and Janssen, Marijn},
title = {Challenges of Blockchain Technology Adoption for E-Government: A Systematic Literature Review},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209317},
doi = {10.1145/3209281.3209317},
abstract = {The ability of blockchain technology to record transactions on distributed ledgers offers new opportunities for governments to improve transparency, prevent fraud, and establish trust in the public sector. However, blockchain adoption and use in the context of e-Government is rather unexplored in academic literature. In this paper, we systematically review relevant research to understand the current research topics, challenges and future directions regarding blockchain adoption for e-Government. The results show that the adoption of blockchain-based applications in e-Government is still very limited and there is a lack of empirical evidence. The main challenges faced in blockchain adoption are predominantly presented as technological aspects such as security, scalability and flexibility. From an organizational point of view, the issues of acceptability and the need of new governance models are presented as the main barriers to adoption. Moreover, the lack of legal and regulatory support is identified as the main environmental barrier of adoption. Based on the challenges presented in the literature, we propose future research questions that need to be addressed to inform how the public sector should approach the blockchain technology adoption.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {76},
numpages = {9},
keywords = {blockchain, public service, adoption, literature review, government},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.5555/3192424.3192631,
author = {Musciotto, Federico and Delpriori, Saverio and Castagno, Paolo and Pournaras, Evangelos},
title = {Mining Social Interactions in Privacy-Preserving Temporal Networks},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {The opportunities to empirically study temporal networks nowadays are immense thanks to Internet of Things technologies along with ubiquitous and pervasive computing that allow a real-time fine-grained collection of social network data. This empowers data analytics and data scientists to reason about complex temporal phenomena, such as disease spread, residential energy consumption, political conflicts etc., using systematic methologies from complex networks and graph spectra analysis. However, a misuse of these methods may result in privacy-intrusive and discriminatory actions that may threaten citizens' autonomy and put their life under surveillance. This paper studies highly sparse temporal networks that model social interactions such as the physical proximity of participants in conferences. When citizens can self-determine the anonymized proximity data they wish to share via privacy-preserving platforms, temporal networks may turn out to be highly sparse and have low quality. This paper shows that even in this challenging scenario of privacy-by-design, significant information can be mined from temporal networks such as the correlation of events happening during a conference or stable groups interacting over time. The findings of this paper contribute to the introduction of privacy-preserving data analytics in temporal networks and their applications.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1103–1110},
numpages = {8},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3287560.3287577,
author = {Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill},
title = {Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287577},
doi = {10.1145/3287560.3287577},
abstract = {Data too sensitive to be "open" for analysis and re-purposing typically remains "closed" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {191–200},
numpages = {10},
keywords = {data ethics, data governance, data sharing, algorithmic bias, privacy},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{10.1109/TNET.2018.2829173,
author = {Tu, Zhen and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng},
title = {A New Privacy Breach: User Trajectory Recovery From Aggregated Mobility Data},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2829173},
doi = {10.1109/TNET.2018.2829173},
abstract = {Human mobility data have been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual’s mobility records usually gives rise to privacy issues, data sets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users’ privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals’ trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual’s trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world data sets collected from both the mobile application and cellular network, we reveal that the attack system is able to recover users’ trajectories with an accuracy of about 73%~91% at the scale of thousands to ten thousands of mobile users, which indicates severe privacy leakage in such data sets. Our extensive analysis also reveals that by generalization and perturbation, this kind of privacy leakage can only be mitigated. Through the investigation on aggregated mobility data, this paper recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both the academy and industry.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1446–1459},
numpages = {14}
}

@inproceedings{10.5555/3433701.3433811,
author = {Grosset, Pascal and Biwer, Christopher M. and Pulido, Jesus and Mohan, Arvind T. and Biswas, Ayan and Patchett, John and Turton, Terece L. and Rogers, David H. and Livescu, Daniel and Ahrens, James},
title = {Foresight: Analysis That Matters for Data Reduction},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {83},
numpages = {15},
keywords = {performance evaluation, multi-layer neural network, data compression},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/2988336.2988349,
author = {Singh, Jatinder and Pasquier, Thomas and Bacon, Jean and Powles, Julia and Diaconu, Raluca and Eyers, David},
title = {Big Ideas Paper: Policy-Driven Middleware for a Legally-Compliant Internet of Things},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988349},
doi = {10.1145/2988336.2988349},
abstract = {Internet of Things (IoT) applications, systems and services are subject to law. We argue that for the IoT to develop lawfully, there must be technical mechanisms that allow the enforcement of specified policy, such that systems align with legal realities. The audit of policy enforcement must assist the apportionment of liability, demonstrate compliance with regulation, and indicate whether policy correctly captures legal responsibilities. As both systems and obligations evolve dynamically, this cycle must be continuously maintained.This poses a huge challenge given the global scale of the IoT vision. The IoT entails dynamically creating new services through managed and flexible data exchange. Data management is complex in this dynamic environment, given the need to both control and share information, often across federated domains of administration.We see middleware playing a key role in managing the IoT. Our vision is for a middleware-enforced, unified policy model that applies end-to-end, throughout the IoT. This is because policy cannot be bound to things, applications, or administrative domains, since functionality is the result of composition, with dynamically formed chains of data flows.We have investigated the use of Information Flow Control (IFC) to manage and audit data flows in cloud computing; a domain where trust can be well-founded, regulations are more mature and associated responsibilities clearer. We feel that IFC has great potential in the broader IoT context. However, the sheer scale and the dynamic, federated nature of the IoT pose a number of significant research challenges.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {13},
numpages = {15},
keywords = {Law, policy specification and enforcement, audit, regulation},
location = {Trento, Italy},
series = {Middleware '16}
}

@article{10.1145/3512944,
author = {Gupta, Srishti and Jablonski, Julia and Tsai, Chun-Hua and Carroll, John M.},
title = {Instagram of Rivers: Facilitating Distributed Collaboration in Hyperlocal Citizen Science},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512944},
doi = {10.1145/3512944},
abstract = {Citizen science project leaders collecting field data in a hyperlocal community often face common socio-technical challenges, which can potentially be addressed by sharing innovations across different groups through peer-to-peer collaboration. However, most citizen science groups practice in isolation, and end up re-inventing the wheel when it comes to addressing these common challenges. This study seeks to investigate distributed collaboration between different water monitoring citizen science groups. We discovered a unique social network application called Water Reporter that mediated distributed collaboration by creating more visibility and transparency between groups using the app. We interviewed 8 citizen science project leaders who were users of this app, and 6 other citizen science project leaders to understand how distributed collaboration mediated by this app differed from collaborative practices of Non Water Reporter users. We found that distributed collaboration was an important goal for both user groups, however, the tasks that support these collaboration activities differed for the two user groups.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {97},
numpages = {22},
keywords = {distributed collaboration, collaboratory, sustainability, citizen science}
}

@inbook{10.1145/3447404.3447419,
author = {McMenemy, David},
title = {Ethical Issues in Brain–Computer Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447419},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {273–275},
numpages = {3}
}

@inbook{10.1145/3447404.3447417,
author = {McMenemy, David},
title = {Ethics and Smart Cities},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447417},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {235–236},
numpages = {2}
}

@inproceedings{10.5555/3103620.3103631,
author = {Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua},
title = {Validating a Deep Learning Framework by Metamorphic Testing},
year = {2017},
isbn = {9781538604243},
publisher = {IEEE Press},
abstract = {Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.},
booktitle = {Proceedings of the 2nd International Workshop on Metamorphic Testing},
pages = {28–34},
numpages = {7},
keywords = {deep learning, support vector machine, software validation, neural network, metamorphic testing},
location = {Buenos Aires, Argentina},
series = {MET '17}
}

@article{10.1145/2724730,
author = {Basole, Rahul C. and Russell, Martha G. and Huhtam\"{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo},
title = {Understanding Business Ecosystem Dynamics: A Data-Driven Approach},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2724730},
doi = {10.1145/2724730},
abstract = {Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jun},
articleno = {6},
numpages = {32},
keywords = {information visualization, Data triangulation, interorganizational networks, business ecosystem}
}

@inproceedings{10.1145/2213836.2213961,
author = {Morton, Kristi and Bunker, Ross and Mackinlay, Jock and Morton, Robert and Stolte, Chris},
title = {Dynamic Workload Driven Data Integration in Tableau},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213961},
doi = {10.1145/2213836.2213961},
abstract = {Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {807–816},
numpages = {10},
keywords = {visualization, data integration},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}

@inproceedings{10.5555/3242181.3242206,
author = {McGinnis, Leon F. and Rose, Oliver},
title = {History and Perspective of Simulation in Manufacturing},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Manufacturing systems incorporate many semi-independent, yet strongly interacting processes, usually exhibiting some stochastic behavior. As a consequence, overall system behavior, in the long run but also in the short run, is very difficult to predict. Not surprisingly, both practitioners and academics recognized in the 1950's the potential value of discrete event simulation technology in supporting manufacturing system decision-making. This short history is one perspective on the development and evolution of discrete event simulation technology and applications, specifically focusing on manufacturing applications. This assessment is based on an examination of the literature, our own experiences, and interviews with leading practitioners. History is interesting, but it's useful only if it helps us see a way forward, so we offer some opinions on the state of the research and practice of simulation in manufacturing, and the opportunities to further advance the field.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {24},
numpages = {13},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3442188.3445918,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {machine learning, datasets, requirements engineering},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/2464464.2464475,
author = {Rieder, Bernhard},
title = {Studying Facebook via Data Extraction: The Netvizz Application},
year = {2013},
isbn = {9781450318891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464464.2464475},
doi = {10.1145/2464464.2464475},
abstract = {This paper describes Netvizz, a data collection and extraction application that allows researchers to export data in standard file formats from different sections of the Facebook social networking service. Friendship networks, groups, and pages can thus be analyzed quantitatively and qualitatively with regards to demographical, post-demographical, and relational characteristics. The paper provides an overview over analytical directions opened up by the data made available, discusses platform specific aspects of data extraction via the official Application Programming Interface, and briefly engages the difficult ethical considerations attached to this type of research.},
booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
pages = {346–355},
numpages = {10},
keywords = {Facebook, social network analysis, data extraction, media studies, social networking services, research tool},
location = {Paris, France},
series = {WebSci '13}
}

@inproceedings{10.1145/3371158.3371167,
author = {Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar},
title = {Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371167},
doi = {10.1145/3371158.3371167},
abstract = {Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {73–81},
numpages = {9},
keywords = {satellite imagery, socio-economic development, census, poverty mapping, Landsat, temporal prediction},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/2723372.2750549,
author = {Wang, Xiaolan and Dong, Xin Luna and Meliou, Alexandra},
title = {Data X-Ray: A Diagnostic Tool for Data Errors},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2750549},
doi = {10.1145/2723372.2750549},
abstract = {A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this paper we focus on data diagnosis: explaining where and how the errors happen in a data generative process.We develop a large-scale diagnostic framework called DATA X-RAY. Our contributions are three-fold. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitude more efficient than existing techniques.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1231–1245},
numpages = {15},
keywords = {data cleaning, error diagnosis, data profiling},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2882903.2915232,
author = {Fan, Wenfei and Wu, Yinghui and Xu, Jingbo},
title = {Functional Dependencies for Graphs},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2915232},
doi = {10.1145/2882903.2915232},
abstract = {We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1843–1857},
numpages = {15},
keywords = {validation, graphs, satisfiability, functional dependencies, implication},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3379504,
author = {Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng},
title = {Multi-Label Active Learning Algorithms for Image Classification: Overview and Future Promise},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379504},
doi = {10.1145/3379504},
abstract = {Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {28},
numpages = {35},
keywords = {active learning, sampling strategy, multi-label image, Image classification, annotation}
}

@article{10.1109/TCBB.2015.2474376,
author = {Lee, En-Shiun Annie and Sze-To, Ho-Yin Antonio and Wong, Man-Hon and Leung, Kwong-Sak and Lau, Terrence Chi-Kong and Wong, Andrew K. C.},
title = {Discovering Protein-DNA Binding Cores by Aligned Pattern Clustering},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2474376},
doi = {10.1109/TCBB.2015.2474376},
abstract = {Understanding binding cores is of fundamental importance in deciphering Protein-DNA TF-TFBS binding and gene regulation. Limited by expensive experiments, it is promising to discover them with variations directly from sequence data. Although existing computational methods have produced satisfactory results, they are one-to-one mappings with no site-specific information on residue/nucleotide variations, where these variations in binding cores may impact binding specificity. This study presents a new representation for modeling binding cores by incorporating variations and an algorithm to discover them from only sequence data. Our algorithm takes protein and DNA sequences from TRANSFAC a Protein-DNA Binding Database as input; discovers from both sets of sequences conserved regions in Aligned Pattern Clusters APCs; associates them as Protein-DNA Co-Occurring APCs; ranks the Protein-DNA Co-Occurring APCs according to their co-occurrence, and among the top ones, finds three-dimensional structures to support each binding core candidate. If successful, candidates are verified as binding cores. Otherwise, homology modeling is applied to their close matches in PDB to attain new chemically feasible binding cores. Our algorithm obtains binding cores with higher precision and much faster runtime (≥1,600x) than that of its contemporaries, discovering candidates that do not co-occur as one-to-one associated patterns in the raw data. Availability: http://www.pami.uwaterloo.ca/~ealee/files/tcbbPnDna2015/Release.zip.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {254–263},
numpages = {10}
}

@article{10.1145/2949741.2949756,
author = {De Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce},
title = {DeepDive: Declarative Knowledge Base Construction},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/2949741.2949756},
doi = {10.1145/2949741.2949756},
abstract = {The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {60–67},
numpages = {8}
}

@article{10.1145/3487043,
author = {Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
title = {Software Engineering for AI-Based Systems: A Survey},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3487043},
doi = {10.1145/3487043},
abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {37e},
numpages = {59},
keywords = {artificial intelligence, systematic mapping study, Software engineering, AI-based systems}
}

@inproceedings{10.1145/3506860.3506939,
author = {Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan},
title = {Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506939},
doi = {10.1145/3506860.3506939},
abstract = { Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed. },
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {381–391},
numpages = {11},
keywords = {co-design, educational quality, implementation strategy, academic analytics, higher education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3428502.3428508,
author = {Viscusi, Gianluigi and Collins, Aengus and Florin, Marie-Valentine},
title = {Governments' Strategic Stance toward Artificial Intelligence: An Interpretive Display on Europe},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428508},
doi = {10.1145/3428502.3428508},
abstract = {This article aims to provide an interpretive display of the strategic stance toward innovation enabled by Artificial Intelligence (AI) of different governments based in Europe. The analysis includes the European Union (EU), some of its members as well as a non-member country, which we argue presents interesting characteristics. Based on a comprehensive analysis of a corpus of documents, which includes national strategies, external reports as well as web resources, the different countries considered in this article are subsequently classified using as interpretive lens, among other frameworks, the strategic types identified by Miles and Snow (defender, prospector, analyzer, reactor). The results show a prevalence of a "prospector" stance, interested in differentiation and the search of novelty. However, the results also show that similar strategic types may be driven by different values as well as governance orientation among the considered countries, thus leading to different potential ways to implement the expected AI-enabled innovation.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {44–53},
numpages = {10},
keywords = {Digitalization, Artificial Intelligence, Strategy, Strategic Types, Public Sector, Miles and Snow, Governance, Policies},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3131542.3131564,
author = {Missier, Paolo and Bajoudah, Shaimaa and Capossele, Angelo and Gaglione, Andrea and Nati, Michele},
title = {Mind My Value: A Decentralized Infrastructure for Fair and Trusted IoT Data Trading},
year = {2017},
isbn = {9781450353182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131542.3131564},
doi = {10.1145/3131542.3131564},
abstract = {Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.},
booktitle = {Proceedings of the Seventh International Conference on the Internet of Things},
articleno = {15},
numpages = {8},
location = {Linz, Austria},
series = {IoT '17}
}

@inproceedings{10.1145/3505711.3505712,
author = {Bala Bisandu, Desmond and Salih Homaid, Mohammed and Moulitsas, irene and Filippone, Salvatore},
title = {A Deep Feedforward Neural Network and Shallow Architectures Effectiveness Comparison: Flight Delays Classification Perspective},
year = {2021},
isbn = {9781450390699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505711.3505712},
doi = {10.1145/3505711.3505712},
abstract = {Flight delays have negatively impacted the socio-economics state of passengers, airlines and airports, resulting in huge economic losses. Hence, it has become necessary to correctly predict their occurrences in decision-making because it is important for the effective management of the aviation industry. Developing accurate flight delays classification models depends mostly on the air transportation system complexity and the infrastructure available in airports, which may be a region-specific issue. However, no specific prediction or classification model can handle the individual characteristics of all airlines and airports at the same time. Hence, the need to further develop and compare predictive models for the aviation decision system of the future cannot be over-emphasised. In this research, flight on-time data records from the United State Bureau of Transportation Statistics was employed to evaluate the performances of Deep Feedforward Neural Network, Neural Network, and Support Vector Machine models on a binary classification problem. The research revealed that the models achieved different accuracies of flight delay classifications. The Support Vector Machine had the worst average accuracy than Neural Network and Deep Feedforward Neural Network in the initial experiment. The Deep Feedforward Neural Network outperformed Support Vector Machines and Neural Network with the best average percentage accuracies. Going further to investigate the Deep Feedforward Neural Network architecture on different parameters against itself suggest that training a Deep Feedforward Neural Network algorithm, regardless of data training size, the classification accuracy peaks. We examine which number of epochs works best in our flight delay classification settings for the Deep Feedforward Neural Network. Our experiment results demonstrate that having many epochs affects the convergence rate of the model; unlike when hidden layers are increased, it does not ensure better or higher accuracy in a binary classification of flight delays. Finally, we recommended further studies on the applicability of the Deep Feedforward Neural Network in flight delays prediction with specific case studies of either airlines or airports to check the impact on the model's performance.},
booktitle = {2021 The 5th International Conference on Advances in Artificial Intelligence (ICAAI)},
pages = {1–10},
numpages = {10},
keywords = {deep learning, flight delays, Support Vector Machine, Classification, deep neural network},
location = {Virtual Event, United Kingdom},
series = {ICAAI 2021}
}

@inproceedings{10.1145/3236461.3241971,
author = {Tufte, Kristin and Datta, Kushal and Jindal, Alekh and Maier, David and Bertini, Robert L.},
title = {Challenges and Opportunities in Transportation Data},
year = {2018},
isbn = {9781450357869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236461.3241971},
doi = {10.1145/3236461.3241971},
abstract = {From the time and money lost sitting in congestion and waiting for traffic signals to change, to the many people injured and killed in traffic crashes each year, to the emissions and energy consumption from our vehicles, the effects of transportation on our daily lives are immense. A wealth of transportation data is available to help address these problems; from data from sensors installed to monitor and operate the roadways and traffic signals to data from cell phone apps and -- just over the horizon -- data from connected vehicles and infrastructure. However, this wealth of data has yet to be effectively leveraged, thus providing opportunities in areas such as improving traffic safety, reducing congestion, improving traffic signal timing, personalizing routing, coordinating across transportation agencies and more. This paper presents opportunities and challenges in applying data management technology to the transportation domain.},
booktitle = {Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {2},
numpages = {8},
keywords = {Smart Cities, Transportation Data, Data Management},
location = {Portland, OR, USA},
series = {SCC '18}
}

@inproceedings{10.5555/3242181.3242220,
author = {Taylor, Simon J. E. and Anagnostou, Anastasia and Fabiyi, Adedeji and Currie, Christine and Monks, Thomas and Barbera, Roberto and Becker, Bruce},
title = {Open Science: Approaches and Benefits for Modeling &amp; Simulation},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Open Science is the practice of making scientific research accessible to all. It promotes open access to the artefacts of research, the software, data, results and the scientific articles in which they appear, so that others can validate, use and collaborate. Open Science is also being mandated by many funding bodies. The concept of Open Science is new to many Modelling &amp; Simulation (M&amp;S) researchers. To introduce Open Science to our field, this paper unpacks Open Science to understand some of its approaches and benefits. Good practice in the reporting of simulation studies is discussed and the Strengthening the Reporting of Empirical Simulation Studies (STRESS) standardized checklist approach is presented. A case study shows how Digital Object Identifiers, Researcher Registries, Open Access Data Repositories and Scientific Gateways can support Open Science practices for M&amp;S research. The article concludes with a set of guidelines for adopting Open Science for M&amp;S.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {36},
numpages = {15},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.1145/3490384,
author = {Anand, Sanjay Kumar and Kumar, Suresh},
title = {Experimental Comparisons of Clustering Approaches for Data Representation},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3490384},
doi = {10.1145/3490384},
abstract = {Clustering approaches are extensively used by many areas such as IR, Data Integration, Document Classification, Web Mining, Query Processing, and many other domains and disciplines. Nowadays, much literature describes clustering algorithms on multivariate data sets. However, there is limited literature that presented them with exhaustive and extensive theoretical analysis as well as experimental comparisons. This experimental survey paper deals with the basic principle, and techniques used, including important characteristics, application areas, run-time performance, internal, external, and stability validity of cluster quality, etc., on five different data sets of eleven clustering algorithms. This paper analyses how these algorithms behave with five different multivariate data sets in data representation. To answer this question, we compared the efficiency of eleven clustering approaches on five different data sets using three validity metrics-internal, external, and stability and found the optimal score to know the feasible solution of each algorithm. In addition, we have also included four popular and modern clustering algorithms with only their theoretical discussion. Our experimental results for only traditional clustering algorithms showed that different algorithms performed different behavior on different data sets in terms of running time (speed), accuracy and, the size of data set. This study emphasized the need for more adaptive algorithms and a deliberate balance between the running time and accuracy with their theoretical as well as implementation aspects.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {45},
numpages = {33},
keywords = {optimal score, external validation, stability validation, internal validation, Clustering approach}
}

@inproceedings{10.1145/3131672.3131690,
author = {Park, Jaeyeon and Nam, Woojin and Choi, Jaewon and Kim, Taeyeong and Yoon, Dukyong and Lee, Sukhoon and Paek, Jeongyeup and Ko, JeongGil},
title = {Glasses for the Third Eye: Improving the Quality of Clinical Data Analysis with Motion Sensor-Based Data Filtering},
year = {2017},
isbn = {9781450354592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131672.3131690},
doi = {10.1145/3131672.3131690},
abstract = {Recent advances in machine learning based data analytics are opening opportunities for designing effective clinical decision support systems (CDSS) which can become the "third-eye" in the current clinical procedures and diagnosis. However, common patient movements in hospital wards may lead to faulty measurements in physiological sensor readings, and training a CDSS from such noisy data can cause misleading predictions, directly leading to potentially dangerous clinical decisions. In this work, we present MediSense, a system to sense, classify, and identify noise-causing motions and activities that affect physiological signal when made by patients on their hospital beds. Essentially, such a system can be considered as "glasses" for the clinical third eye in correctly observing medical data. MediSense combines wirelessly connected embedded platforms for motion detection with physiological signal data collected from patients to identify faulty physiological signal measurements and filters such noisy data from being used in CDSS training or testing datasets. We deploy our system in real intensive care units (ICUs), and evaluate its performance from real patient traces collected at these ICUs through a 4-month pilot study at the Ajou University Hospital Trauma Center, a major hospital facility located in Suwon, South Korea. Our results show that MediSense successfully classifies patient motions on the bed with &gt;90% accuracy, shows 100% reliability in determining the locations of beds within the ICU, and each bed-attached sensor achieves a lifetime of more than 33 days, which satisfies the application-level requirements suggested by our clinical partners. Furthermore, a simple case-study with arrhythmia patient data shows that MediSense can help improve the clinical diagnosis accuracy.},
booktitle = {Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems},
articleno = {8},
numpages = {14},
keywords = {Clinical Decision Support System, Health Care Information Systems, Wireless Sensor Network, Motion Sensing, Noise Filter},
location = {Delft, Netherlands},
series = {SenSys '17}
}

@article{10.14778/2876473.2876478,
author = {Li, Zeyu and Wang, Hongzhi and Shao, Wei and Li, Jianzhong and Gao, Hong},
title = {Repairing Data through Regular Expressions},
year = {2016},
issue_date = {January 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2876473.2876478},
doi = {10.14778/2876473.2876478},
abstract = {Since regular expressions are often used to detect errors in sequences such as strings or date, it is natural to use them for data repair. Motivated by this, we propose a data repair method based on regular expression to make the input sequence data obey the given regular expression with minimal revision cost. The proposed method contains two steps, sequence repair and token value repair.For sequence repair, we propose the Regular-expression-based Structural Repair (RSR in short) algorithm. RSR algorithm is a dynamic programming algorithm that utilizes Nondeterministic Finite Automata (NFA) to calculate the edit distance between a prefix of the input string and a partial pattern regular expression with time complexity of O(nm2) and space complexity of O(mn) where m is the edge number of NFA and n is the input string length. We also develop an optimization strategy to achieve higher performance for long strings. For token value repair, we combine the edit-distance-based method and associate rules by a unified argument for the selection of the proper method. Experimental results on both real and synthetic data show that the proposed method could repair the data effectively and efficiently.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {432–443},
numpages = {12}
}

@article{10.1145/3402524,
author = {M\"{a}kitalo, Niko and Flores-Martin, Daniel and Flores, Huber and Lagerspetz, Eemil and Christophe, Francois and Ihantola, Petri and Babazadeh, Masiar and Hui, Pan and Murillo, Juan Manuel and Tarkoma, Sasu and Mikkonen, Tommi},
title = {Human Data Model: Improving Programmability of Health and Well-Being Data for Enhanced Perception and Interaction},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3402524},
doi = {10.1145/3402524},
abstract = {Today, an increasing number of systems produce, process, and store personal and intimate data. Such data has plenty of potential for entirely new types of software applications, as well as for improving old applications, particularly in the domain of smart healthcare. However, utilizing this data, especially when it is continuously generated by sensors and other devices, with the current approaches is complex—data is often using proprietary formats and storage, and mixing and matching data of different origin is not easy. Furthermore, many of the systems are such that they should stimulate interactions with humans, which further complicates the systems. In this article, we introduce the Human Data Model—a new tool and a programming model for programmers and end users with scripting skills that help combine data from various sources, perform computations, and develop and schedule computer-human interactions. Written in JavaScript, the software implementing the model can be run on almost any computer either inside the browser or using Node.js. Its source code can be freely downloaded from GitHub, and the implementation can be used with the existing IoT platforms. As a whole, the work is inspired by several interviews with professionals, and an online survey among healthcare and education professionals, where the results show that the interviewed subjects almost entirely lack ideas on how to benefit the ever-increasing amount of data measured of the humans. We believe that this is because of the missing support for programming models for accessing and handling the data, which can be satisfied with the Human Data Model.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {26},
numpages = {39},
keywords = {wearable computers, pervasive computing, Mobile computing, Human Data Model, programmable world, Internet of Things, data management, data mashups, ubiquitous computing, IoT}
}

@article{10.1145/3243043,
author = {Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.},
title = {Gait-Based Person Re-Identification: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3243043},
doi = {10.1145/3243043},
abstract = {The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {33},
numpages = {34},
keywords = {person re-identification, Video surveillance, gait analysis, biometrics, machine learning, computer vision}
}

@inproceedings{10.1145/2618243.2618244,
author = {Chen, I-Min A. and Markowitz, Victor M. and Szeto, Ernest and Palaniappan, Krishna and Chu, Ken},
title = {Maintaining a Microbial Genome &amp; Metagenome Data Analysis System in an Academic Setting},
year = {2014},
isbn = {9781450327220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618243.2618244},
doi = {10.1145/2618243.2618244},
abstract = {The Integrated Microbial Genomes (IMG) system integrates microbial community aggregate genomes (metagenomes) with genomes from all domains of life. IMG provides tools for analyzing and reviewing the structural and functional annotations of metagenomes and genomes in a comparative context. At the core of the IMG system is a data warehouse that contains genome and metagenome datasets provided by scientific users, as well as public bacterial, archaeal, eukaryotic, and viral genomes from the US National Center for Biotechnology Information genomic archive and a rich set of engineered, environmental and host associated metagenomes. Genomes and metagenome datasets are processed using IMG's microbial genome and metagenome sequence data processing pipelines and then are integrated into the data warehouse using IMG's data integration toolkit. Microbial genome and metagenome application specific user interfaces provide access to different subsets of IMG's data and analysis toolkits. Genome and metagenome analysis is a gene centric iterative process that involves a sequence (composition) of data exploration and comparative analysis operations, with individual operations expected to have rapid response time.From its first release in 2005, IMG has grown from an initial content of about 300 genomes with a total of 2 million genes, to 22,578 bacterial, archaeal, eukaryotic and viral genomes, and 4,188 metagenome samples, with about 24.6 billion genes as of May 1st, 2014. IMG's database architecture is continuously revised in order to cope with the rapid increase in the number and size of the genome and metagenome datasets, maintain good query performance, and accommodate new data types. We present in this paper IMG's new database architecture developed over the past three years in the context of limited financial, engineering and data management resources customary to academic database systems. We discuss the alternative commercial and open source database management systems we considered and experimented with and describe the hybrid architecture we devised for sustaining IMG's rapid growth.},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
articleno = {3},
numpages = {11},
keywords = {genome data analysis system, data warehouse},
location = {Aalborg, Denmark},
series = {SSDBM '14}
}

@inbook{10.1145/2993318.2993320,
author = {Esteves, Diego and Mendes, Pablo N. and Moussallem, Diego and Duarte, Julio Cesar and Zaveri, Amrapali and Lehmann, Jens},
title = {MEX Interfaces: Automating Machine Learning Metadata Generation},
year = {2016},
isbn = {9781450347525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993318.2993320},
abstract = {Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: "What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?". We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary.},
booktitle = {Proceedings of the 12th International Conference on Semantic Systems},
pages = {17–24},
numpages = {8}
}

@article{10.14778/3231751.3231759,
author = {Nazi, Azade and Ding, Bolin and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Efficient Estimation of Inclusion Coefficient Using Hyperloglog Sketches},
year = {2018},
issue_date = {June 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3231751.3231759},
doi = {10.14778/3231751.3231759},
abstract = {Efficiently estimating the inclusion coefficient - the fraction of values of one column that are contained in another column - is useful for tasks such as data profiling and foreign-key detection. We present a new estimator, BML, for inclusion coefficient based on Hyperloglog sketches that results in significantly lower error compared to the state-of-the art approach that uses Bottom-k sketches. We evaluate the error of the BML estimator using experiments on industry benchmarks such as TPC-H and TPC-DS, and several real-world databases. As an independent contribution, we show how Hyperloglog sketches can be maintained incrementally with data deletions using only a constant amount of additional memory.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {1097–1109},
numpages = {13}
}

@inproceedings{10.1145/3209811.3209877,
author = {Zegura, Ellen and DiSalvo, Carl and Meng, Amanda},
title = {Care and the Practice of Data Science for Social Good},
year = {2018},
isbn = {9781450358163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209811.3209877},
doi = {10.1145/3209811.3209877},
abstract = {Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of "good" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.},
booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
articleno = {34},
numpages = {9},
keywords = {Data science for social good, HCI, care, community engagement},
location = {Menlo Park and San Jose, CA, USA},
series = {COMPASS '18}
}

@article{10.1145/2629446,
author = {Partington, Andrew and Wynn, Moe and Suriadi, Suriadi and Ouyang, Chun and Karnon, Jonathan},
title = {Process Mining for Clinical Processes: A Comparative Analysis of Four Australian Hospitals},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629446},
doi = {10.1145/2629446},
abstract = {Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jan},
articleno = {19},
numpages = {18},
keywords = {data preparation, patient pathways, comparative analysis, health care delivery, Process mining}
}

@article{10.1145/3480947,
author = {Shaw, Mary},
title = {Myths and Mythconceptions: What Does It Mean to Be a Programming Language, Anyhow?},
year = {2022},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3480947},
doi = {10.1145/3480947},
abstract = {Modern software does not stand alone; it is embedded in complex physical and sociotechnical systems. It relies on computational support from interdependent subsystems as well as non-code resources such as data, communications, sensors, and interactions with humans. Both general-purpose programming languages and mainstream programming language research focus on symbolic notations with well-defined abstractions that are intended for use by professionals to write programs that solve precisely specified problems. There is a strong emphasis on correctness of the resulting programs, preferably by formal reasoning. However, these languages, despite their careful design and formal foundations, address only a modest portion of modern software and only a minority of software developers.  Several persistent myths reinforce this focus. These myths express an idealized model of software and software development. They provide a lens for examining modern software and software development practice: highly trained professionals are outnumbered by vernacular developers. Writing new code is dominated by composition of ill-specified software and non-software components. General-purpose languages may be less appropriate for a task than domain-specific languages, and functional correctness is often a less appropriate goal than overall fitness for task. Support for programming to meet a specification is of little help to people who are programming in order to understand their problems. Reasoning about software is challenged by uncertainty and nondeterminism in the execution environment and by the increasingly dominant role of data, especially with the advent of systems that rely on machine learning. The lens of our persistent myths illuminates the dissonance between our idealized view of software development and common practice, which enables us to identify emerging opportunities and challenges for programming language research.},
journal = {Proc. ACM Program. Lang.},
month = {apr},
articleno = {234},
numpages = {44},
keywords = {domain-specific programming language, problem-setting design, closed software system, open resource coalition, generality-power tradeoffs, general-purpose programming language, vernacular software developer, sufficient correctness, fitness to task, formal specifications, exploratory programming, software credentials, problem-solving design}
}

@article{10.1145/3494566,
author = {Sharma, Ms Promila and Meena, Uma and Sharma, Girish Kumar},
title = {Intelligent Data Analysis Using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3494566},
doi = {10.1145/3494566},
abstract = {Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {94},
numpages = {20},
keywords = {Intelligent data, tourism industry}
}

@inproceedings{10.1145/3465481.3470037,
author = {Hus\'{a}k, Martin and La\v{s}tovi\v{c}ka, Martin and Tovar\v{n}\'{a}k, Daniel},
title = {System for Continuous Collection of Contextual Information for Network Security Management and Incident Handling},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470037},
doi = {10.1145/3465481.3470037},
abstract = { In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.},
booktitle = {The 16th International Conference on Availability, Reliability and Security},
articleno = {112},
numpages = {8},
keywords = {Network Monitoring, Cyber Situational Awareness, Incident Handling, Cybersecurity, Incident Response},
location = {Vienna, Austria},
series = {ARES 2021}
}

@inproceedings{10.5555/3017447.3017501,
author = {Tang, Rong and Mon, Lorri and Beheshti, Jamshid and Li, Yuelin and Pollock, Danielle and Ni, Chaoqun and Chu, Samuel and Xiao, Lu and Caffrey, Julia and Gentry, Steven},
title = {Needs Assessment of ASIS&amp;T Publications: Bridging Information Research and Practice},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {This study reports the results of a 2016 online survey on perceptions and uses of ASIS&amp;T publications. The 190 survey respondents represented 26 countries and 5 continents, with 77% of participants coming from academia rather than practitioners. Among the emerging themes were calls for a wider scope of research from information science to be reflected in the publications (including JASIS&amp;T and the ASIS&amp;T Proceedings), and ongoing challenges in the role of the Bulletin as a bridge between research and practice. The study provides insights into the scholarly publishing practices of the ASIS&amp;T community and highlights key issues for the future direction of ASIS&amp;T's scholarly communication.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {54},
numpages = {10},
keywords = {knowledge transfer, publication format and processes, ASIS&amp;T publications, user groups},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inbook{10.1145/3447404.3447407,
author = {McMenemy, David},
title = {Ethical Issues in Digital Signal Processing and Machine Learning},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447407},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {15–19},
numpages = {5}
}

@inproceedings{10.1145/3239060.3239090,
author = {Baltodano, Sonia and Garcia-Mancilla, Jesus and Ju, Wendy},
title = {Eliciting Driver Stress Using Naturalistic Driving Scenarios on Real Roads},
year = {2018},
isbn = {9781450359467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239060.3239090},
doi = {10.1145/3239060.3239090},
abstract = {We propose a novel method for reliably inducing stress in drivers for the purpose of generating real-world participant data for machine learning, using both scripted in-vehicle stressor events and unscripted on-road stressors such as pedestrians and construction zones. On-road drives took place in a vehicle outfitted with an experimental display that lead drivers to believe they had prematurely ran out of charge on an isolated road. We describe the elicitation method, course design, instrumentation, data collection procedure and the post-hoc labeling of unplanned road events to illustrate how rich data about a variety of stress-related events can be elicited from study participants on-road. We validate this method with data including psychophysiological measurements, video, voice, and GPS data from (N=20) participants. Results from algorithmic psychophysiological stress analysis were validated using participant self-reports. Results of stress elicitation analysis show that our method elicited a stress-state in 89% of participants.},
booktitle = {Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {298–309},
numpages = {12},
keywords = {Wizard of Oz, Stress, Driver Benchmarking, Design Methods, Interaction Design, Driver Evaluation},
location = {Toronto, ON, Canada},
series = {AutomotiveUI '18}
}

@inproceedings{10.1145/3149869.3149873,
author = {Ronaghi, Zahra and Thomas, Rollin and Deslippe, Jack and Bailey, Stephen and Gursoy, Doga and Kisner, Theodore and Keskitalo, Reijo and Borrill, Julian},
title = {Python in the NERSC Exascale Science Applications Program for Data},
year = {2017},
isbn = {9781450351249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149869.3149873},
doi = {10.1145/3149869.3149873},
abstract = {We describe a new effort at the National Energy Research Scientific Computing Center (NERSC) in performance analysis and optimization of scientific Python applications targeting the Intel Xeon Phi (Knights Landing, KNL) manycore architecture. The Python-centered work outlined here is part of a larger effort called the NERSC Exascale Science Applications Program (NESAP) for Data. NESAP for Data focuses on applications that process and analyze high-volume, high-velocity data sets from experimental or observational science (EOS) facilities supported by the US Department of Energy Office of Science. We present three case study applications from NESAP for Data that use Python. These codes vary in terms of "Python purity" from applications developed in pure Python to ones that use Python mainly as a convenience layer for scientists without expertise in lower level programming languages like C, C++ or Fortran. The science case, requirements, constraints, algorithms, and initial performance optimizations for each code are discussed. Our goal with this paper is to contribute to the larger conversation around the role of Python in high-performance computing today and tomorrow, highlighting areas for future work and emerging best practices.},
booktitle = {Proceedings of the 7th Workshop on Python for High-Performance and Scientific Computing},
articleno = {4},
numpages = {10},
location = {Denver, CO, USA},
series = {PyHPC'17}
}

@inproceedings{10.1145/2611040.2611044,
author = {Kagklis, Vasileios and Verykios, Vassilios S. and Tzimas, Giannis and Tsakalidis, Athanasios K.},
title = {Knowledge Sanitization on the Web},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611044},
doi = {10.1145/2611040.2611044},
abstract = {The widespread use of the Internet caused the rapid growth of data on the Web. But as data on the Web grew larger in numbers, so did the perils due to the applications of data mining. Privacy preserving data mining (PPDM) is the field that investigates techniques to preserve the privacy of data and patterns. Knowledge Hiding, a subfield of PPDM, aims at preserving the sensitive patterns included in the data, which are going to be published. A wide variety of techniques fall under the umbrella of Knowledge Hiding, such as frequent pattern hiding, sequence hiding, classification rule hiding and so on.In this tutorial we create a taxonomy for the frequent itemset hiding techniques. We also provide as examples for each category representative works that appeared recently and fall into each one of these categories. Then, we focus on the detailed overview of a specific category, the so called linear programming-based techniques. Finally, we make a quantitative and qualitative comparison among some of the existing techniques that are classified into this category.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {4},
numpages = {11},
keywords = {Knowledge Hiding, Frequent Itemset Hiding, LP-Based Hiding Approaches, Privacy Preserving Data Mining},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@article{10.1145/3344258,
author = {Jin, Zhuochen and Cui, Shuyuan and Guo, Shunan and Gotz, David and Sun, Jimeng and Cao, Nan},
title = {CarePre: An Intelligent Clinical Decision Assistance System},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3344258},
doi = {10.1145/3344258},
abstract = {Clinical decision support systems are widely used to assist with medical decision making. However, clinical decision support systems typically require manually curated rules and other data that are difficult to maintain and keep up to date. Recent systems leverage advanced deep learning techniques and electronic health records to provide a more timely and precise result. Many of these techniques have been developed with a common focus on predicting upcoming medical events. However, although the prediction results from these approaches are promising, their value is limited by their lack of interpretability. To address this challenge, we introduce CarePre, an intelligent clinical decision assistance system. The system extends a state-of-the-art deep learning model to predict upcoming diagnosis events for a focal patient based on his or her historical medical records. The system includes an interactive framework together with intuitive visualizations designed to support diagnosis, treatment outcome analysis, and the interpretation of the analysis results. We demonstrate the effectiveness and usefulness of the CarePre&nbsp;system by reporting results from a quantities evaluation of the prediction algorithm, two case studies, and interviews with senior physicians and pulmonologists.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {6},
numpages = {20},
keywords = {visual analytics, user interface design, reasoning about belief and knowledge, neural networks, Personal health records}
}

@inproceedings{10.1145/2998181.2998223,
author = {Fiesler, Casey and Dye, Michaelanne and Feuston, Jessica L. and Hiruncharoenvate, Chaya and Hutto, C.J. and Morrison, Shannon and Khanipour Roshan, Parisa and Pavalanathan, Umashanthi and Bruckman, Amy S. and De Choudhury, Munmun and Gilbert, Eric},
title = {What (or Who) Is Public? Privacy Settings and Social Media Content Sharing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998223},
doi = {10.1145/2998181.2998223},
abstract = {When social networking sites give users granular control over their privacy settings, the result is that some content across the site is public and some is not. How might this content--or characteristics of users who post publicly versus to a limited audience--be different? If these differences exist, research studies of public content could potentially be introducing systematic bias. Via Mechanical Turk, we asked 1,815 Facebook users to share recent posts. Using qualitative coding and quantitative measures, we characterize and categorize the nature of the content. Using machine learning techniques, we analyze patterns of choices for privacy settings. Contrary to expectations, we find that content type is not a significant predictor of privacy setting; however, some demographics such as gender and age are predictive. Additionally, with consent of participants, we provide a dataset of nearly 9,000 public and non-public Facebook posts.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {567–580},
numpages = {14},
keywords = {content analysis, dataset, privacy, research methods, machine learning, facebook, social media, mechanical turk, mixed methods, prediction},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/2487259.2487260,
author = {Sadoghi, Mohammad and Jacobsen, Hans-Arno},
title = {Analysis and Optimization for Boolean Expression Indexing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/2487259.2487260},
doi = {10.1145/2487259.2487260},
abstract = {BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75%BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions.},
journal = {ACM Trans. Database Syst.},
month = {jul},
articleno = {8},
numpages = {47},
keywords = {Boolean expressions, complex event processing, data structure, publish/subscribe}
}

@article{10.1145/3531533,
author = {Gram, Dennis and Karapanagiotis, Pantelis and Liebald, Marius and Walz, Uwe},
title = {Design and Implementation of a Historical German Firm-Level Financial Database},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3531533},
doi = {10.1145/3531533},
abstract = {Broad, long-term financial, and economic datasets are scarce resources, particularly in the European context. In this paper, we present an approach for an extensible data model that is adaptable to future changes in technologies and sources. This model may constitute a basis for digitized and structured long-term historical datasets for different jurisdictions and periods. The data model covers the specific peculiarities of historical financial and economic data and is flexible enough to reach out for data of different types (quantitative as well as qualitative) from different historical sources, hence, achieving extensibility. Furthermore, we outline a relational implementation of this approach based on historical German firm and stock market data from 1920 to 1932.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {feb},
keywords = {economic history, cliometrics, financial data, germany, databases}
}

@inbook{10.1145/3310205.3310209,
title = {Data Deduplication},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310209},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3322276.3322354,
author = {Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.},
title = {The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale Community Input},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322354},
doi = {10.1145/3322276.3322354},
abstract = {Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {1171–1181},
numpages = {11},
keywords = {qualitative dataanalysis, community engagement, public inpu, digital civics},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inproceedings{10.5555/2555523.2555543,
author = {Zoumpatianos, Konstantinos and Palpanas, Themis and Mylopoulos, John and Mat\'{e}, Alejandro and Trujillo, Juan},
title = {Monitoring and Diagnosing Indicators for Business Analytics},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Modeling the strategic objectives has been shown to be useful both for understanding a business as well as planning and guiding the overall activities within an enterprise. Business strategy is modeled according to human expertise, setting up the goals as well as the indicators that monitor activities and goals. However, usually indicators provide high-level aggregated views of data, making it difficult to pinpoint problems within specific sub-areas until they have a significant impact into the aggregated value. By the time these problems become evident, they have already hindered the performance of the organization. However, performing a detailed analysis manually can be a daunting task, due to the size of the data space. In order to solve this problem, we propose a user-driven method to analyze the data related to each business indicator by means of data mining. We illustrate our approach with a real world example based on the Europe 2020 framework. Our approach allows us not only to identify latent problems, but also to highlight deviations from anticipated trends that may represent opportunities and exceptional situations, thereby enabling an organization to take advantage of them.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {177–191},
numpages = {15},
location = {Ontario, Canada},
series = {CASCON '13}
}

@article{10.1007/s00779-019-01239-8,
author = {Jin, Hong and Miao, Yunting and Jung, Jae-Rim and Li, Dongjin},
title = {Construction of Information Search Behavior Based on Data Mining},
year = {2022},
issue_date = {Apr 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {2},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01239-8},
doi = {10.1007/s00779-019-01239-8},
abstract = {With the increasing maturity of Web 2.0–related technologies and the expansion of applications, a large number of social network services have emerged at home and abroad. These network platforms have greatly enriched the lives of netizens and become an important platform for studying user information behavior. At the same time, the development of technologies such as global positioning system technology, search engine, and data mining has made users’ data in the mobile social network platform receive extensive attention. This paper constructs a model of information search behavior based on data mining technology in mobile socialized networks and tests it with empirical methods. The results show that the usefulness of information plays a mediating role in the path of information usability impact on information search behavior. Product interaction and human-computer interaction can significantly affect the information search behavior. Trust plays a mediating role in the interaction of virtual community interaction (product interaction, interpersonal interaction, and human-computer interaction) on information search behavior. Using data mining technology to mine user needs, mining relevant data in search and mining information utilization, can improve user information search efficiency and efficiency. What’s more, it can provide basis and support for users and website decision-making.},
journal = {Personal Ubiquitous Comput.},
month = {apr},
pages = {233–245},
numpages = {13},
keywords = {Data mining, Information search behavior, Model construction, Mobile social network}
}

@inproceedings{10.1145/3078861.3078876,
author = {Karafili, Erisa and Lupu, Emil C.},
title = {Enabling Data Sharing in Contextual Environments: Policy Representation and Analysis},
year = {2017},
isbn = {9781450347020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078861.3078876},
doi = {10.1145/3078861.3078876},
abstract = {Internet of Things environments enable us to capture more and more data about the physical environment we live in and about ourselves. The data enable us to optimise resources, personalise services and offer unprecedented insights into our lives. However, to achieve these insights data need to be shared (and sometimes sold) between organisations imposing rights and obligations upon the sharing parties and in accordance with multiple layers of sometimes conflicting legislation at international, national and organisational levels. In this work, we show how such rules can be captured in a formal representation called "Data Sharing Agreements". We introduce the use of abductive reasoning and argumentation based techniques to work with context dependent rules, detect inconsistencies between them, and resolve the inconsistencies by assigning priorities to the rules. We show how through the use of argumentation based techniques use-cases taken from real life application are handled flexibly addressing trade-offs between confidentiality, privacy, availability and safety.},
booktitle = {Proceedings of the 22nd ACM on Symposium on Access Control Models and Technologies},
pages = {231–238},
numpages = {8},
keywords = {data access, abductive reasoning, policy language, cloud, data sharing, usage control, argumentation reasoning},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '17 Abstracts}
}

@inproceedings{10.1145/2806416.2806444,
author = {Wang, Xianzhi and Sheng, Quan Z. and Fang, Xiu Susie and Li, Xue and Xu, Xiaofei and Yao, Lina},
title = {Approximate Truth Discovery via Problem Scale Reduction},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806444},
doi = {10.1145/2806416.2806444},
abstract = {Many real-world applications rely on multiple data sources to provide information on their interested items. Due to the noises and uncertainty in data, given a specific item, the information from different sources may conflict. To make reliable decisions based on these data, it is important to identify the trustworthy information by resolving these conflicts, i.e., the truth discovery problem. Current solutions to this problem detect the veracity of each value jointly with the reliability of each source for each data item. In this way, the efficiency of truth discovery is strictly confined by the problem scale, which in turn limits truth discovery algorithms from being applicable on a large scale. To address this issue, we propose an approximate truth discovery approach, which divides sources and values into groups according to a user-specified approximation criterion. The groups are then used for efficient inter-value influence computation to improve the accuracy. Our approach is applicable to most existing truth discovery algorithms. Experiments on real-world datasets show that our approach improves the efficiency compared to existing algorithms while achieving similar or even better accuracy. The scalability is further demonstrated by experiments on large synthetic datasets.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {503–512},
numpages = {10},
keywords = {truth discovery, recursive method, problem scale reduction, consistency assurance},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.1145/3308560.3316485,
author = {Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang, Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni, Martin and Sarraute, Carlos},
title = {Detecting Areas of Potential High Prevalence of Chagas in Argentina},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316485},
doi = {10.1145/3308560.3316485},
abstract = {A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {262–271},
numpages = {10},
keywords = {epidemics, call detail records, migrations, health vulnerability, social network analysis, neglected tropical diseases, Chagas disease},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{10.1145/3442200,
author = {Barlaug, Nils and Gulla, Jon Atle},
title = {Neural Networks for Entity Matching: A Survey},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3442200},
doi = {10.1145/3442200},
abstract = {Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {52},
numpages = {37},
keywords = {entity resolution, Deep learning, entity matching, record linkage, data matching}
}

@article{10.1145/3476058,
author = {Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily},
title = {Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476058},
doi = {10.1145/3476058},
abstract = {Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {317},
numpages = {37},
keywords = {machine learning, datasets, computer vision, work practice, values in design}
}

@article{10.1145/3185511,
author = {Dong, Roy and Ratliff, Lillian J. and C\'{a}rdenas, Alvaro A. and Ohlsson, Henrik and Sastry, S. Shankar},
title = {Quantifying the Utility--Privacy Tradeoff in the Internet of Things},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3185511},
doi = {10.1145/3185511},
abstract = {The Internet of Things (IoT) promises many advantages in the control and monitoring of physical systems from both efficacy and efficiency perspectives. However, in the wrong hands, the data might pose a privacy threat. In this article, we consider the tradeoff between the operational value of data collected in the IoT and the privacy of consumers. We present a general framework for quantifying this tradeoff in the IoT, and focus on a smart grid application for a proof of concept. In particular, we analyze the tradeoff between smart grid operations and how often data are collected by considering a realistic direct-load control example using thermostatically controlled loads, and we give simulation results to show how its performance degrades as the sampling frequency decreases. Additionally, we introduce a new privacy metric, which we call inferential privacy. This privacy metric assumes a strong adversary model and provides an upper bound on the adversary’s ability to infer a private parameter, independent of the algorithm he uses. Combining these two results allows us to directly consider the tradeoff between better operational performance and consumer privacy.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {may},
articleno = {8},
numpages = {28},
keywords = {Privacy, smart grid}
}

@article{10.1145/3106774,
author = {Pellungrini, Roberto and Pappalardo, Luca and Pratesi, Francesca and Monreale, Anna},
title = {A Data Mining Approach to Assess Privacy Risk in Human Mobility Data},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106774},
doi = {10.1145/3106774},
abstract = {Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {31},
numpages = {27},
keywords = {Human mobility, privacy, data mining}
}

@inproceedings{10.1145/3490099.3511144,
author = {Salminen, Joni and Jung, Soon-Gyo and Jansen, Bernard},
title = {Developing Persona Analytics Towards Persona Science},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511144},
doi = {10.1145/3490099.3511144},
abstract = {Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users’ mouse and gaze behavior to measure users’ interaction with algorithmically generated personas and use of system features for an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain scientifically valid evidence. Conducting a study with 144 participants, we demonstrate how PA can be deployed for remote user studies during exceptional times when physical user studies are difficult, if not impossible.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {323–344},
numpages = {22},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3472163.3472267,
author = {Holubova, Irena and Contos, Pavel and Svoboda, Martin},
title = {Multi-Model Data Modeling and Representation: State of the Art and Research Challenges},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472267},
doi = {10.1145/3472163.3472267},
abstract = { Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.},
booktitle = {25th International Database Engineering &amp; Applications Symposium},
pages = {242–251},
numpages = {10},
keywords = {Conceptual modeling, Inter-model relationships, Logical models, Category theory, Multi-model data},
location = {Montreal, QC, Canada},
series = {IDEAS 2021}
}

@article{10.1007/s00778-016-0430-9,
author = {K\"{o}hler, Henning and Leck, Uwe and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain Keys for SQL},
year = {2016},
issue_date = {August    2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-016-0430-9},
doi = {10.1007/s00778-016-0430-9},
abstract = {Driven by the dominance of the relational model and the requirements of modern applications, we revisit the fundamental notion of a key in relational databases with NULL. In SQL, primary key columns are NOT NULL, and UNIQUE constraints guarantee uniqueness only for tuples without NULL. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that originate from an SQL table, respectively. Possible keys coincide with UNIQUE, thus providing a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns and can uniquely identify entities whenever feasible, while primary keys may not. In addition to basic characterization, axiomatization, discovery, and extremal combinatorics problems, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs occur in real-world data, and related computational problems can be solved efficiently. Certain keys are therefore semantically well founded and able to meet Codd's entity integrity rule while handling high volumes of incomplete data from different formats.},
journal = {The VLDB Journal},
month = {aug},
pages = {571–596},
numpages = {26},
keywords = {Armstrong database, Null marker, Data profiling, Implication problem, Discovery, SQL, Index, Axiomatization, Key, Extremal combinatorics}
}

@inproceedings{10.1145/3236024.3236060,
author = {Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei},
title = {Predicting Node Failure in Cloud Service Systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236060},
doi = {10.1145/3236024.3236060},
abstract = {In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {480–490},
numpages = {11},
keywords = {node failure, maintenance, cloud service systems, Failure prediction, service availability},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3219819.3219840,
author = {Ruhrl\"{a}nder, Rui Paulo and Boissier, Martin and Uflacker, Matthias},
title = {Improving Box Office Result Predictions for Movies Using Consumer-Centric Models},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219840},
doi = {10.1145/3219819.3219840},
abstract = {Recent progress in machine learning and related fields like recommender systems open up new possibilities for data-driven approaches. One example is the prediction of a movie's box office revenue, which is highly relevant for optimizing production and marketing. We use individual recommendations and user-based forecast models in a system that forecasts revenue and additionally provides actionable insights for industry professionals. In contrast to most existing models that completely neglect user preferences, our approach allows us to model the most important source for movie success: moviegoer taste and behavior. We divide the problem into three distinct stages: (i) we use matrix factorization recommenders to model each user's taste, (ii) we then predict the individual consumption behavior, and (iii) eventually aggregate users to predict the box office result. We compare our approach to the current industry standard and show that the inclusion of user rating data reduces the error by a factor of 2x and outperforms recently published research.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {655–664},
numpages = {10},
keywords = {logistic regression, user ratings, box office predictions, motion picture industry, gradient-boosted trees, recommender systems},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3269206.3271747,
author = {Gupchup, Jayant and Hosseinkashi, Yasaman and Dmitriev, Pavel and Schneider, Daniel and Cutler, Ross and Jefremov, Andrei and Ellis, Martin},
title = {Trustworthy Experimentation Under Telemetry Loss},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271747},
doi = {10.1145/3269206.3271747},
abstract = {Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {387–396},
numpages = {10},
keywords = {telemetry loss, client experimentation, online controlled experiments, ab testing, experimentation trustworthiness, data loss},
location = {Torino, Italy},
series = {CIKM '18}
}

@inbook{10.1145/3404835.3462918,
author = {Zhou, Yujia and Dou, Zhicheng and Wei, Bingzheng and Xie, Ruobing and Wen, Ji-Rong},
title = {Group Based Personalized Search by Integrating Search Behaviour and Friend Network},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462918},
abstract = {The key to personalized search is to build the user profile based on historical behaviour. To deal with the users who lack historical data, group based personalized models were proposed to incorporate the profiles of similar users when re-ranking the results. However, similar users are mostly found based on simple lexical or topical similarity in search behaviours. In this paper, we propose a neural network enhanced method to highlight similar users in semantic space. Furthermore, we argue that the behaviour-based similar users are still insufficient to understand a new query when user's historical activities are limited. To tackle this issue, we introduce the friend network into personalized search to determine the closeness between users in another way. Since the friendship is often formed based on similar background or interest, there are plenty of personalized signals hidden in the friend network naturally. Specifically, we propose a friend network enhanced personalized search model, which groups the user into multiple friend circles based on search behaviours and friend relations respectively. These two types of friend circles are complementary to construct a more comprehensive group profile for refining the personalization. Experimental results show the significant improvement of our model over existing personalized search models.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {92–101},
numpages = {10}
}

@inproceedings{10.1145/3219819.3219978,
author = {Yang, Tong and Gong, Junzhi and Zhang, Haowei and Zou, Lei and Shi, Lei and Li, Xiaoming},
title = {HeavyGuardian: Separate and Guard Hot Items in Data Streams},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219978},
doi = {10.1145/3219819.3219978},
abstract = {Data stream processing is a fundamental issue in many fields, such as data mining, databases, network traffic measurement. There are five typical tasks in data stream processing: frequency estimation, heavy hitter detection, heavy change detection, frequency distribution estimation, and entropy estimation. Different algorithms are proposed for different tasks, but they seldom achieve high accuracy and high speed at the same time. To address this issue, we propose a novel data structure named HeavyGuardian. The key idea is to intelligently separate and guard the information of hot items while approximately record the frequencies of cold items. We deploy HeavyGuardian on the above five typical tasks. Extensive experimental results show that HeavyGuardian achieves both much higher accuracy and higher speed than the state-of-the-art solutions for each of the five typical tasks. The source codes of HeavyGuardian and other related algorithms are available at GitHub.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2584–2593},
numpages = {10},
keywords = {data stream processing, data sturcture, probabilistic and approximate data},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1109/TNET.2015.2421897,
author = {Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian},
title = {Incentive Mechanisms for Crowdsensing: Crowdsourcing with Smartphones},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2015.2421897},
doi = {10.1109/TNET.2015.2421897},
abstract = {Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1732–1744},
numpages = {13},
keywords = {crowdsourcing, Stackelberg game, crowdsensing, incentive mechanism}
}

@inproceedings{10.1145/2750858.2806897,
author = {Saleheen, Nazir and Ali, Amin Ahsan and Hossain, Syed Monowar and Sarker, Hillol and Chatterjee, Soujanya and Marlin, Benjamin and Ertin, Emre and al'Absi, Mustafa and Kumar, Santosh},
title = {PuffMarker: A Multi-Sensor Approach for Pinpointing the Timing of First Lapse in Smoking Cessation},
year = {2015},
isbn = {9781450335744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2750858.2806897},
doi = {10.1145/2750858.2806897},
abstract = {Recent researches have demonstrated the feasibility of detecting smoking from wearable sensors, but their performance on real-life smoking lapse detection is unknown. In this paper, we propose a new model and evaluate its performance on 61 newly abstinent smokers for detecting a first lapse. We use two wearable sensors --- breathing pattern from respiration and arm movements from 6-axis inertial sensors worn on wrists. In 10-fold cross-validation on 40 hours of training data from 6 daily smokers, our model achieves a recall rate of 96.9%, for a false positive rate of 1.1%. When our model is applied to 3 days of post-quit data from 32 lapsers, it correctly pinpoints the timing of first lapse in 28 participants. Only 2 false episodes are detected on 20 abstinent days of these participants. When tested on 84 abstinent days from 28 abstainers, the false episode per day is limited to 1/6.},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {999–1010},
numpages = {12},
keywords = {smoking detection, mobile health (mHealth), smoking cessation, wearable sensors, smartwatch},
location = {Osaka, Japan},
series = {UbiComp '15}
}

@inproceedings{10.1145/3213846.3213866,
author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
title = {An Empirical Study on TensorFlow Program Bugs},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213866},
doi = {10.1145/3213846.3213866},
abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {129–140},
numpages = {12},
keywords = {TensorFlow Program Bug, Deep Learning, Empirical Study},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@article{10.14778/2535568.2448938,
author = {Dong, Xin Luna and Saha, Barna and Srivastava, Divesh},
title = {Less is More: Selecting Sources Wisely for Integration},
year = {2012},
issue_date = {December 2012},
publisher = {VLDB Endowment},
volume = {6},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/2535568.2448938},
doi = {10.14778/2535568.2448938},
abstract = {We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, "the more the better" does not always hold for data integration and often "less is more".In this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {37–48},
numpages = {12}
}

@article{10.1145/3468854,
author = {Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang},
title = {SPI: Automated Identification of Security Patches via Commits},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3468854},
doi = {10.1145/3468854},
abstract = {Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {13},
numpages = {27},
keywords = {software security, Machine learning, deep learning}
}

@article{10.1145/3154815,
author = {Li, Chao and Xue, Yushu and Wang, Jing and Zhang, Weigong and Li, Tao},
title = {Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154815},
doi = {10.1145/3154815},
abstract = {While cloud computing has brought paradigm shifts to computing services, researchers and developers have also found some problems inherent to its nature such as bandwidth bottleneck, communication overhead, and location blindness. The concept of fog/edge computing is therefore coined to extend the services from the core in cloud data centers to the edge of the network. In recent years, many systems are proposed to better serve ubiquitous smart devices closer to the user. This article provides a complete and up-to-date review of edge-oriented computing systems by encapsulating relevant proposals on their architecture features, management approaches, and design objectives.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {39},
numpages = {34},
keywords = {resource management, ubiquitous data processing, edge computing, fog computing, Distributed cloud, architecture design}
}

@inbook{10.1145/3447404.3447408,
author = {Chatzigiannakis, Ioannis and Tselios, Christos},
title = {Internet of Everything},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447408},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {21–56},
numpages = {36}
}

@inproceedings{10.1145/3318464.3386143,
author = {Yan, Yan and Meyles, Stephen and Haghighi, Aria and Suciu, Dan},
title = {Entity Matching in the Wild: A Consistent and Versatile Framework to Unify Data in Industrial Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386143},
doi = {10.1145/3318464.3386143},
abstract = {Entity matching -- the task of clustering duplicated database records to underlying entities -- has become an increasingly critical component in modern data integration management. Amperity provides a platform for businesses to manage customer data that utilizes a machine-learning approach to entity matching, resolving billions of customer records on a daily basis. We face several challenges in deploying entity matching to industrial applications at scale, and they are less prominent in the literature. These challenges include: (1) Providing not just a single entity clustering, but supporting clusterings at multiple confidence levels to enable downstream applications with varying precision/recall trade-off needs. (2) Many customer record attributes may be systematically missing from different sources of data, creating many pairs of records in a cluster that appear to not match due to incomplete, rather than conflicting information. Allowing these records to connect transitively without introducing conflicts is invaluable to businesses because they can acquire a more comprehensive profile of their customers without incorrect entity merges. (3) How to cluster records over time and assign persistent cluster IDs that can be used for downstream use cases such as A/B tests or predictive model training; this is made more challenging by the fact that we receive new customer data every day and clusters naturally evolving over time still require persistent IDs that refer to the same entity. In this work, we describe Amperity's entity matching framework, Fusion, and how its design provides solutions to these challenges. In particular, we describe our pairwise matching model based on ordinal regression that permits a well-defined way to produce entity clusterings at different confidence levels, a novel clustering algorithm that separates conflicting record pairs in clusters while allowing for pairs that may appear dissimilar due to missing data, and a persistent ID generation algorithm which balances stability of the identifier with ever-evolving entities.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2287–2301},
numpages = {15},
keywords = {conflict resolution in clustering, cluster id assignment, multi-level entity matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3372274,
author = {Horv\'{a}th, G\'{a}bor and Kov\'{a}cs, Edith and Molontay, Roland and Nov\'{a}czki, Szabolcs},
title = {Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372274},
doi = {10.1145/3372274},
abstract = {The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly.The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets.In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {26},
numpages = {26},
keywords = {Anomaly scoring, copula fitting, unsupervised learning}
}

@inproceedings{10.1145/3306446.3340829,
author = {TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga},
title = {Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340829},
doi = {10.1145/3306446.3340829},
abstract = {Much existing knowledge about global consumption of peer-produced information goods is supported by data on Wikipedia page view counts and surveys. In 2017, the Wikimedia Foundation began measuring the time readers spend on a given page view (dwell time), enabling a more detailed understanding of such reading patterns. In this paper, we validate and model this new data source and, building on existing findings, use regression analysis to test hypotheses about how patterns in reading time vary between global contexts. Consistent with prior findings from self-report data, our complementary analysis of behavioral data provides evidence that Global South readers are more likely to use Wikipedia to gain in-depth understanding of a topic. We find that Global South readers spend more time per page view and that this difference is amplified on desktop devices, which are thought to be better suited for in-depth information seeking tasks.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {14},
numpages = {14},
keywords = {readership, digital divides, peer production, web analytics, dwell time, Wikipedia, quantitative methods},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/3014087.3014110,
author = {Pereira, Gabriela Viale and Testa, Maur\'{\i}cio Gregianin and Macadar, Marie Anne and Parycek, Peter and de Azambuja, Luiza Schuch},
title = {Building Understanding of Municipal Operations Centers as Smart City' Initiatives: Insights from a Cross-Case Analysis},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014110},
doi = {10.1145/3014087.3014110},
abstract = {Cities around the world have been facing complex challenges from the growing urbanization. The increase of urban problems is a consequence of this phenomenon, added to the lack of policies focusing in citizens' well-being and safety. Municipal operations centers have played an important role in response of social events and natural disasters as a way to address the urgency and dynamism of urban problems. This research aims at analyzing the main dimensions and factors for implementing municipal operations centers as smart city initiatives. In order to explore this phenomenon it was conducted an exploratory study, based on multiple case studies. The empirical setting of this research is determined by municipal operations centers in Rio de Janeiro, Porto Alegre and Belo Horizonte. The research findings evidenced that the implementation of the centers comprises technological, organizational and managerial factors, in addition to political and institutional factors. Increasing smart cities governance is the main result from the initiatives.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {19–30},
numpages = {12},
keywords = {multiple cases study, municipal operations center, smart cities governance, smart cities},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inbook{10.1145/3447404.3447422,
author = {Liu, Can and Lindqvist, Janne},
title = {Secure Gestures—Case Study 4},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447422},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {323–338},
numpages = {16}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized On-Demand Data Streaming from Sensor Nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {oversampling, real-time analysis, sensor sharing, sensor data, user-defined sampling, adaptive sampling, on-demand streaming},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3324884.3416568,
author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Pu, Yanjun and Liu, Xudong},
title = {Learning to Handle Exceptions},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416568},
doi = {10.1145/3324884.3416568},
abstract = {Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {29–41},
numpages = {13},
keywords = {code generation, deep learning, exception handling, neural network},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.5555/3122009.3242050,
author = {Vaughan, Jennifer Wortman},
title = {Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {7026–7071},
numpages = {46},
keywords = {hybrid intelligence, behavioral experiments, model evaluation, data generation, mechanical turk, crowdsourcing, incentives}
}

@article{10.1145/3167970,
author = {Chung, Yeounoh and Mortensen, Michael Lind and Binnig, Carsten and Kraska, Tim},
title = {Estimating the Impact of Unknown Unknowns on Aggregate Query Results},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/3167970},
doi = {10.1145/3167970},
abstract = {It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results?In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.},
journal = {ACM Trans. Database Syst.},
month = {mar},
articleno = {3},
numpages = {37},
keywords = {species estimation, unknown unknowns, Aggregate query processing, crowdsourcing}
}

@inproceedings{10.1145/3034786.3056114,
author = {Fan, Wenfei and Lu, Ping},
title = {Dependencies for Graphs},
year = {2017},
isbn = {9781450341981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034786.3056114},
doi = {10.1145/3034786.3056114},
abstract = {This paper proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities in a graph.We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound and complete axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication and validation problems for the extensions.},
booktitle = {Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {403–416},
numpages = {14},
keywords = {validation, conditional functional dependencies, satisfiability, disjunction, graph dependencies, tgds, built-in predicates, keys, egds, axiom system, implication},
location = {Chicago, Illinois, USA},
series = {PODS '17}
}

@article{10.1145/3487893,
author = {Xie, Yiqun and Shekhar, Shashi and Li, Yan},
title = {Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots: A Survey},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3487893},
doi = {10.1145/3487893},
abstract = {Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, and so on. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey, we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {36},
numpages = {38},
keywords = {Hotspot, scan statistics, clustering, mapping, statistical rigor}
}

@article{10.1145/3501295,
author = {Zafar, Farkhanda and Khattak, Hasan Ali and Aloqaily, Moayad and Hussain, Rasheed},
title = {Carpooling in Connected and Autonomous Vehicles: Current Solutions and Future Directions},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3501295},
doi = {10.1145/3501295},
abstract = {Owing to the advancements in communication and computation technologies, the dream of commercialized connected and autonomous cars is becoming a reality. However, among other challenges such as environmental pollution, cost, maintenance, security, and privacy, the ownership of vehicles (especially for Autonomous Vehicles (AV)) is the major obstacle in the realization of this technology at the commercial level. Furthermore, the business model of pay-as-you-go type services further attracts the consumer because there is no need for upfront investment. In this vein, the idea of car-sharing (aka carpooling) is getting ground due to, at least in part, its simplicity, cost-effectiveness, and affordable choice of transportation. Carpooling systems are still in their infancy and face challenges such as scheduling, matching passengers interests, business model, security, privacy, and communication. To date, a plethora of research work has already been done covering different aspects of carpooling services (ranging from applications to communication and technologies); however, there is still a lack of a holistic, comprehensive survey that can be a one-stop-shop for the researchers in this area to, i) find all the relevant information, and ii) identify the future research directions. To fill these research challenges, this paper provides a comprehensive survey on carpooling in autonomous and connected vehicles and covers architecture, components, and solutions, including scheduling, matching, mobility, pricing models of carpooling. We also discuss the current challenges in carpooling and identify future research directions. This survey is aimed to spur further discussion among the research community for the effective realization of carpooling.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {nov},
keywords = {Vehicular Networks, Intelligent Transportation Systems, Connected Autonomous Vehicles, Ride-sharing, Carpooling}
}

@article{10.1145/3177848,
author = {Brandt, Tobias and Grawunder, Marco},
title = {GeoStreams: A Survey},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3177848},
doi = {10.1145/3177848},
abstract = {Positional data from small and mobile Global Positioning Systems has become ubiquitous and allows for many new applications such as road traffic or vessel monitoring as well as location-based services. To make these applications possible, for which information on location is more important than ever, streaming spatial data needs to be managed, mined, and used intelligently. This article provides an overview of previous work in this evolving research field and discusses different applications as well as common problems and solutions. The conclusion indicates promising directions for future research.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {44},
numpages = {37},
keywords = {GeoStreams, data stream management systems, data stream engines}
}

@article{10.1145/3329124,
author = {McDaniel, Melinda and Storey, Veda C.},
title = {Evaluating Domain Ontologies: Clarification, Classification, and Challenges},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329124},
doi = {10.1145/3329124},
abstract = {The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {70},
numpages = {44},
keywords = {ontology development, ontology application, applied ontology, Ontology, metrics, evaluation, task-ontology fit, domain ontology, assessment}
}

@article{10.1145/3385031,
author = {Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao},
title = {Catching Numeric Inconsistencies in Graphs},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3385031},
doi = {10.1145/3385031},
abstract = {Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are Σp2-complete, Πp2-complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates ΔG to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in ΔG instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.},
journal = {ACM Trans. Database Syst.},
month = {jun},
articleno = {9},
numpages = {47},
keywords = {graph dependencies, incremental validation, Numeric errors}
}

@inproceedings{10.1145/3299869.3324956,
author = {Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
title = {Raha: A Configuration-Free Error Detection System},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3324956},
doi = {10.1145/3299869.3324956},
abstract = {Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {865–882},
numpages = {18},
keywords = {label propagation, clustering, semi-supervised learning, data cleaning, historical data, classification, machine learning, error detection},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.1109/TASLP.2021.3078883,
author = {Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong},
title = {Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078883},
doi = {10.1109/TASLP.2021.3078883},
abstract = {Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on &lt;italic&gt;TF masking&lt;/italic&gt;, &lt;italic&gt;Filter&amp;Sum&lt;/italic&gt; and &lt;italic&gt;mask-based MVDR&lt;/italic&gt; neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {2067–2082},
numpages = {16}
}


Skip to content
Search or jump to…
Pull requests
Issues
Marketplace
Explore
 
@miguelrezende7 
Zenzow
/
MBA_IMPACTA
Public
Code
Issues
Pull requests
Actions
Projects
Wiki
Security
Insights
MBA_IMPACTA/PYTHON_DA/arquivos_bibtex/ieee.bib
@Zenzow
Zenzow arrumando estrutura de pastas e enviando arquivos bib
Latest commit 0565f70 yesterday
 History
 1 contributor
2400 lines (2400 sloc)  345 KB
   
@INPROCEEDINGS{8332632,
author = {Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun},
booktitle = {2017 14th Web Information Systems and Applications Conference (WISA)}, title = {A Big Data Framework for Electric Power Data Quality Assessment},
year = {2017},
volume = {},
number = {},
pages = {289-292},
abstract = {Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.},
keywords = {Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework},
doi = {10.1109/WISA.2017.29},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8029366,
author = {Taleb, Ikbal and Serhani, Mohamed Adel},
booktitle = {2017 IEEE International Congress on Big Data (BigData Congress)}, title = {Big Data Pre-Processing: Closing the Data Quality Enforcement Loop},
year = {2017},
volume = {},
number = {},
pages = {498-501},
abstract = {In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.},
keywords = {Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing},
doi = {10.1109/BigDataCongress.2017.73},
ISSN = {},
month = {June},}
@INPROCEEDINGS{9245455,
author = {Loetpipatwanich, Sakda and Vichitthamaros, Preecha},
booktitle = {2020 1st International Conference on Big Data Analytics and Practices (IBDAP)}, title = {Sakdas: A Python Package for Data Profiling and Data Quality Auditing},
year = {2020},
volume = {},
number = {},
pages = {1-4},
abstract = {Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.},
keywords = {Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline},
doi = {10.1109/IBDAP50342.2020.9245455},
ISSN = {},
month = {Sep.},}
@INPROCEEDINGS{8386521,
author = {Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai},
booktitle = {2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, title = {Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory},
year = {2018},
volume = {},
number = {},
pages = {248-252},
abstract = {Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.},
keywords = {Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment},
doi = {10.1109/ICCCBDA.2018.8386521},
ISSN = {},
month = {April},}
@INPROCEEDINGS{8605945,
author = {Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle = {2018 International Conference on Innovations in Information Technology (IIT)}, title = {Big Data Quality Assessment Model for Unstructured Data},
year = {2018},
volume = {},
number = {},
pages = {69-74},
abstract = {Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.},
keywords = {Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data},
doi = {10.1109/INNOVATIONS.2018.8605945},
ISSN = {2325-5498},
month = {Nov},}
@INPROCEEDINGS{8078796,
author = {HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang},
booktitle = {2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)}, title = {Some key problems of data management in army data engineering based on big data},
year = {2017},
volume = {},
number = {},
pages = {149-152},
abstract = {This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.},
keywords = {Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality},
doi = {10.1109/ICBDA.2017.8078796},
ISSN = {},
month = {March},}
@INPROCEEDINGS{7364064,
author = {Becker, David and King, Trish Dunn and McMullen, Bill},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Big data, big data quality problem},
year = {2015},
volume = {},
number = {},
pages = {2644-2653},
abstract = {A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.},
keywords = {Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale},
doi = {10.1109/BigData.2015.7364064},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9260067,
author = {Wong, Ka Yee. and Wong, Raymond K.},
booktitle = {2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)}, title = {Big Data Quality Prediction on Banking Applications: Extended Abstract},
year = {2020},
volume = {},
number = {},
pages = {791-792},
abstract = {Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.},
keywords = {Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning},
doi = {10.1109/DSAA49011.2020.00119},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{8686099,
author = {Abdallah, Mohammad},
booktitle = {2019 International Conference on Big Data and Computational Intelligence (ICBDCI)}, title = {Big Data Quality Challenges},
year = {2019},
volume = {},
number = {},
pages = {1-3},
abstract = {Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.},
keywords = {Big Data;Quality Measurement;Quality Model;Quality Assurance},
doi = {10.1109/ICBDCI.2019.8686099},
ISSN = {},
month = {Feb},}
@INPROCEEDINGS{7816918,
author = {Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
booktitle = {2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, title = {Big Data Quality: A Quality Dimensions Evaluation},
year = {2016},
volume = {},
number = {},
pages = {759-765},
abstract = {Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.},
keywords = {Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling},
doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122},
ISSN = {},
month = {July},}
@INPROCEEDINGS{8862267,
author = {Juneja, Ashish and Das, Nripendra Narayan},
booktitle = {2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)}, title = {Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application},
year = {2019},
volume = {},
number = {},
pages = {559-563},
abstract = {Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.},
keywords = {Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing},
doi = {10.1109/COMITCon.2019.8862267},
ISSN = {},
month = {Feb},}
@INPROCEEDINGS{7364065,
author = {Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Data quality issues in big data},
year = {2015},
volume = {},
number = {},
pages = {2654-2660},
abstract = {Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.},
keywords = {Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality},
doi = {10.1109/BigData.2015.7364065},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{7840595,
author = {Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)}, title = {Antecedents of big data quality: An empirical examination in financial service organizations},
year = {2016},
volume = {},
number = {},
pages = {116-121},
abstract = {Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.},
keywords = {Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance},
doi = {10.1109/BigData.2016.7840595},
ISSN = {},
month = {Dec},}
@ARTICLE{8935096,
author = {Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal = {Big Data Mining and Analytics}, title = {Mining conditional functional dependency rules on big data},
year = {2020},
volume = {3},
number = {1},
pages = {68-84},
abstract = {Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords = {Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi = {10.26599/BDMA.2019.9020019},
ISSN = {2096-0654},
month = {March},}
@INPROCEEDINGS{8397554,
author = {Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin},
booktitle = {2017 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computed, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title = {Data quality in big data processing: Issues, solutions and open problems},
year = {2017},
volume = {},
number = {},
pages = {1-7},
abstract = {With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.},
keywords = {Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system},
doi = {10.1109/UIC-ATC.2017.8397554},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{9006294,
author = {Arruda, Darlan and Madhavji, Nazim H.},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)}, title = {QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications},
year = {2019},
volume = {},
number = {},
pages = {5977-5979},
abstract = {The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.},
keywords = {Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool},
doi = {10.1109/BigData47090.2019.9006294},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8465129,
author = {Juddoo, Suraj and George, Carlisle},
booktitle = {2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)}, title = {Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis},
year = {2018},
volume = {},
number = {},
pages = {1-6},
abstract = {Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.},
keywords = {Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis},
doi = {10.1109/ICABCD.2018.8465129},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{7364060,
author = {Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Evaluation of data quality of multisite electronic health record data for secondary analysis},
year = {2015},
volume = {},
number = {},
pages = {2612-2620},
abstract = {Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.},
keywords = {Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics},
doi = {10.1109/BigData.2015.7364060},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9006358,
author = {Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)}, title = {Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams},
year = {2019},
volume = {},
number = {},
pages = {3260-3266},
abstract = {Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.},
keywords = {Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science},
doi = {10.1109/BigData47090.2019.9006358},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9323615,
author = {Han, Weiguo and Jochum, Matthew},
booktitle = {IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium}, title = {A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System},
year = {2020},
volume = {},
number = {},
pages = {3101-3103},
abstract = {In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.},
keywords = {Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest},
doi = {10.1109/IGARSS39084.2020.9323615},
ISSN = {2153-7003},
month = {Sep.},}
@INPROCEEDINGS{9073586,
author = {Khaleel, Majida Yaseen and Hamad, Murtadha M.},
booktitle = {2019 12th International Conference on Developments in eSystems Engineering (DeSE)}, title = {Data Quality Management for Big Data Applications},
year = {2019},
volume = {},
number = {},
pages = {357-362},
abstract = {Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.},
keywords = {Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.},
doi = {10.1109/DeSE.2019.00072},
ISSN = {2161-1351},
month = {Oct},}
@INPROCEEDINGS{9006187,
author = {Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)}, title = {DQA: Scalable, Automated and Interactive Data Quality Advisor},
year = {2019},
volume = {},
number = {},
pages = {2913-2922},
abstract = {Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.},
keywords = {Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science},
doi = {10.1109/BigData47090.2019.9006187},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7374131,
author = {Juddoo, Suraj},
booktitle = {2015 International Conference on Computing, Communication and Security (ICCCS)}, title = {Overview of data quality challenges in the context of Big Data},
year = {2015},
volume = {},
number = {},
pages = {1-9},
abstract = {Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.},
keywords = {Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics},
doi = {10.1109/CCCS.2015.7374131},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7347061,
author = {Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce},
booktitle = {2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)}, title = {Computing data quality indicators on Big Data streams using a CEP},
year = {2015},
volume = {},
number = {},
pages = {1-5},
abstract = {Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.},
keywords = {Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids},
doi = {10.1109/IWCIM.2015.7347061},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9616552,
author = {Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da},
booktitle = {2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)}, title = {Research on smart grid big data amp;#x2019;s curve mean clustering algorithm for edge-cloud collaborative application},
year = {2021},
volume = {},
number = {},
pages = {395-398},
abstract = {As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.},
keywords = {Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm},
doi = {10.1109/ICWCSG53609.2021.00085},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8942297,
author = {Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz},
booktitle = {2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)}, title = {Towards a multi-agents model for errors detection and correction in big data flows},
year = {2019},
volume = {},
number = {},
pages = {1-5},
abstract = {The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.},
keywords = {Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors},
doi = {10.1109/ICDS47004.2019.8942297},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{8258267,
author = {Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {My (fair) big data},
year = {2017},
volume = {},
number = {},
pages = {2974-2979},
abstract = {Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.},
keywords = {Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality},
doi = {10.1109/BigData.2017.8258267},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8596389,
author = {Ezzine, Imane and Benhlima, Laila},
booktitle = {2018 IEEE 5th International Congress on Information Science and Technology (CiSt)}, title = {A Study of Handling Missing Data Methods for Big Data},
year = {2018},
volume = {},
number = {},
pages = {498-501},
abstract = {Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.},
keywords = {Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning},
doi = {10.1109/CIST.2018.8596389},
ISSN = {2327-1884},
month = {Oct},}
@INPROCEEDINGS{7207219,
author = {Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel},
booktitle = {2015 IEEE International Congress on Big Data}, title = {Big Data Pre-processing: A Quality Framework},
year = {2015},
volume = {},
number = {},
pages = {191-198},
abstract = {With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.},
keywords = {Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing},
doi = {10.1109/BigDataCongress.2015.35},
ISSN = {2379-7703},
month = {June},}
@INPROCEEDINGS{7384166,
author = {Chenran, Xiong and Youde, Wu},
booktitle = {2015 International Conference on Intelligent Transportation, Big Data and Smart City}, title = {The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology},
year = {2015},
volume = {},
number = {},
pages = {869-872},
abstract = {This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.},
keywords = {Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment},
doi = {10.1109/ICITBS.2015.220},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9314391,
author = {Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif},
booktitle = {2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)}, title = {Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security},
year = {2020},
volume = {},
number = {},
pages = {1-8},
abstract = {Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.},
keywords = {Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration},
doi = {10.1109/ICECOCS50124.2020.9314391},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7840935,
author = {Haug, Frank S.},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)}, title = {Bad big data science},
year = {2016},
volume = {},
number = {},
pages = {2863-2871},
abstract = {As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.},
keywords = {Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata},
doi = {10.1109/BigData.2016.7840935},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8457745,
author = {Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle = {2018 IEEE International Congress on Big Data (BigData Congress)}, title = {Big Data Quality: A Survey},
year = {2018},
volume = {},
number = {},
pages = {166-173},
abstract = {With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.},
keywords = {Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data},
doi = {10.1109/BigDataCongress.2018.00029},
ISSN = {},
month = {July},}
@INPROCEEDINGS{6755349,
author = {Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo},
booktitle = {2013 IEEE 16th International Conference on Computational Science and Engineering}, title = {Information Governance, Big Data and Data Quality},
year = {2013},
volume = {},
number = {},
pages = {1142-1143},
abstract = {The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.},
keywords = {Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality},
doi = {10.1109/CSE.2013.168},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8276745,
author = {Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib},
booktitle = {2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)}, title = {Towards a Data Quality Framework for Heterogeneous Data},
year = {2017},
volume = {},
number = {},
pages = {155-162},
abstract = {Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.},
keywords = {Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment},
doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28},
ISSN = {},
month = {June},}
@ARTICLE{8667300,
author = {Lee, Doyoung},
journal = {IEEE Access}, title = {Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year = {2019},
volume = {7},
number = {},
pages = {36294-36299},
abstract = {In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords = {Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi = {10.1109/ACCESS.2019.2904286},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{9297009,
author = {Juddoo, Suraj and George, Carlisle},
booktitle = {2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)}, title = {A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry},
year = {2020},
volume = {},
number = {},
pages = {58-66},
abstract = {Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.},
keywords = {Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning},
doi = {10.1109/ELECOM49001.2020.9297009},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8416208,
author = {Blanquer, Ignacio and Meira, Wagner},
booktitle = {2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, title = {EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform},
year = {2018},
volume = {},
number = {},
pages = {47-48},
abstract = {This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.},
keywords = {Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics},
doi = {10.1109/DSN-W.2018.00023},
ISSN = {2325-6664},
month = {June},}
@INPROCEEDINGS{9529590,
author = {Bhardwaj, Dave and Ormandjieva, Olga},
booktitle = {2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title = {Toward a Novel Measurement Framework for Big Data (MEGA)},
year = {2021},
volume = {},
number = {},
pages = {1579-1586},
abstract = {Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.},
keywords = {Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines},
doi = {10.1109/COMPSAC51774.2021.00235},
ISSN = {0730-3157},
month = {July},}
@INPROCEEDINGS{9134140,
author = {Gan, Wenting},
booktitle = {2020 International Conference on E-Commerce and Internet Technology (ECIT)}, title = {Design of Network Precision Marketing Based on Big Data Analysis Technology},
year = {2020},
volume = {},
number = {},
pages = {77-81},
abstract = {In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.},
keywords = {Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design},
doi = {10.1109/ECIT50008.2020.00026},
ISSN = {},
month = {April},}
@INPROCEEDINGS{8029373,
author = {Zhou, Lixiao and Huang, Maohai},
booktitle = {2017 IEEE International Congress on Big Data (BigData Congress)}, title = {Challenges of Software Testing for Astronomical Big Data},
year = {2017},
volume = {},
number = {},
pages = {529-532},
abstract = {Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.},
keywords = {Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software},
doi = {10.1109/BigDataCongress.2017.91},
ISSN = {},
month = {June},}
@INPROCEEDINGS{8258221,
author = {Hee, Kim},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {Is data quality enough for a clinical decision?: Apply machine learning and avoid bias},
year = {2017},
volume = {},
number = {},
pages = {2612-2619},
abstract = {This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.},
keywords = {Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias},
doi = {10.1109/BigData.2017.8258221},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9391025,
author = {Wang, Fengling and Wang, Han and Xue, Liang},
booktitle = {2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, title = {Research on Data Security in Big Data Cloud Computing Environment},
year = {2021},
volume = {5},
number = {},
pages = {1446-1450},
abstract = {In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.},
keywords = {Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy},
doi = {10.1109/IAEAC50856.2021.9391025},
ISSN = {2689-6621},
month = {March},}
@INPROCEEDINGS{8787092,
author = {Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai},
booktitle = {2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)}, title = {A Survey on Big Data Pre-processing},
year = {2017},
volume = {},
number = {},
pages = {241-247},
abstract = {In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.},
keywords = {Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality},
doi = {10.1109/ACIT-CSII-BCD.2017.49},
ISSN = {},
month = {July},}
@INPROCEEDINGS{8621924,
author = {Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
booktitle = {2018 IEEE International Conference on Big Data (Big Data)}, title = {Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example},
year = {2018},
volume = {},
number = {},
pages = {5328-5329},
abstract = {Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
keywords = {Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot},
doi = {10.1109/BigData.2018.8621924},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7498367,
author = {Sadiq, Shazia and Papotti, Paolo},
booktitle = {2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, title = {Big data quality - whose problem is it?},
year = {2016},
volume = {},
number = {},
pages = {1446-1447},
abstract = {The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.},
keywords = {Big data;Cleaning;Computer science;Databases;Investment},
doi = {10.1109/ICDE.2016.7498367},
ISSN = {},
month = {May},}
@INPROCEEDINGS{7363865,
author = {Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Online anomaly detection over Big Data streams},
year = {2015},
volume = {},
number = {},
pages = {1113-1122},
abstract = {Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.},
keywords = {Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks},
doi = {10.1109/BigData.2015.7363865},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9678209,
author = {Yalaoui, Mehdi and Boukhedouma, Saida},
booktitle = {2021 International Conference on Information Systems and Advanced Technologies (ICISAT)}, title = {A survey on data quality: principles, taxonomies and comparison of approaches},
year = {2021},
volume = {},
number = {},
pages = {1-9},
abstract = {Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.},
keywords = {Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement},
doi = {10.1109/ICISAT54145.2021.9678209},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7944952,
author = {Xie, Chunli and Gao, Jerry and Tao, Chuanqi},
booktitle = {2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)}, title = {Big Data Validation Case Study},
year = {2017},
volume = {},
number = {},
pages = {281-286},
abstract = {With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.},
keywords = {Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study},
doi = {10.1109/BigDataService.2017.44},
ISSN = {},
month = {April},}
@INPROCEEDINGS{9172875,
author = {Zhang, Guobao},
booktitle = {2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)}, title = {A data traceability method to improve data quality in a big data environment},
year = {2020},
volume = {},
number = {},
pages = {290-294},
abstract = {In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.},
keywords = {Data Governance;Data Credibility;Data Traceability},
doi = {10.1109/DSC50466.2020.00051},
ISSN = {},
month = {July},}
@INPROCEEDINGS{9671672,
author = {Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming},
booktitle = {2021 IEEE International Conference on Big Data (Big Data)}, title = {Unsupervised Anomaly Detection in Data Quality Control},
year = {2021},
volume = {},
number = {},
pages = {2327-2336},
abstract = {Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.},
keywords = {Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control},
doi = {10.1109/BigData52589.2021.9671672},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7363987,
author = {Priebe, Torsten and Markus, Stefan},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Business information modeling: A methodology for data-intensive projects, data science and big data governance},
year = {2015},
volume = {},
number = {},
pages = {2056-2065},
abstract = {This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.},
keywords = {Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog},
doi = {10.1109/BigData.2015.7363987},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{7584971,
author = {Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana},
booktitle = {2016 IEEE International Congress on Big Data (BigData Congress)}, title = {An Hybrid Approach to Quality Evaluation across Big Data Value Chain},
year = {2016},
volume = {},
number = {},
pages = {418-425},
abstract = {While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.},
keywords = {Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment},
doi = {10.1109/BigDataCongress.2016.65},
ISSN = {},
month = {June},}
@INPROCEEDINGS{8258380,
author = {Fu, Qian and Easton, John M.},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {Understanding data quality: Ensuring data quality by design in the rail industry},
year = {2017},
volume = {},
number = {},
pages = {3792-3799},
abstract = {The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords = {Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi = {10.1109/BigData.2017.8258380},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8257913,
author = {Benbernou, Salima and Ouziri, Mourad},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {Enhancing data quality by cleaning inconsistent big RDF data},
year = {2017},
volume = {},
number = {},
pages = {74-79},
abstract = {We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.},
keywords = {Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems},
doi = {10.1109/BigData.2017.8257913},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7502278,
author = {Shanmugam, Srinivasan and Seshadri, Gokul},
booktitle = {2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)}, title = {Aspects of Data Cataloguing for Enterprise Data Platforms},
year = {2016},
volume = {},
number = {},
pages = {134-139},
abstract = {As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.},
keywords = {Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service},
doi = {10.1109/BigDataSecurity-HPSC-IDS.2016.52},
ISSN = {},
month = {April},}
@INPROCEEDINGS{9006446,
author = {Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)}, title = {An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection},
year = {2019},
volume = {},
number = {},
pages = {200-205},
abstract = {Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.},
keywords = {Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning},
doi = {10.1109/BigData47090.2019.9006446},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7474375,
author = {Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel},
booktitle = {2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)}, title = {Big Data Quality: A Roadmap for Open Data},
year = {2016},
volume = {},
number = {},
pages = {210-215},
abstract = {Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.},
keywords = {Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction},
doi = {10.1109/BigDataService.2016.37},
ISSN = {},
month = {March},}
@ARTICLE{7974765,
author = {Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman},
journal = {IEEE Transactions on Big Data}, title = {A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment},
year = {2021},
volume = {7},
number = {2},
pages = {468-482},
abstract = {In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.},
keywords = {Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON},
doi = {10.1109/TBDATA.2017.2725904},
ISSN = {2332-7790},
month = {June},}
@INPROCEEDINGS{7473058,
author = {Gao, Jerry and Xie, Chunli and Tao, Chuanqi},
booktitle = {2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, title = {Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs},
year = {2016},
volume = {},
number = {},
pages = {433-441},
abstract = {With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.},
keywords = {Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation},
doi = {10.1109/SOSE.2016.63},
ISSN = {},
month = {March},}
@INPROCEEDINGS{9378148,
author = {O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD},
year = {2020},
volume = {},
number = {},
pages = {1914-1923},
abstract = {The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).},
keywords = {Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality},
doi = {10.1109/BigData50022.2020.9378148},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7877056,
author = {Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva},
booktitle = {2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)}, title = {Towards a Comprehensive Data LifeCycle Model for Big Data Environments},
year = {2016},
volume = {},
number = {},
pages = {100-106},
abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
keywords = {Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges},
doi = {},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8622229,
author = {Austin, Claire C.},
booktitle = {2018 IEEE International Conference on Big Data (Big Data)}, title = {A Path to Big Data Readiness},
year = {2018},
volume = {},
number = {},
pages = {4844-4853},
abstract = {"Big Data readiness" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.},
keywords = {Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science},
doi = {10.1109/BigData.2018.8622229},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8275101,
author = {Setiadi, Yazid and Uluwiyah, Ana},
booktitle = {2017 International Workshop on Big Data and Information Security (IWBIS)}, title = {Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics},
year = {2017},
volume = {},
number = {},
pages = {43-48},
abstract = {Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.},
keywords = {1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics},
doi = {10.1109/IWBIS.2017.8275101},
ISSN = {},
month = {Sep.},}
@ARTICLE{9481251,
author = {Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed},
journal = {IEEE Internet of Things Journal}, title = {An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques},
year = {2022},
volume = {9},
number = {3},
pages = {2443-2454},
abstract = {With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.},
keywords = {Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection},
doi = {10.1109/JIOT.2021.3096637},
ISSN = {2327-4662},
month = {Feb},}
@INPROCEEDINGS{9671890,
author = {Geronazzo, Angela and Ziegler, Markus},
booktitle = {2021 IEEE International Conference on Big Data (Big Data)}, title = {QMLEx: Data Driven Digital Transformation in Marketing Analytics},
year = {2021},
volume = {},
number = {},
pages = {5900-5902},
abstract = {This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.},
keywords = {Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality},
doi = {10.1109/BigData52589.2021.9671890},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7435456,
author = {Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun},
booktitle = {2015 Third International Conference on Advanced Cloud and Big Data}, title = {An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data},
year = {2015},
volume = {},
number = {},
pages = {73-79},
abstract = {The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.},
keywords = {Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis},
doi = {10.1109/CBD.2015.22},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{7840769,
author = {Ganapathi, Archana and Chen, Yanpei},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)}, title = {Data quality: Experiences and lessons from operationalizing big data},
year = {2016},
volume = {},
number = {},
pages = {1595-1602},
abstract = {Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.},
keywords = {Cleaning;Big data;Measurement;Business;Software;Instruments;Industries},
doi = {10.1109/BigData.2016.7840769},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9590700,
author = {Du, Jinming},
booktitle = {2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)}, title = {Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model},
year = {2021},
volume = {},
number = {},
pages = {363-367},
abstract = {With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.},
keywords = {Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics},
doi = {10.1109/ICISCAE52414.2021.9590700},
ISSN = {},
month = {Sep.},}
@INPROCEEDINGS{8695373,
author = {Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita},
booktitle = {2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)}, title = {A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control},
year = {2019},
volume = {},
number = {},
pages = {463-466},
abstract = {Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.},
keywords = {Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control},
doi = {10.1109/MIPR.2019.00093},
ISSN = {},
month = {March},}
@INPROCEEDINGS{9209633,
author = {Byabazaire, John and O’Hare, Gregory and Delaney, Declan},
booktitle = {2020 29th International Conference on Computer Communications and Networks (ICCCN)}, title = {Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments},
year = {2020},
volume = {},
number = {},
pages = {1-9},
abstract = {Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.},
keywords = {Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi = {10.1109/ICCCN49398.2020.9209633},
ISSN = {2637-9430},
month = {Aug},}
@INPROCEEDINGS{8029349,
author = {Kim, Hee Young and Cho, June-Suh},
booktitle = {2017 IEEE International Congress on Big Data (BigData Congress)}, title = {Data Governance Framework for Big Data Implementation with a Case of Korea},
year = {2017},
volume = {},
number = {},
pages = {384-391},
abstract = {Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.},
keywords = {Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis},
doi = {10.1109/BigDataCongress.2017.56},
ISSN = {},
month = {June},}
@INPROCEEDINGS{8532518,
author = {Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy},
booktitle = {2018 IEEE AUTOTESTCON}, title = {Measuring Manufacturing Test Data Analysis Quality},
year = {2018},
volume = {},
number = {},
pages = {1-6},
abstract = {Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.},
keywords = {Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality},
doi = {10.1109/AUTEST.2018.8532518},
ISSN = {1558-4550},
month = {Sep.},}
@INPROCEEDINGS{9263667,
author = {Xiangwei, Kong},
booktitle = {2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, title = {Evaluation of Flight Test Data Quality Based on Rough Set Theory},
year = {2020},
volume = {},
number = {},
pages = {1053-1057},
abstract = {With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.},
keywords = {Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation},
doi = {10.1109/CISP-BMEI51763.2020.9263667},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9298378,
author = {Desai, Vinod and H A, Dinesha},
booktitle = {2020 IEEE International Conference for Innovation in Technology (INOCON)}, title = {A Hybrid Approach to Data Pre-processing Methods},
year = {2020},
volume = {},
number = {},
pages = {1-4},
abstract = {This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.},
keywords = {Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks},
doi = {10.1109/INOCON50539.2020.9298378},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8367700,
author = {Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan},
booktitle = {2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)}, title = {Verification method of data quality in science and technology cloud in Shaanxi province},
year = {2018},
volume = {},
number = {},
pages = {319-323},
abstract = {This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.},
keywords = {Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing},
doi = {10.1109/ICBDA.2018.8367700},
ISSN = {},
month = {March},}
@ARTICLE{8950481,
author = {Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang},
journal = {Chinese Journal of Electrical Engineering}, title = {A missing power data filling method based on improved random forest algorithm},
year = {2019},
volume = {5},
number = {4},
pages = {33-39},
abstract = {Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.},
keywords = {Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality},
doi = {10.23919/CJEE.2019.000025},
ISSN = {2096-1529},
month = {Dec},}
@INPROCEEDINGS{8465132,
author = {Segooa, Mmatshuene Anna and Kalema, Billy Mathias},
booktitle = {2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)}, title = {Improve Decision Making Towards Universities Performance Through Big Data Analytics},
year = {2018},
volume = {},
number = {},
pages = {1-5},
abstract = {The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.},
keywords = {Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality},
doi = {10.1109/ICABCD.2018.8465132},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8102206,
author = {Alqarni, Mohammed A.},
booktitle = {2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT IoT (HONET-ICT)}, title = {Benefits of SDN for Big data applications},
year = {2017},
volume = {},
number = {},
pages = {74-77},
abstract = {Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.},
keywords = {Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data},
doi = {10.1109/HONET.2017.8102206},
ISSN = {1949-4106},
month = {Oct},}
@INPROCEEDINGS{9545944,
author = {Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong},
booktitle = {2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)}, title = {Research on instance-level data cleaning technology},
year = {2021},
volume = {},
number = {},
pages = {238-242},
abstract = {Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.},
keywords = {Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning},
doi = {10.1109/CAIBDA53561.2021.00057},
ISSN = {},
month = {May},}
@INPROCEEDINGS{7051902,
author = {Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)}, title = {Towards a technology roadmap for big data applications in the healthcare domain},
year = {2014},
volume = {},
number = {},
pages = {291-296},
abstract = {Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.},
keywords = {Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality},
doi = {10.1109/IRI.2014.7051902},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8785536,
author = {Auer, Florian and Felderer, Michael},
booktitle = {2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET)}, title = {Addressing Data Quality Problems with Metamorphic Data Relations},
year = {2019},
volume = {},
number = {},
pages = {76-83},
abstract = {In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
keywords = {Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations},
doi = {10.1109/MET.2019.00019},
ISSN = {},
month = {May},}
@ARTICLE{6949519,
author = {O'Leary, Daniel E.},
journal = {IEEE Intelligent Systems}, title = {Embedding AI and Crowdsourcing in the Big Data Lake},
year = {2014},
volume = {29},
number = {5},
pages = {70-73},
abstract = {Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake "work".},
keywords = {Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems},
doi = {10.1109/MIS.2014.82},
ISSN = {1941-1294},
month = {Sep.},}
@INPROCEEDINGS{8258218,
author = {Colborne, Adrienne and Smit, Michael},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {Identifying and mitigating risks to the quality of open data in the post-truth era},
year = {2017},
volume = {},
number = {},
pages = {2588-2594},
abstract = {Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.},
keywords = {Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance},
doi = {10.1109/BigData.2017.8258218},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8713218,
author = {Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin},
booktitle = {2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)}, title = {An Automated Big Data Accuracy Assessment Tool},
year = {2019},
volume = {},
number = {},
pages = {193-197},
abstract = {Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.},
keywords = {Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees},
doi = {10.1109/ICBDA.2019.8713218},
ISSN = {},
month = {March},}
@INPROCEEDINGS{9620761,
author = {Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon},
booktitle = {2021 International Conference on Information and Communication Technology Convergence (ICTC)}, title = {Mechanism of a big-data platform for residential heat energy consumption},
year = {2021},
volume = {},
number = {},
pages = {1450-1452},
abstract = {Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.},
keywords = {Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection},
doi = {10.1109/ICTC52510.2021.9620761},
ISSN = {2162-1233},
month = {Oct},}
@INPROCEEDINGS{7823519,
author = {Li, Tao and He, Yihai and Zhu, Chunling},
booktitle = {2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)}, title = {Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry},
year = {2016},
volume = {},
number = {},
pages = {181-186},
abstract = {The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.},
keywords = {Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM},
doi = {10.1109/ICIICII.2016.0052},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8740576,
author = {Zan, Songting and Zhang, Xu},
booktitle = {2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)}, title = {Medical Data Quality Assessment Model Based on Credibility Analysis},
year = {2018},
volume = {},
number = {},
pages = {940-944},
abstract = {The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.},
keywords = {Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process},
doi = {10.1109/ITOEC.2018.8740576},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8109143,
author = {Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang},
booktitle = {2017 IEEE 13th International Conference on e-Science (e-Science)}, title = {ScienceDB: A Public Multidisciplinary Research Data Repository for eScience},
year = {2017},
volume = {},
number = {},
pages = {248-255},
abstract = {Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.},
keywords = {Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data},
doi = {10.1109/eScience.2017.38},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{6972054,
author = {Ludwig, Heiko},
booktitle = {2014 IEEE 18th International Enterprise Distributed Object Computing Conference}, title = {Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective},
year = {2014},
volume = {},
number = {},
pages = {91-91},
abstract = {Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.},
keywords = {Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments},
doi = {10.1109/EDOC.2014.21},
ISSN = {1541-7719},
month = {Sep.},}
@INPROCEEDINGS{9491629,
author = {Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi},
booktitle = {2021 International Conference on Information Technology (ICIT)}, title = {Recent Quality Models in BigData Applications},
year = {2021},
volume = {},
number = {},
pages = {811-815},
abstract = {In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.},
keywords = {Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data},
doi = {10.1109/ICIT52682.2021.9491629},
ISSN = {},
month = {July},}
@INPROCEEDINGS{7321725,
author = {Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
booktitle = {2014 IEEE/ACM International Symposium on Big Data Computing}, title = {A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year = {2014},
volume = {},
number = {},
pages = {16-25},
abstract = {In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
keywords = {Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop},
doi = {10.1109/BDC.2014.10},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8034978,
author = {Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat},
booktitle = {2017 IEEE International Conference on Services Computing (SCC)}, title = {Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service},
year = {2017},
volume = {},
number = {},
pages = {140-147},
abstract = {Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.},
keywords = {Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation},
doi = {10.1109/SCC.2017.25},
ISSN = {2474-2473},
month = {June},}
@INPROCEEDINGS{7321730,
author = {Ahnn, Jong Hoon},
booktitle = {2014 IEEE/ACM International Symposium on Big Data Computing}, title = {A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung},
year = {2014},
volume = {},
number = {},
pages = {64-73},
abstract = {We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.},
keywords = {Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization},
doi = {10.1109/BDC.2014.11},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9232648,
author = {Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur},
booktitle = {2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)}, title = {A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data},
year = {2020},
volume = {},
number = {},
pages = {209-213},
abstract = {The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.},
keywords = {Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning},
doi = {10.1109/ICSGRC49013.2020.9232648},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{7507493,
author = {Rathore, Purva and Shukla, Deepak},
booktitle = {2015 International Conference on Communication Networks (ICCN)}, title = {Analysis and performance improvement of K-means clustering in big data environment},
year = {2015},
volume = {},
number = {},
pages = {43-46},
abstract = {The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.},
keywords = {Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation},
doi = {10.1109/ICCN.2015.9},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{9352886,
author = {Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu},
booktitle = {2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE)}, title = {Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform},
year = {2020},
volume = {},
number = {},
pages = {229-233},
abstract = {Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.},
keywords = {Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform},
doi = {10.1109/ICECE51594.2020.9352886},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9497187,
author = {Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak},
booktitle = {2021 International Conference on Computer Information Sciences (ICCOINS)}, title = {Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data},
year = {2021},
volume = {},
number = {},
pages = {1-6},
abstract = {A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.},
keywords = {Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality},
doi = {10.1109/ICCOINS49721.2021.9497187},
ISSN = {},
month = {July},}
@INPROCEEDINGS{8315370,
author = {Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal},
booktitle = {2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)}, title = {Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements},
year = {2017},
volume = {},
number = {},
pages = {149-156},
abstract = {Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.},
keywords = {Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS},
doi = {10.1109/SC2.2017.30},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8997699,
author = {Pan, Lingling and Liu, Jun and Li, Feng},
booktitle = {2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, title = {Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation},
year = {2019},
volume = {1},
number = {},
pages = {2386-2390},
abstract = {The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.},
keywords = {Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index},
doi = {10.1109/IAEAC47372.2019.8997699},
ISSN = {2381-0947},
month = {Dec},}
@INPROCEEDINGS{8406658,
author = {Palacio, Ana León and López, Óscar Pastor},
booktitle = {2018 12th International Conference on Research Challenges in Information Science (RCIS)}, title = {From big data to smart data: A genomic information systems perspective},
year = {2018},
volume = {},
number = {},
pages = {1-11},
abstract = {During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.},
keywords = {Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics},
doi = {10.1109/RCIS.2018.8406658},
ISSN = {2151-1357},
month = {May},}
@INPROCEEDINGS{8836982,
author = {Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie},
booktitle = {2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)}, title = {Big Data Driven Smart Agriculture: Pathway for Sustainable Development},
year = {2019},
volume = {},
number = {},
pages = {60-65},
abstract = {Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.},
keywords = {Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming},
doi = {10.1109/ICAIBD.2019.8836982},
ISSN = {},
month = {May},}
@ARTICLE{7937830,
author = {Benbernou, Salima and Huang, Xin and Ouziri, Mourad},
journal = {IEEE Transactions on Big Data}, title = {Semantic-Based and Entity-Resolution Fusion to Enhance Quality of Big RDF Data},
year = {2021},
volume = {7},
number = {2},
pages = {436-450},
abstract = {Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.},
keywords = {Big Data;Resource description framework;Semantics;Joining processes;Erbium;Organizations;Data quality;big data fusion;inferences;entity resolution;query rewriting},
doi = {10.1109/TBDATA.2017.2710346},
ISSN = {2332-7790},
month = {June},}
@INPROCEEDINGS{9599192,
author = {Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie},
booktitle = {2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)}, title = {A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality},
year = {2021},
volume = {},
number = {},
pages = {64-69},
abstract = {With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.},
keywords = {Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition},
doi = {10.1109/ISCEIC53685.2021.00021},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8603594,
author = {Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad},
booktitle = {2018 14th International Conference on Emerging Technologies (ICET)}, title = {Data Quality Techniques in the Internet of Things: Random Forest Regression},
year = {2018},
volume = {},
number = {},
pages = {1-4},
abstract = {Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.},
keywords = {Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning},
doi = {10.1109/ICET.2018.8603594},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8258543,
author = {Liu, Lixin and Chen, Jun},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {The influences of deep-sea vision data quality on observational analysis},
year = {2017},
volume = {},
number = {},
pages = {4789-4791},
abstract = {Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.},
keywords = {Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging},
doi = {10.1109/BigData.2017.8258543},
ISSN = {},
month = {Dec},}
@ARTICLE{9320548,
author = {Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu},
journal = {IEEE Transactions on Automation Science and Engineering}, title = {Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data},
year = {2021},
volume = {18},
number = {4},
pages = {2142-2155},
abstract = {A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation},
keywords = {Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service},
doi = {10.1109/TASE.2020.3040400},
ISSN = {1558-3783},
month = {Oct},}
@ARTICLE{8361574,
author = {Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi},
journal = {Big Data Mining and Analytics}, title = {QoE-driven big data management in pervasive edge computing environment},
year = {2018},
volume = {1},
number = {3},
pages = {222-233},
abstract = {In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.},
keywords = {Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge},
doi = {10.26599/BDMA.2018.9020020},
ISSN = {2096-0654},
month = {Sep.},}
@INPROCEEDINGS{7363818,
author = {Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain},
year = {2015},
volume = {},
number = {},
pages = {737-746},
abstract = {Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.},
keywords = {Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins},
doi = {10.1109/BigData.2015.7363818},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{7840906,
author = {Angryk, Rafal A. and Galarus, Douglas E.},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)}, title = {The SMART approach to comprehensive quality assessment of site-based spatial-temporal data},
year = {2016},
volume = {},
number = {},
pages = {2636-2645},
abstract = {There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.},
keywords = {Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation},
doi = {10.1109/BigData.2016.7840906},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7818902,
author = {Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina},
booktitle = {2016 24th Telecommunications Forum (TELFOR)}, title = {Big data and quality: A literature review},
year = {2016},
volume = {},
number = {},
pages = {1-4},
abstract = {Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.},
keywords = {Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks},
doi = {10.1109/TELFOR.2016.7818902},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{7809598,
author = {Kläs, Michael and Putz, Wolfgang and Lutz, Tobias},
booktitle = {2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)}, title = {Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results},
year = {2016},
volume = {},
number = {},
pages = {115-124},
abstract = {High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.},
keywords = {Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP},
doi = {10.1109/IWSM-Mensura.2016.026},
ISSN = {},
month = {Oct},}
@ARTICLE{8658160,
author = {Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.},
journal = {IEEE Engineering Management Review}, title = {Big Data Technology: Challenges, Prospects, and Realities},
year = {2019},
volume = {47},
number = {1},
pages = {58-66},
abstract = {We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.},
keywords = {Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation},
doi = {10.1109/EMR.2019.2900208},
ISSN = {1937-4178},
month = {Firstquarter},}
@INPROCEEDINGS{9006422,
author = {Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)}, title = {A Policy-based Approach for Measuring Data Quality},
year = {2019},
volume = {},
number = {},
pages = {4025-4031},
abstract = {With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.},
keywords = {Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management},
doi = {10.1109/BigData47090.2019.9006422},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8421858,
author = {Jiang, Wei and Ning, Xiuli and Xu, Yingcheng},
booktitle = {2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)}, title = {Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods},
year = {2018},
volume = {},
number = {},
pages = {95-102},
abstract = {Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.},
keywords = {Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution},
doi = {10.1109/CSCloud/EdgeCom.2018.00025},
ISSN = {},
month = {June},}
@INPROCEEDINGS{9079066,
author = {Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh},
booktitle = {2020 11th International Conference on Information and Communication Systems (ICICS)}, title = {An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time},
year = {2020},
volume = {},
number = {},
pages = {022-026},
abstract = {Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.},
keywords = {Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality},
doi = {10.1109/ICICS49469.2020.239526},
ISSN = {2573-3346},
month = {April},}
@INPROCEEDINGS{7364058,
author = {Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {A memory capacity model for high performing data-filtering applications in Samza framework},
year = {2015},
volume = {},
number = {},
pages = {2600-2605},
abstract = {Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.},
keywords = {Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance},
doi = {10.1109/BigData.2015.7364058},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9251206,
author = {Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing},
booktitle = {2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title = {Applying Machine Learning to Aviation Big Data for Flight Delay Prediction},
year = {2020},
volume = {},
number = {},
pages = {665-672},
abstract = {Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.},
keywords = {Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics},
doi = {10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{9102068,
author = {Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian},
booktitle = {2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)}, title = {Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration},
year = {2020},
volume = {},
number = {},
pages = {687-692},
abstract = {Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.},
keywords = {Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics},
doi = {10.1109/ICIEA49774.2020.9102068},
ISSN = {},
month = {April},}
@INPROCEEDINGS{8539254,
author = {Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel},
booktitle = {2018 7th International Conference on Computer and Communication Engineering (ICCCE)}, title = {Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier},
year = {2018},
volume = {},
number = {},
pages = {208-213},
abstract = {Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.},
keywords = {Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index},
doi = {10.1109/ICCCE.2018.8539254},
ISSN = {},
month = {Sep.},}
@INPROCEEDINGS{9378401,
author = {Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability},
year = {2020},
volume = {},
number = {},
pages = {4026-4035},
abstract = {Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.},
keywords = {Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality},
doi = {10.1109/BigData50022.2020.9378401},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7751629,
author = {Li, Mingxin and Wei, Heng and Liao, Hongxi},
booktitle = {2016 16th International Symposium on Communications and Information Technologies (ISCIT)}, title = {Mobile terminal quality of experience analysis based on big data},
year = {2016},
volume = {},
number = {},
pages = {241-245},
abstract = {In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.},
keywords = {Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method},
doi = {10.1109/ISCIT.2016.7751629},
ISSN = {},
month = {Sep.},}
@INPROCEEDINGS{7273312,
author = {Wang, May D.},
booktitle = {2015 IEEE 39th Annual Computer Software and Applications Conference}, title = {Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health},
year = {2015},
volume = {3},
number = {},
pages = {1-2},
abstract = {Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of "big data" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.},
keywords = {Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making},
doi = {10.1109/COMPSAC.2015.343},
ISSN = {0730-3157},
month = {July},}
@INPROCEEDINGS{9378296,
author = {Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {DQLearn : A Toolkit for Structured Data Quality Learning},
year = {2020},
volume = {},
number = {},
pages = {1644-1653},
abstract = {Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - "Workflows", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.},
keywords = {Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis},
doi = {10.1109/BigData50022.2020.9378296},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{6816764,
author = {Saha, Barna and Srivastava, Divesh},
booktitle = {2014 IEEE 30th International Conference on Data Engineering}, title = {Data quality: The other face of Big Data},
year = {2014},
volume = {},
number = {},
pages = {1294-1297},
abstract = {In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.},
keywords = {Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning},
doi = {10.1109/ICDE.2014.6816764},
ISSN = {2375-026X},
month = {March},}
@INPROCEEDINGS{7363743,
author = {Libes, Don and Shin, Seungjun and Woo, Jungyub},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {Considerations and recommendations for data availability for data analytics for manufacturing},
year = {2015},
volume = {},
number = {},
pages = {68-75},
abstract = {Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.},
keywords = {Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing},
doi = {10.1109/BigData.2015.7363743},
ISSN = {},
month = {Oct},}
@ARTICLE{8809689,
author = {Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni},
journal = {IEEE Access}, title = {An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management},
year = {2019},
volume = {7},
number = {},
pages = {117652-117677},
abstract = {Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.},
keywords = {Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city},
doi = {10.1109/ACCESS.2019.2936941},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{8258222,
author = {Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)}, title = {Data quality challenges with missing values and mixed types in joint sequence analysis},
year = {2017},
volume = {},
number = {},
pages = {2620-2627},
abstract = {The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.},
keywords = {Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality},
doi = {10.1109/BigData.2017.8258222},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9148148,
author = {Bai, Zhongxian and Zhuo, Rongqing},
booktitle = {2020 International Conference on Computer Information and Big Data Applications (CIBDA)}, title = {Quality Management of Crowd Sensing Data Based on Machine Learning},
year = {2020},
volume = {},
number = {},
pages = {185-188},
abstract = {Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t  =  0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.},
keywords = {Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method},
doi = {10.1109/CIBDA50819.2020.00049},
ISSN = {},
month = {April},}
@INPROCEEDINGS{9672007,
author = {Zhou, Xiantian and Ordonez, Carlos},
booktitle = {2021 IEEE International Conference on Big Data (Big Data)}, title = {Programming Languages in Data Science: a Comparison from a Database Angle},
year = {2021},
volume = {},
number = {},
pages = {3147-3154},
abstract = {In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.},
keywords = {Soft sensors;Redundancy;Relational databases;Big Data;Data science;Database systems;Task analysis},
doi = {10.1109/BigData52589.2021.9672007},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8289792,
author = {El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.},
booktitle = {2017 13th International Computer Engineering Conference (ICENCO)}, title = {Record linkage approaches in big data: A state of art study},
year = {2017},
volume = {},
number = {},
pages = {224-230},
abstract = {Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.},
keywords = {Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage},
doi = {10.1109/ICENCO.2017.8289792},
ISSN = {2475-2320},
month = {Dec},}
@INPROCEEDINGS{9397990,
author = {Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman},
booktitle = {2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)}, title = {Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework},
year = {2020},
volume = {},
number = {},
pages = {284-287},
abstract = {In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.},
keywords = {Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change},
doi = {10.1109/WIECON-ECE52138.2020.9397990},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8121809,
author = {Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han},
booktitle = {2017 14th International Symposium on Pervasive Systems, Algorithms and Networks 2017 11th International Conference on Frontier of Computer Science and Technology 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)}, title = {An Electric Power Sensor Data Oriented Data Cleaning Solution},
year = {2017},
volume = {},
number = {},
pages = {430-435},
abstract = {With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.},
keywords = {Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark},
doi = {10.1109/ISPAN-FCST-ISCC.2017.29},
ISSN = {2375-527X},
month = {June},}
@INPROCEEDINGS{8241335,
author = {Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón},
booktitle = {2017 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)}, title = {Effective Data Quality Diagnostic Schema for Big Data},
year = {2017},
volume = {},
number = {},
pages = {163-168},
abstract = {Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.},
keywords = {Databases;Big Data;Measurement;Cleaning;Data mining},
doi = {10.1109/ICMEAE.2017.29},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8748630,
author = {Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy},
booktitle = {2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)}, title = {Big Data and Internet of Things: A Survey},
year = {2018},
volume = {},
number = {},
pages = {150-156},
abstract = {In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.},
keywords = {Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing},
doi = {10.1109/ICACCCN.2018.8748630},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{9284399,
author = {Mystakidis, Aristeidis and Tjortjis, Christos},
booktitle = {2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA}, title = {Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification},
year = {2020},
volume = {},
number = {},
pages = {1-8},
abstract = {This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.},
keywords = {Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion},
doi = {10.1109/IISA50023.2020.9284399},
ISSN = {},
month = {July},}
@INPROCEEDINGS{9378483,
author = {Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {Large-scale Data-driven Segmentation of Banking Customers},
year = {2020},
volume = {},
number = {},
pages = {4392-4401},
abstract = {This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.},
keywords = {Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection},
doi = {10.1109/BigData50022.2020.9378483},
ISSN = {},
month = {Dec},}
@ARTICLE{9082104,
author = {Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime},
journal = {IEEE Transactions on Vehicular Technology}, title = {TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System},
year = {2020},
volume = {69},
number = {7},
pages = {6869-6879},
abstract = {Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.},
keywords = {Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet},
doi = {10.1109/TVT.2020.2991372},
ISSN = {1939-9359},
month = {July},}
@ARTICLE{8667006,
author = {Xu, Xuefang and Lei, Yaguo and Li, Zeda},
journal = {IEEE Transactions on Industrial Electronics}, title = {An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring},
year = {2020},
volume = {67},
number = {3},
pages = {2326-2336},
abstract = {The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.},
keywords = {Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF)},
doi = {10.1109/TIE.2019.2903774},
ISSN = {1557-9948},
month = {March},}
@INPROCEEDINGS{8991080,
author = {Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang},
booktitle = {2019 International Conference on Electronic Engineering and Informatics (EEI)}, title = {Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm},
year = {2019},
volume = {},
number = {},
pages = {506-508},
abstract = {Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.},
keywords = {Apriori algorithm;relay protection;defect data;integrity check},
doi = {10.1109/EEI48997.2019.00115},
ISSN = {},
month = {Nov},}
@ARTICLE{9496639,
author = {Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal},
journal = {IEEE Access}, title = {Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach},
year = {2021},
volume = {9},
number = {},
pages = {107309-107332},
abstract = {Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.},
keywords = {Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS},
doi = {10.1109/ACCESS.2021.3100287},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{7584934,
author = {Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui},
booktitle = {2016 IEEE International Congress on Big Data (BigData Congress)}, title = {From Big Data to Great Services},
year = {2016},
volume = {},
number = {},
pages = {165-172},
abstract = {Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.},
keywords = {Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS},
doi = {10.1109/BigDataCongress.2016.28},
ISSN = {},
month = {June},}
@INPROCEEDINGS{7371861,
author = {Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge},
booktitle = {2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)}, title = {A Survey on Data Quality: Classifying Poor Data},
year = {2015},
volume = {},
number = {},
pages = {179-188},
abstract = {Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.},
keywords = {Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems},
doi = {10.1109/PRDC.2015.41},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8444581,
author = {Zhang, Mingming and Wo, Tianyu and Xie, Tao},
booktitle = {2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, title = {A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services},
year = {2018},
volume = {},
number = {},
pages = {1-7},
abstract = {Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.},
keywords = {Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles},
doi = {10.1109/PERCOM.2018.8444581},
ISSN = {2474-249X},
month = {March},}
@INPROCEEDINGS{9263243,
author = {Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed},
booktitle = {2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, title = {HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors},
year = {2020},
volume = {},
number = {},
pages = {155-164},
abstract = {Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.},
keywords = {Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity},
doi = {10.1109/ICACSIS51025.2020.9263243},
ISSN = {2330-4588},
month = {Oct},}
@INPROCEEDINGS{7029280,
author = {Earley, Seth},
booktitle = {2014 IT Professional Conference}, title = {Presentation 1. Information governance in the age of big data},
year = {2014},
volume = {},
number = {},
pages = {1-3},
abstract = {Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.},
keywords = {Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data},
doi = {10.1109/ITPRO.2014.7029280},
ISSN = {},
month = {May},}
@INPROCEEDINGS{9357200,
author = {Ezzine, Imane and Benhlima, Laila},
booktitle = {2020 6th IEEE Congress on Information Science and Technology (CiSt)}, title = {Technology against COVID-19 A Blockchain-based framework for Data Quality},
year = {2020},
volume = {},
number = {},
pages = {84-89},
abstract = {The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.},
keywords = {COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance},
doi = {10.1109/CiSt49399.2021.9357200},
ISSN = {2327-1884},
month = {June},}
@INPROCEEDINGS{8900190,
author = {Han, Weiguo and Jochum, Matthew},
booktitle = {IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title = {Practices and Experiences in High Volumes of Satellite Data Management},
year = {2019},
volume = {},
number = {},
pages = {4364-4367},
abstract = {High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.},
keywords = {Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository},
doi = {10.1109/IGARSS.2019.8900190},
ISSN = {2153-7003},
month = {July},}
@INPROCEEDINGS{9179628,
author = {Tsoumakos, Dimitrios and Giannakopoulos, Ioannis},
booktitle = {2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)}, title = {Content-Based Analytics: Moving beyond Data Size},
year = {2020},
volume = {},
number = {},
pages = {33-40},
abstract = {Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the "right" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.},
keywords = {Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling},
doi = {10.1109/BigDataService49289.2020.00013},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8456103,
author = {Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh},
booktitle = {2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, title = {Optimized Data de-Identification Using Multidimensional k-Anonymity},
year = {2018},
volume = {},
number = {},
pages = {1610-1614},
abstract = {In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.},
keywords = {Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss},
doi = {10.1109/TrustCom/BigDataSE.2018.00235},
ISSN = {2324-9013},
month = {Aug},}
@INPROCEEDINGS{8622989,
author = {Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician},
booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, title = {Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data},
year = {2018},
volume = {},
number = {},
pages = {1528-1535},
abstract = {When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.},
keywords = {Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency},
doi = {10.1109/HPCC/SmartCity/DSS.2018.00251},
ISSN = {},
month = {June},}
@INPROCEEDINGS{9403739,
author = {Wei, Li and Dawei, Wang and Lixia, Wang},
booktitle = {2020 International Conference on Big Data Artificial Intelligence Software Engineering (ICBASE)}, title = {Research on data Traceability Method Based on blockchain Technology},
year = {2020},
volume = {},
number = {},
pages = {45-49},
abstract = {Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.},
keywords = {Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data},
doi = {10.1109/ICBASE51474.2020.00017},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{8113071,
author = {Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He},
booktitle = {2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)}, title = {Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation},
year = {2017},
volume = {},
number = {},
pages = {232-238},
abstract = {High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.},
keywords = {Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN},
doi = {10.1109/BIGCOM.2017.48},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{9671538,
author = {Feric, Zlatan and Agostini, Nicolas Bohm and Beene, Daniel and Signes-Pastor, Antonio J. and Halchenko, Yuliya and Watkins, Deborah and MacKenzie, Debra and Karagas, Margaret and Manjourides, Justin and Alshawabkeh, Akram and Kaeli, David},
booktitle = {2021 IEEE International Conference on Big Data (Big Data)}, title = {A Secure and Reusable Software Architecture for Supporting Online Data Harmonization},
year = {2021},
volume = {},
number = {},
pages = {2801-2812},
abstract = {Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.},
keywords = {Dictionaries;Software architecture;Databases;Soft sensors;Data visualization;Big Data;Natural language processing},
doi = {10.1109/BigData52589.2021.9671538},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9101464,
author = {Swami, Arun and Vasudevan, Sriram and Huyn, Joojay},
booktitle = {2020 IEEE 36th International Conference on Data Engineering (ICDE)}, title = {Data Sentinel: A Declarative Production-Scale Data Validation Platform},
year = {2020},
volume = {},
number = {},
pages = {1579-1590},
abstract = {Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.},
keywords = {Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules},
doi = {10.1109/ICDE48307.2020.00140},
ISSN = {2375-026X},
month = {April},}
@INPROCEEDINGS{9377900,
author = {Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {Synchronized Preprocessing of Sensor Data},
year = {2020},
volume = {},
number = {},
pages = {3522-3531},
abstract = {Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.},
keywords = {Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management},
doi = {10.1109/BigData50022.2020.9377900},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8622249,
author = {Huang, Yu and Milani, Mostafa and Chiang, Fei},
booktitle = {2018 IEEE International Conference on Big Data (Big Data)}, title = {PACAS: Privacy-Aware, Data Cleaning-as-a-Service},
year = {2018},
volume = {},
number = {},
pages = {1023-1030},
abstract = {Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.},
keywords = {Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy},
doi = {10.1109/BigData.2018.8622249},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8075416,
author = {Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong},
booktitle = {2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)}, title = {Multiple stratified sampling strategy for assessing the big remote sensing products},
year = {2015},
volume = {},
number = {},
pages = {1-4},
abstract = {The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.},
keywords = {Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data},
doi = {10.1109/WHISPERS.2015.8075416},
ISSN = {2158-6276},
month = {June},}
@ARTICLE{8756123,
author = {Zhang, Xi and Zhu, Qixuan},
journal = {IEEE Journal on Selected Areas in Communications}, title = {Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks},
year = {2019},
volume = {37},
number = {8},
pages = {1721-1738},
abstract = {The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.},
keywords = {Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection},
doi = {10.1109/JSAC.2019.2927088},
ISSN = {1558-0008},
month = {Aug},}
@INPROCEEDINGS{8622349,
author = {Sinha, Shweta and Seys, Marcia},
booktitle = {2018 IEEE International Conference on Big Data (Big Data)}, title = {HL7 Data Acquisition amp; Integration: Challenges and Best Practices},
year = {2018},
volume = {},
number = {},
pages = {2453-2457},
abstract = {Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.},
keywords = {Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality},
doi = {10.1109/BigData.2018.8622349},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9659660,
author = {Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and AyshAlhroob},
booktitle = {2021 International Conference on Engineering and Emerging Technologies (ICEET)}, title = {An Improve The Quality Of Data Considering Big Data Aspect Based On Sensitive Of Cost Time},
year = {2021},
volume = {},
number = {},
pages = {1-6},
abstract = {Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.},
keywords = {Costs;Correlation;Data integrity;Volume measurement;Project management;Big Data;Time measurement;Big Data;Project Management;Sensitive Rule;Quality},
doi = {10.1109/ICEET53442.2021.9659660},
ISSN = {2409-2983},
month = {Oct},}
@ARTICLE{8641478,
author = {Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
journal = {IEEE Access}, title = {One-Pass Inconsistency Detection Algorithms for Big Data},
year = {2019},
volume = {7},
number = {},
pages = {22377-22394},
abstract = {Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.},
keywords = {Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint},
doi = {10.1109/ACCESS.2019.2898707},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{8771733,
author = {Radhakrishnan, Asha and Das, Sarasij},
booktitle = {2018 20th National Power Systems Conference (NPSC)}, title = {Quality Assessment of Smart Grid Data},
year = {2018},
volume = {},
number = {},
pages = {1-6},
abstract = {Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.},
keywords = {Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid},
doi = {10.1109/NPSC.2018.8771733},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{7724441,
author = {Aggarwal, Ankur},
booktitle = {2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, title = {Identification of quality parameters associated with 3V's of Big Data},
year = {2016},
volume = {},
number = {},
pages = {1135-1140},
abstract = {Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.},
keywords = {Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity},
doi = {},
ISSN = {},
month = {March},}
@INPROCEEDINGS{8592556,
author = {Hao, Jiao and Jinming, Chen and Yajuan, Guo},
booktitle = {2018 China International Conference on Electricity Distribution (CICED)}, title = {Data-driven lean Management for Distribution Network},
year = {2018},
volume = {},
number = {},
pages = {701-705},
abstract = {This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.},
keywords = {Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis},
doi = {10.1109/CICED.2018.8592556},
ISSN = {2161-749X},
month = {Sep.},}
@INPROCEEDINGS{9378192,
author = {Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.},
booktitle = {2020 IEEE International Conference on Big Data (Big Data)}, title = {An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data},
year = {2020},
volume = {},
number = {},
pages = {5068-5077},
abstract = {Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.},
keywords = {NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series},
doi = {10.1109/BigData50022.2020.9378192},
ISSN = {},
month = {Dec},}
@ARTICLE{8715359,
author = {Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed},
journal = {IEEE Access}, title = {Intelligent Data Engineering for Migration to NoSQL Based Secure Environments},
year = {2019},
volume = {7},
number = {},
pages = {69042-69057},
abstract = {In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.},
keywords = {Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing},
doi = {10.1109/ACCESS.2019.2916912},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{8029838,
author = {Wang, Haiyan and Zhang, Han},
booktitle = {2017 IEEE International Conference on Web Services (ICWS)}, title = {User Requirements Based Service Identification for Big Data},
year = {2017},
volume = {},
number = {},
pages = {800-807},
abstract = {Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios.},
keywords = {Clustering algorithms;Big Data;Quality of service;Monitoring;Reliability;Optimization;Service Identification;User Requirements;PSO Algorithm;Quality of Experience (QoE)},
doi = {10.1109/ICWS.2017.11},
ISSN = {},
month = {June},}
@ARTICLE{8415743,
author = {Yao, Le and Ge, Zhiqiang},
journal = {IEEE Transactions on Industrial Electronics}, title = {Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes},
year = {2019},
volume = {66},
number = {5},
pages = {3681-3692},
abstract = {In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.},
keywords = {Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)},
doi = {10.1109/TIE.2018.2856200},
ISSN = {1557-9948},
month = {May},}
@INPROCEEDINGS{9107851,
author = {Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao},
booktitle = {2019 6th International Conference on Information Science and Control Engineering (ICISCE)}, title = {Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality of Substation Equipment},
year = {2019},
volume = {},
number = {},
pages = {753-757},
abstract = {This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.},
keywords = {big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring},
doi = {10.1109/ICISCE48695.2019.00154},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{8901326,
author = {Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong},
booktitle = {2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)}, title = {Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data},
year = {2019},
volume = {},
number = {},
pages = {88-91},
abstract = {This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.},
keywords = {risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index},
doi = {10.1109/ICSGEA.2019.00028},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{7009043,
author = {Pankowska, Malgorzata},
booktitle = {International Conference on Information Society (i-Society 2014)}, title = {Service science facing Big Data},
year = {2014},
volume = {},
number = {},
pages = {207-212},
abstract = {The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.},
keywords = {Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality},
doi = {10.1109/i-Society.2014.7009043},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{7432708,
author = {Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian},
booktitle = {2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)}, title = {Data quality analysis and improved strategy research on operations management system for electric vehicles},
year = {2015},
volume = {},
number = {},
pages = {2715-2720},
abstract = {It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.},
keywords = {Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing},
doi = {10.1109/DRPT.2015.7432708},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{8121916,
author = {Sattart, Farook and McQuay, Colter and Driessen, Peter F.},
booktitle = {2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)}, title = {Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data},
year = {2017},
volume = {},
number = {},
pages = {1-6},
abstract = {This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.},
keywords = {Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale},
doi = {10.1109/PACRIM.2017.8121916},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8972078,
author = {Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer},
booktitle = {2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, title = {Data Preparation for Data Mining in Chemical Plants using Big Data},
year = {2019},
volume = {1},
number = {},
pages = {1185-1191},
abstract = {Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.},
keywords = {Data quality;Soft sensors;Big data},
doi = {10.1109/INDIN41052.2019.8972078},
ISSN = {2378-363X},
month = {July},}
@ARTICLE{9302878,
author = {Song, Shaoxu and Gao, Fei and Huang, Ruihong and Wang, Chaokun},
journal = {IEEE Transactions on Knowledge and Data Engineering}, title = {Data Dependencies over Big Data: A Family Tree},
year = {2020},
volume = {},
number = {},
pages = {1-1},
abstract = {Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1)categorical data with equality relationships to (2)heterogeneous data with similarity relationships, and (3)numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a)the concepts of these data dependency notations, we investigate (b)the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c)the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d)the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.},
keywords = {Big Data;Phase frequency detectors;Lakes;Picture archiving and communication systems;Databases;Task analysis;Proposals;Integrity constraints;data dependencies},
doi = {10.1109/TKDE.2020.3046443},
ISSN = {1558-2191},
month = {},}
@INPROCEEDINGS{6984221,
author = {Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio},
booktitle = {2014 International Conference on Future Internet of Things and Cloud}, title = {Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper},
year = {2014},
volume = {},
number = {},
pages = {367-372},
abstract = {Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.},
keywords = {Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery},
doi = {10.1109/FiCloud.2014.65},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{7545062,
author = {Tu, Shouzhong and Huang, Minlie},
booktitle = {2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, title = {Scalable Functional Dependencies Discovery from Big Data},
year = {2016},
volume = {},
number = {},
pages = {426-431},
abstract = {Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.},
keywords = {Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data},
doi = {10.1109/BigMM.2016.63},
ISSN = {},
month = {April},}
@INPROCEEDINGS{9480634,
author = {Elsahlamy, Ebtsam and Eshra, Abeer and Eshra, Nadia and El-Fishawy, Nawal},
booktitle = {2021 International Conference on Electronic Engineering (ICEEM)}, title = {Empowering GIS with Big Data: A review of recent advances},
year = {2021},
volume = {},
number = {},
pages = {1-7},
abstract = {In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It's known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.},
keywords = {Government;Data visualization;Production;Big Data;Agriculture;Mobile handsets;Servers;GIS;Big-Data;Geospatial;Agriculture;map-reduce},
doi = {10.1109/ICEEM52022.2021.9480634},
ISSN = {},
month = {July},}
@INPROCEEDINGS{8126976,
author = {Han, Weiguo and Jochum, Matthew},
booktitle = {2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}, title = {Latency analysis of large volume satellite data transmissions},
year = {2017},
volume = {},
number = {},
pages = {384-387},
abstract = {A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.},
keywords = {Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB},
doi = {10.1109/IGARSS.2017.8126976},
ISSN = {2153-7003},
month = {July},}
@INPROCEEDINGS{8882761,
author = {Ullah, Faheem and Ali Babar, M.},
booktitle = {2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)}, title = {QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics},
year = {2019},
volume = {},
number = {},
pages = {81-86},
abstract = {Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.},
keywords = {Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy},
doi = {10.1109/ICECCS.2019.00016},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{9140534,
author = {Molinari, Andrea and Nollo, Giandomenico},
booktitle = {2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)}, title = {The quality concerns in health care Big Data},
year = {2020},
volume = {},
number = {},
pages = {302-305},
abstract = {Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.},
keywords = {Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation},
doi = {10.1109/MELECON48756.2020.9140534},
ISSN = {2158-8481},
month = {June},}
@INPROCEEDINGS{7011535,
author = {Liu, Wanting and Peng, Yonghong and Tobin, Desmond J},
booktitle = {2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)}, title = {Integrated analytics of microarray big data reveals robust gene signature},
year = {2014},
volume = {},
number = {},
pages = {1-7},
abstract = {The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.},
keywords = {Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers},
doi = {10.1109/CIBD.2014.7011535},
ISSN = {},
month = {Dec},}
@ARTICLE{6527249,
author = {Wigan, Marcus R. and Clarke, Roger},
journal = {Computer}, title = {Big Data's Big Unintended Consequences},
year = {2013},
volume = {46},
number = {6},
pages = {46-53},
abstract = {Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article "Big Data's Big Unintended Consequences" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.},
keywords = {Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons},
doi = {10.1109/MC.2013.195},
ISSN = {1558-0814},
month = {June},}
@ARTICLE{9409047,
author = {Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
journal = {IEEE Access}, title = {From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction},
year = {2021},
volume = {9},
number = {},
pages = {60447-60458},
abstract = {In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
keywords = {Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
doi = {10.1109/ACCESS.2021.3074559},
ISSN = {2169-3536},
month = {},}
@INPROCEEDINGS{7364061,
author = {Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)}, title = {CrowdMD: Crowdsourcing-based approach for deduplication},
year = {2015},
volume = {},
number = {},
pages = {2621-2627},
abstract = {Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.},
keywords = {Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality},
doi = {10.1109/BigData.2015.7364061},
ISSN = {},
month = {Oct},}
@INPROCEEDINGS{6602615,
author = {Tien, James M.},
booktitle = {2013 10th International Conference on Service Systems and Service Management}, title = {Big Data: Unleashing information},
year = {2013},
volume = {},
number = {},
pages = {4-4},
abstract = {Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.},
keywords = {Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras},
doi = {10.1109/ICSSSM.2013.6602615},
ISSN = {2161-1904},
month = {July},}
@INPROCEEDINGS{9215250,
author = {Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L},
booktitle = {2020 International Conference on Smart Electronics and Communication (ICOSEC)}, title = {An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics},
year = {2020},
volume = {},
number = {},
pages = {228-235},
abstract = {Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.},
keywords = {Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization},
doi = {10.1109/ICOSEC49089.2020.9215250},
ISSN = {},
month = {Sep.},}
@INPROCEEDINGS{9491733,
author = {Alzyadat, Wael and AlHroob, Aysh and Almukahel, Ikhlas Hassan and Muhairat, Mohammad and Abdallah, Mohammad and Althunibat, Ahmad},
booktitle = {2021 International Conference on Information Technology (ICIT)}, title = {Big Data, Classification, Clustering and Generate Rules: An inevitably intertwined for Prediction},
year = {2021},
volume = {},
number = {},
pages = {149-155},
abstract = {Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.},
keywords = {Databases;Data integrity;Big Data;Data warehouses;Data mining;Information technology;Standards;classification;marketing;association rules;Big Data;k-mean;prediction;preprocess},
doi = {10.1109/ICIT52682.2021.9491733},
ISSN = {},
month = {July},}
@ARTICLE{9407288,
author = {Hampson, Gary and Hargreaves, Neil and Jakubowicz, Helmut and Williams, Gareth and Hatton, Les},
journal = {IEEE Software}, title = {Open Collaboration, Data Quality, and COVID-19},
year = {2021},
volume = {38},
number = {3},
pages = {137-141},
abstract = {The flavor of this "Impact" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.},
keywords = {Industries;Pathogens;Pandemics;Data integrity;Collaboration;Data models;Numerical models},
doi = {10.1109/MS.2021.3056642},
ISSN = {1937-4194},
month = {May},}
@INPROCEEDINGS{7336197,
author = {Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng},
booktitle = {2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, title = {Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction},
year = {2015},
volume = {},
number = {},
pages = {417-422},
abstract = {Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.},
keywords = {Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer},
doi = {10.1109/HPCC-CSS-ICESS.2015.16},
ISSN = {},
month = {Aug},}
@INPROCEEDINGS{8035055,
author = {Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso},
booktitle = {2017 International Conference on High Performance Computing Simulation (HPCS)}, title = {Linked Thesauri Quality Assessment and Documentation for Big Data Discovery},
year = {2017},
volume = {},
number = {},
pages = {37-44},
abstract = {Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.},
keywords = {Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV},
doi = {10.1109/HPCS.2017.16},
ISSN = {},
month = {July},}
@INPROCEEDINGS{8669595,
author = {Jiang, Ying and Zhang, Na and Fang, Ying},
booktitle = {2019 International Conference on Intelligent Transportation, Big Data Smart City (ICITBS)}, title = {The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology},
year = {2019},
volume = {},
number = {},
pages = {456-459},
abstract = {As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.},
keywords = {Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict},
doi = {10.1109/ICITBS.2019.00118},
ISSN = {},
month = {Jan},}
@INPROCEEDINGS{7979931,
author = {Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang},
booktitle = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)}, title = {Big Data Analysis on Radiographic Image Quality},
year = {2016},
volume = {},
number = {},
pages = {341-346},
abstract = {Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.},
keywords = {Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control},
doi = {10.1109/CCBD.2016.073},
ISSN = {},
month = {Nov},}
@INPROCEEDINGS{6597123,
author = {Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat},
booktitle = {2013 IEEE International Congress on Big Data}, title = {Towards a Quality-centric Big Data Architecture for Federated Sensor Services},
year = {2013},
volume = {},
number = {},
pages = {86-93},
abstract = {As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.},
keywords = {Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization},
doi = {10.1109/BigData.Congress.2013.21},
ISSN = {2379-7703},
month = {June},}
@ARTICLE{7809119,
author = {Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert},
journal = {IEEE Transactions on Big Data}, title = {Large-Scale Data Pollution with Apache Spark},
year = {2020},
volume = {6},
number = {2},
pages = {396-411},
abstract = {Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.},
keywords = {Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark},
doi = {10.1109/TBDATA.2016.2637378},
ISSN = {2332-7790},
month = {June},}
@INPROCEEDINGS{9732098,
author = {Wrembel, Robert},
booktitle = {2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)}, title = {Still Open Problems in Data Warehouse and Data Lake Research: extended abstract},
year = {2021},
volume = {},
number = {},
pages = {01-03},
abstract = {During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.},
keywords = {Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata},
doi = {10.1109/SNAMS53716.2021.9732098},
ISSN = {},
month = {Dec},}
@INPROCEEDINGS{9035250,
author = {Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.},
booktitle = {2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)}, title = {Assessing Context-Aware Data Consistency},
year = {2019},
volume = {},
number = {},
pages = {1-6},
abstract = {Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.},
keywords = {Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark},
doi = {10.1109/AICCSA47632.2019.9035250},
ISSN = {2161-5330},
month = {Nov},}
@INPROCEEDINGS{8859426,
author = {He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia},
booktitle = {2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, title = {Quality Driven Judicial Data Governance},
year = {2019},
volume = {},
number = {},
pages = {66-70},
abstract = {With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.},
keywords = {Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement},
doi = {10.1109/QRS-C.2019.00026},
ISSN = {},
month = {July},}
@INPROCEEDINGS{7159309,
author = {Subhashini, R. and Akila, G},
booktitle = {2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]}, title = {Valence arousal similarity based recommendation services},
year = {2015},
volume = {},
number = {},
pages = {1-4},
abstract = {Web Services play a vital role in e-commerce and e-business applications. A WS (Web Service) application is interoperable and can work on any platform i.e.; platform independent, large scale distributed systems can be established easily. A Recommender System is a precious tool for providing appropriate recommendations to all users in a Hotel Reservation Website. User based, Top k and profile based approaches are used in collaborative filtering algorithm which does not provide personalized results to the users and inefficiency and scalability problem also occurs due to the increase in the size of large datasets. To address the above mentioned challenges, a Valence-Arousal Similarity based Recommendation Services, called VAS based RS, is proposed. Our proposed mechanism aims to presents a personalized service recommendation list and recommending the most suitable service to the end users. Moreover, it classifies the positive and negative preferences of the users from their reviews to improve the prediction accuracy. For improve its efficiency and scalability in big data environment, VAS based RS is implemented using collaborative filtering algorithm on MapReduce parallel processing paradigm in Hadoop, a widely-adopted distributed computing platform.},
keywords = {Web services;Collaboration;Big data;Quality of service;Recommender systems;Scalability;Web Service;Big Data;Recommender System;MapReduce;Hadoop},
doi = {10.1109/ICCPCT.2015.7159309},
ISSN = {},
month = {March},}
© 2022 GitHub, Inc.
Terms
Privacy
Security
Status
Docs
Contact GitHub
Pricing
API
Training
Blog
About
Loading complete
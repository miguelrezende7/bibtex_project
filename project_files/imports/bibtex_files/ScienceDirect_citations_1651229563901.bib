@article{GHASEMAGHAEI201969,
title = {Does big data enhance firm innovation competency? The mediating role of data-driven insights},
journal = {Journal of Business Research},
volume = {104},
pages = {69-84},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304138},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency},
abstract = {Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.}
}
@article{GUPTA201878,
title = {Big data with cognitive computing: A review for the future},
journal = {International Journal of Information Management},
volume = {42},
pages = {78-89},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218304110},
author = {Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter},
keywords = {Big data, Cognitive computing, Literature review, Resource based View, Institutional theory},
abstract = {Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.}
}
@article{HASHEM201598,
title = {The rise of “big data” on cloud computing: Review and open research issues},
journal = {Information Systems},
volume = {47},
pages = {98-115},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001288},
author = {Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}},
keywords = {Big data, Cloud computing, Hadoop},
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.}
}
@article{COX2018111,
title = {Big data: Some statistical issues},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {111-115},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300609},
author = {D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh},
keywords = {Big data, Electronic health records, Epidemiology, Metrology, Precision},
abstract = {A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.}
}
@article{SUN201827,
title = {Lossless Pruned Naive Bayes for Big Data Classifications},
journal = {Big Data Research},
volume = {14},
pages = {27-36},
year = {2018},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616301320},
author = {Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu},
keywords = {Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned},
abstract = {In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.}
}
@article{TAI2019101704,
title = {Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry},
journal = {Artificial Intelligence in Medicine},
volume = {99},
pages = {101704},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101704},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717301781},
author = {Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre},
keywords = {Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines},
abstract = {Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.}
}
@article{SAGGI2018758,
title = {A survey towards an integration of big data analytics to big insights for value-creation},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {758-790},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316307178},
author = {Mandeep Kaur Saggi and Sushma Jain},
keywords = {Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization},
abstract = {Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.}
}
@article{DEFREITASVISCONDI201954,
title = {A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting},
journal = {Sustainable Energy Technologies and Assessments},
volume = {31},
pages = {54-63},
year = {2019},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213138818301036},
author = {Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza},
keywords = {Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining},
abstract = {Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.}
}
@article{COBB2018640,
title = {Big data: More than big data sets},
journal = {Surgery},
volume = {164},
number = {4},
pages = {640-642},
year = {2018},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2018.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0039606018303660},
author = {Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo},
abstract = {The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.}
}
@article{BAYNE2018481,
title = {Big Data in Neonatal Health Care: Big Reach, Big Reward?},
journal = {Critical Care Nursing Clinics of North America},
volume = {30},
number = {4},
pages = {481-497},
year = {2018},
note = {Neonatal Nursing},
issn = {0899-5885},
doi = {https://doi.org/10.1016/j.cnc.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899588518309754},
author = {Lynn E. Bayne},
keywords = {Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics}
}
@article{SAFHI201930,
title = {Assessing reliability of Big Data Knowledge Discovery process},
journal = {Procedia Computer Science},
volume = {148},
pages = {30-36},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300055},
author = {Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi},
keywords = {Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining},
abstract = {Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.}
}
@article{MOKTADIR20191063,
title = {Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1063-1075},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301505},
author = {Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla},
keywords = {AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains},
abstract = {Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.}
}
@article{XIA2018191,
title = {Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {191-203},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300401},
author = {Jizhe Xia and Chaowei Yang and Qingquan Li},
keywords = {Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS},
abstract = {Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.}
}
@article{HONG2018175,
title = {Big Data in Health Care: Applications and Challenges},
journal = {Data and Information Management},
volume = {2},
number = {3},
pages = {175-197},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000791},
author = {Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu},
keywords = {Big Data, public health, cloud computing, medical applications},
abstract = {The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.}
}
@article{GUPTA2019466,
title = {Circular economy and big data analytics: A stakeholder perspective},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {466-474},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314488},
author = {Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}},
keywords = {Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability},
abstract = {The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.}
}
@article{JANSSEN2017338,
title = {Factors influencing big data decision-making quality},
journal = {Journal of Business Research},
volume = {70},
pages = {338-345},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304945},
author = {Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi},
keywords = {Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality},
abstract = {Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.}
}
@article{LI20191259,
title = {Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology},
journal = {Applied Energy},
volume = {242},
pages = {1259-1273},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919305495},
author = {Shuangqi Li and Hongwen He and Jianwei Li},
keywords = {Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning},
abstract = {As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.}
}
@article{CORTEREAL2019160,
title = {Unlocking the drivers of big data analytics value in firms},
journal = {Journal of Business Research},
volume = {97},
pages = {160-173},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318306908},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič},
keywords = {IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage},
abstract = {Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.}
}
@incollection{SHARMA202137,
title = {Chapter 2 - Deep learning in big data and data mining},
editor = {Vincenzo Piuri and Sandeep Raj and Angelo Genovese and Rajshree Srivastava},
booktitle = {Trends in Deep Learning Methodologies},
publisher = {Academic Press},
pages = {37-61},
year = {2021},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-12-822226-3},
doi = {https://doi.org/10.1016/B978-0-12-822226-3.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222263000027},
author = {Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha},
keywords = {Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning},
abstract = {The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.}
}
@article{PEZOULAS2019270,
title = {Medical data quality assessment: On the development of an automated framework for medical data curation},
journal = {Computers in Biology and Medicine},
volume = {107},
pages = {270-283},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519300733},
author = {Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis},
keywords = {Big data, Data quality, Data quality assessment, Data curation, Data standardization},
abstract = {Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.}
}
@article{REN20191343,
title = {A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions},
journal = {Journal of Cleaner Production},
volume = {210},
pages = {1343-1365},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618334255},
author = {Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida},
keywords = {Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle},
abstract = {Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.}
}
@article{SIEMSANDERSON2019100071,
title = {An adaptive big data weather system for surface transportation},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {3},
pages = {100071},
year = {2019},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2019.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2590198219300703},
author = {Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt},
keywords = {Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts},
abstract = {Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.}
}
@article{ALBADI2018271,
title = {Exploring Big Data Governance Frameworks},
journal = {Procedia Computer Science},
volume = {141},
pages = {271-277},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318313},
author = {Ali Al-Badi and Ali Tarhini and Asharul Islam Khan},
keywords = {Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic},
abstract = {The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.}
}
@article{SILVA2020893,
title = {Identification of Patterns of Fatal Injuries in Humans through Big Data},
journal = {Procedia Computer Science},
volume = {170},
pages = {893-898},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305524},
author = {Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran},
keywords = {Recognition of automated standards, mining, decision trees},
abstract = {External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.}
}
@incollection{HAYMOND202137,
title = {Chapter 3 - Machine learning and big data in pediatric laboratory medicine},
editor = {Dennis Dietzen and Michael Bennett and Edward Wong and Shannon Haymond},
booktitle = {Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
pages = {37-70},
year = {2021},
isbn = {978-0-12-817962-8},
doi = {https://doi.org/10.1016/B978-0-12-817962-8.00018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179628000184},
author = {Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master},
keywords = {Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation},
abstract = {Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.}
}
@article{LIN2019197,
title = {Data source selection for information integration in big data era},
journal = {Information Sciences},
volume = {479},
pages = {197-213},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309162},
author = {Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao},
keywords = {Source selection, Data integration, Data cleaning},
abstract = {In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.}
}
@article{ZARRINPAR2020599,
title = {What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?},
journal = {Journal of Surgical Research},
volume = {246},
pages = {599-604},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0022480419306985},
author = {Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo},
keywords = {Electronic health record, Big data, Drug safety, Health care database, Cancer risk},
abstract = {As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.}
}
@article{NASHIPUDIMATH2020100033,
title = {An efficient integration and indexing method based on feature patterns and semantic analysis for big data},
journal = {Array},
volume = {7},
pages = {100033},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300187},
author = {Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain},
keywords = {Big data, Integration, Feature patterns, Indexing, Semantic analysis},
abstract = {Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.}
}
@article{LI2018301,
title = {Big data in tourism research: A literature review},
journal = {Tourism Management},
volume = {68},
pages = {301-323},
year = {2018},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517718300591},
author = {Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li},
keywords = {Tourism research, Big data, Literature review, Tourism management, Tourist behavior},
abstract = {Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.}
}
@article{VANDERVOORT201927,
title = {Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {27-38},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304951},
author = {H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer},
abstract = {Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.}
}
@article{MIKALEF2020103169,
title = {Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities},
journal = {Information & Management},
volume = {57},
number = {2},
pages = {103169},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618301022},
author = {Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou},
keywords = {Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view},
abstract = {A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.}
}
@article{ANEJIONU2019456,
title = {Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {456-473},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18319046},
author = {Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole},
keywords = {Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data},
abstract = {The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.}
}
@article{RAJABION2019271,
title = {Healthcare big data processing mechanisms: The role of cloud computing},
journal = {International Journal of Information Management},
volume = {49},
pages = {271-289},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217304917},
author = {Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar},
keywords = {Cloud computing, Processing, Healthcare, Big data, Review},
abstract = {Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.}
}
@article{UEDA2019150,
title = {Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era},
journal = {Molecular Plant},
volume = {12},
number = {2},
pages = {150-152},
year = {2019},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1674205219300139},
author = {Yoshiaki Ueda and Shuichi Yanagisawa}
}
@article{LIANG2019290,
title = {A survey on big data-driven digital phenotyping of mental health},
journal = {Information Fusion},
volume = {52},
pages = {290-307},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518305244},
author = {Yunji Liang and Xiaolong Zheng and Daniel D. Zeng},
keywords = {Digital phenotyping, Big data, Mental health, Data mining, Information fusion},
abstract = {The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.}
}
@article{BRADLOW201779,
title = {The Role of Big Data and Predictive Analytics in Retailing},
journal = {Journal of Retailing},
volume = {93},
number = {1},
pages = {79-95},
year = {2017},
note = {The Future of Retailing},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022435916300835},
author = {Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti},
keywords = {Big data, Predictive analytics, Retailing, Pricing},
abstract = {The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.}
}
@article{CUQUET201874,
title = {The societal impact of big data: A research roadmap for Europe},
journal = {Technology in Society},
volume = {54},
pages = {74-86},
year = {2018},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X17300131},
author = {Martí Cuquet and Anna Fensel},
keywords = {Big data, Research roadmap, Societal externalities, Skills development, Standardisation},
abstract = {With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.}
}
@article{HUANCHUN2020102024,
title = {Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS},
journal = {Sustainable Cities and Society},
volume = {55},
pages = {102024},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102024},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300111},
author = {Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng},
keywords = {land-surface temperature, thermal field pattern, POI data, GIS, air temperature},
abstract = {The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.}
}
@article{GALETSI2019112533,
title = {Values, challenges and future directions of big data analytics in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {241},
pages = {112533},
year = {2019},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112533},
url = {https://www.sciencedirect.com/science/article/pii/S0277953619305271},
author = {P. Galetsi and K. Katsaliaki and S. Kumar},
keywords = {Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses},
abstract = {The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.}
}
@incollection{LYTRAS2021xvii,
title = {Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {xvii-xxvii},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000188},
author = {Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui}
}
@article{URREHMAN2019247,
title = {The role of big data analytics in industrial Internet of Things},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {247-259},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313645},
author = {Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera},
keywords = {Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data},
abstract = {Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.}
}
@article{NIMMAGADDA2018143,
title = {On big data-guided upstream business research and its knowledge management},
journal = {Journal of Business Research},
volume = {89},
pages = {143-158},
year = {2018},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318302054},
author = {Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood},
keywords = {Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions},
abstract = {The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.}
}
@incollection{DAS2020127,
title = {Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare},
editor = {Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}},
booktitle = {Terahertz Biomedical and Healthcare Technologies},
publisher = {Elsevier},
pages = {127-143},
year = {2020},
isbn = {978-0-12-818556-8},
doi = {https://doi.org/10.1016/B978-0-12-818556-8.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185568000070},
author = {Debashis Das and Chinmay Chakraborty and Sourav Banerjee},
keywords = {3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images},
abstract = {This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.}
}
@article{HOLMLUND2020356,
title = {Customer experience management in the age of big data analytics: A strategic framework},
journal = {Journal of Business Research},
volume = {116},
pages = {356-365},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320300345},
author = {Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki},
keywords = {Customer experience, Customer experience management, Customer experience insight, Big data analytics},
abstract = {Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{BENASSULI20197,
title = {Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms},
journal = {Health Policy and Technology},
volume = {8},
number = {1},
pages = {7-13},
year = {2019},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718301631},
author = {Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner},
keywords = {Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network},
abstract = {Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.}
}
@article{RAUT201910,
title = {Linking big data analytics and operational sustainability practices for sustainable business management},
journal = {Journal of Cleaner Production},
volume = {224},
pages = {10-24},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.181},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619308753},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede},
keywords = {Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies},
abstract = {Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.}
}
@article{GHANI2019417,
title = {Social media big data analytics: A survey},
journal = {Computers in Human Behavior},
volume = {101},
pages = {417-428},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.08.039},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830414X},
author = {Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed},
keywords = {Big data, Social media, Machine learning, Analytics},
abstract = {Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.}
}
@incollection{LOSHIN201339,
title = {Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {39-48},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000053},
author = {David Loshin},
keywords = {Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement},
abstract = {In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.}
}
@article{BOLDOSOVA2020122,
title = {Telling stories that sell: The role of storytelling and big data analytics in smart service sales},
journal = {Industrial Marketing Management},
volume = {86},
pages = {122-134},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118303353},
author = {Valeriia Boldosova},
keywords = {Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships},
abstract = {The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.}
}
@article{FENG2021103636,
title = {Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning},
journal = {Tunnelling and Underground Space Technology},
volume = {110},
pages = {103636},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103636},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820305903},
author = {Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing},
keywords = {TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction},
abstract = {This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.}
}
@article{ELAGGOUNE2020465,
title = {A fuzzy agent approach for smart data extraction in big data environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {4},
pages = {465-478},
year = {2020},
note = {Emerging Software Systems},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819302010},
author = {Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough},
keywords = {Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data},
abstract = {The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.}
}
@incollection{HULSEN202169,
title = {Chapter 4 - Challenges and solutions for big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {69-94},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000167},
author = {Tim Hulsen},
keywords = {Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics},
abstract = {“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.}
}
@article{MILNE2019235,
title = {Big data and understanding change in the context of planning transport systems},
journal = {Journal of Transport Geography},
volume = {76},
pages = {235-244},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300984},
author = {Dave Milne and David Watling},
abstract = {This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.}
}
@article{VIEIRA2020125,
title = {Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio},
journal = {Procedia Manufacturing},
volume = {42},
pages = {125-131},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306582},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.}
}
@article{DAISSAOUI2020161,
title = {IoT and Big Data Analytics for Smart Buildings: A Survey},
journal = {Procedia Computer Science},
volume = {170},
pages = {161-168},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304506},
author = {Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath},
keywords = {Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing},
abstract = {The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.}
}
@article{BELHADI2019106099,
title = {Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies},
journal = {Computers & Industrial Engineering},
volume = {137},
pages = {106099},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106099},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219305686},
author = {Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}},
keywords = {Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study},
abstract = {Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.}
}
@article{HAMALAINEN2019100105,
title = {Industrial applications of big data in disruptive innovations supporting environmental reporting},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100105},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100105},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300044},
author = {Esa Hämäläinen and Tommi Inkinen},
keywords = {Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography},
abstract = {Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.}
}
@article{ZHANG2019814,
title = {Big data driven decision-making for batch-based production systems},
journal = {Procedia CIRP},
volume = {83},
pages = {814-818},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119310169},
author = {Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li},
keywords = {Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan},
abstract = {The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).}
}
@article{TU2020101428,
title = {Portraying the spatial dynamics of urban vibrancy using multisource urban big data},
journal = {Computers, Environment and Urban Systems},
volume = {80},
pages = {101428},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101428},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519302674},
author = {Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li},
keywords = {Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data},
abstract = {Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.}
}
@incollection{MCGILVRAY20217,
title = {Chapter 1 - Data Quality and the Data-Dependent World},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {7-14},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000219},
author = {Danette McGilvray},
keywords = {Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19},
abstract = {Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.}
}
@article{URBAN2020117792,
title = {Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function},
journal = {Fuel},
volume = {273},
pages = {117792},
year = {2020},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.117792},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120307870},
author = {András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský},
keywords = {Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood},
abstract = {In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.}
}
@article{PERAKIS2020107035,
title = {CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics},
journal = {Computer Networks},
volume = {168},
pages = {107035},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.107035},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619305353},
author = {Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne},
keywords = {Precision agriculture, Precision livestock farming, High performance computing, Big data analytics},
abstract = {According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.}
}
@article{ALWAN2022101951,
title = {Data quality challenges in large-scale cyber-physical systems: A systematic review},
journal = {Information Systems},
volume = {105},
pages = {101951},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101951},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001484},
author = {Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin},
keywords = {Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations},
abstract = {Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.}
}
@article{RANA2019807,
title = {How Big Data Science Can Improve Linkage and Retention in Care},
journal = {Infectious Disease Clinics of North America},
volume = {33},
number = {3},
pages = {807-815},
year = {2019},
note = {HIV},
issn = {0891-5520},
doi = {https://doi.org/10.1016/j.idc.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0891552019300455},
author = {Aadia I. Rana and Michael J. Mugavero},
keywords = {HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance}
}
@article{WORDSWORTH20181048,
title = {Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions},
journal = {Value in Health},
volume = {21},
number = {9},
pages = {1048-1053},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518322654},
author = {Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier},
keywords = {Big data, cost-effectiveness, next generation sequencing},
abstract = {Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.}
}
@article{PEDRO20193,
title = {Capabilities and Readiness for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {164},
pages = {3-10},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.147},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321866},
author = {Jenifer Pedro and Irwin Brown and Mike Hart},
keywords = {Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis},
abstract = {Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.}
}
@article{AITHAMMOU2020102122,
title = {Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics},
journal = {Information Processing & Management},
volume = {57},
number = {1},
pages = {102122},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102122},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319305163},
author = {Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline},
keywords = {Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics},
abstract = {Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.}
}
@article{WANG2020119299,
title = {Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119299},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119299},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341691},
author = {Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu},
keywords = {multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration},
abstract = {Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.}
}
@article{DREMEL2020103121,
title = {Actualizing big data analytics affordances: A revelatory case study},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103121},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308522},
author = {Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}},
keywords = {Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization},
abstract = {Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.}
}
@article{BACHECHI2022100292,
title = {Big Data Analytics and Visualization in Traffic Monitoring},
journal = {Big Data Research},
volume = {27},
pages = {100292},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100292},
url = {https://www.sciencedirect.com/science/article/pii/S221457962100109X},
author = {Chiara Bachechi and Laura Po and Federica Rollo},
keywords = {Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities},
abstract = {This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.}
}
@article{LIM201886,
title = {Smart cities with big data: Reference models, challenges, and considerations},
journal = {Cities},
volume = {82},
pages = {86-99},
year = {2018},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264275117308545},
author = {Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Smart city, Big data, Reference model, Challenge, Consideration},
abstract = {Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.}
}
@article{MIKALEF2019261,
title = {Big data analytics and firm performance: Findings from a mixed-method approach},
journal = {Journal of Business Research},
volume = {98},
pages = {261-276},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930061X},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty},
abstract = {Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.}
}
@article{GUO20181,
title = {Research on case retrieval of Bayesian network under big data},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {1-13},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18300624},
author = {Yuan Guo and Yuan Guo and K. Wu},
keywords = {Case retrieval, Big data, BN model, Hadoop platform},
abstract = {Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.}
}
@article{LI2021103928,
title = {Search query of English translation text based on embedded system and big data},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103928},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103928},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001071},
author = {Zhihong Li},
keywords = {Cross-language information retrieval, Optical character recognition, Embedded applications},
abstract = {Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.}
}
@article{ARIYALURANHABEEB2019289,
title = {Real-time big data processing for anomaly detection: A Survey},
journal = {International Journal of Information Management},
volume = {45},
pages = {289-307},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218301658},
author = {Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran},
keywords = {Real-time, Big data processing, Anomaly detection and machine learning algorithms},
abstract = {The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.}
}
@article{SHADROO201819,
title = {Systematic survey of big data and data mining in internet of things},
journal = {Computer Networks},
volume = {139},
pages = {19-47},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618301579},
author = {Shabnam Shadroo and Amir Masoud Rahmani},
keywords = {Internet of things, Systematic survey, Big data, Data mining},
abstract = {In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.}
}
@article{LI2019168,
title = {Lithium-ion battery modeling based on Big Data},
journal = {Energy Procedia},
volume = {159},
pages = {168-173},
year = {2019},
note = {Renewable Energy Integration with Mini/Microgrid},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2018.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S1876610218313419},
author = {Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang},
keywords = {electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning},
abstract = {Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.}
}
@article{MEHTA201857,
title = {Concurrence of big data analytics and healthcare: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {114},
pages = {57-65},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302466},
author = {Nishita Mehta and Anil Pandit},
keywords = {Big data, Analytics, Healthcare, Predictive analytics, Evidence-based medicine},
abstract = {Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.}
}
@article{SHAW2021108953,
title = {Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard},
journal = {Ocean Engineering},
volume = {232},
pages = {108953},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821003887},
author = {Heiu-Jou Shaw and Cheng-Kuan Lin},
keywords = {Energy efficiency management, ISO 19030, Hull and propeller maintenance},
abstract = {This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.}
}
@article{MANTELERO2018754,
title = {AI and Big Data: A blueprint for a human rights, social and ethical impact assessment},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {754-772},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918302012},
author = {Alessandro Mantelero},
keywords = {Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation},
abstract = {The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.}
}
@article{JONES20193,
title = {What we talk about when we talk about (big) data},
journal = {The Journal of Strategic Information Systems},
volume = {28},
number = {1},
pages = {3-16},
year = {2019},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0963868718302622},
author = {Matthew Jones},
abstract = {In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.}
}
@incollection{ILMUDEEN202133,
title = {Chapter 3 - Big data-based frameworks for healthcare systems},
editor = {Pradeep N and Sandeep Kautish and Sheng-Lung Peng},
booktitle = {Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics},
publisher = {Academic Press},
pages = {33-56},
year = {2021},
isbn = {978-0-12-821633-0},
doi = {https://doi.org/10.1016/B978-0-12-821633-0.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128216330000039},
author = {Aboobucker Ilmudeen},
keywords = {Big data, Frameworks, Healthcare, Healthcare systems},
abstract = {Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.}
}
@article{CANITO20181,
title = {Unfolding the relations between companies and technologies under the Big Data umbrella},
journal = {Computers in Industry},
volume = {99},
pages = {1-8},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830040X},
author = {João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita},
keywords = {Big data companies, Big data technologies, Online news, Gartner magic quadrant},
abstract = {Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{SILVA2020107828,
title = {Can big data explain yield variability and water productivity in intensive cropping systems?},
journal = {Field Crops Research},
volume = {255},
pages = {107828},
year = {2020},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2020.107828},
url = {https://www.sciencedirect.com/science/article/pii/S0378429019318039},
author = {João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma},
keywords = {Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands},
abstract = {Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.}
}
@article{LI2019991,
title = {Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?},
journal = {Applied Energy},
volume = {239},
pages = {991-1002},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919302922},
author = {Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan},
keywords = {Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data},
abstract = {Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.}
}
@article{MCISAAC2020510,
title = {Real-world evaluation of enhanced recovery after surgery: big data under the microscope},
journal = {British Journal of Anaesthesia},
volume = {124},
number = {5},
pages = {510-512},
year = {2020},
issn = {0007-0912},
doi = {https://doi.org/10.1016/j.bja.2020.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0007091220300593},
author = {Daniel I. McIsaac},
keywords = {big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design}
}
@article{LI202018,
title = {Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning},
journal = {Data and Information Management},
volume = {4},
number = {1},
pages = {18-43},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000572},
author = {Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox},
keywords = {information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning},
abstract = {Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.}
}
@article{CONNELLY20161,
title = {The role of administrative data in the big data revolution in social science research},
journal = {Social Science Research},
volume = {59},
pages = {1-12},
year = {2016},
note = {Special issue on Big Data in the Social Sciences},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X1630206X},
author = {Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben},
keywords = {Big data, Administrative data, Data management, Data quality, Data access},
abstract = {The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.}
}
@article{BARJAMARTINEZ2021111459,
title = {Artificial intelligence techniques for enabling Big Data services in distribution networks: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {150},
pages = {111459},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111459},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121007413},
author = {Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles},
keywords = {Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service},
abstract = {Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.}
}
@incollection{RISTEVSKI202185,
title = {4 - Healthcare and medical Big Data analytics},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {85-112},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000059},
author = {Blagoj Ristevski and Snezana Savoska},
keywords = {Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems},
abstract = {In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.}
}
@article{MCCOY201774,
title = {Geospatial Big Data and archaeology: Prospects and problems too great to ignore},
journal = {Journal of Archaeological Science},
volume = {84},
pages = {74-94},
year = {2017},
note = {Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0305440317300821},
author = {Mark D. McCoy},
keywords = {Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science},
abstract = {As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.}
}
@article{QIAN2021645,
title = {Visual recognition processing of power monitoring data based on big data computing},
journal = {Energy Reports},
volume = {7},
pages = {645-657},
year = {2021},
note = {2021 International Conference on Energy Engineering and Power Systems},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S235248472101009X},
author = {Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi},
keywords = {Power control data, Monitoring, Visual identification, Iterative screening, CARIMA},
abstract = {The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.}
}
@article{CAISSIE2020e773,
title = {Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {108},
number = {3, Supplement },
pages = {e773-e774},
year = {2020},
note = {Proceedings of the American Society for Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2020.07.224},
url = {https://www.sciencedirect.com/science/article/pii/S0360301620316436},
author = {A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo}
}
@article{KUMARI2018169,
title = {Multimedia big data computing and Internet of Things applications: A taxonomy and process model},
journal = {Journal of Network and Computer Applications},
volume = {124},
pages = {169-195},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303011},
author = {Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo},
keywords = {Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media},
abstract = {With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.}
}
@article{ZHAO2020132,
title = {Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives},
journal = {Information Sciences},
volume = {515},
pages = {132-155},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309764},
author = {Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang},
keywords = {CPSS, Big data, Cloud computing, Privacy preserving, Clustering},
abstract = {Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.}
}
@article{ULLAH201981,
title = {Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review},
journal = {Journal of Systems and Software},
volume = {151},
pages = {81-118},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300172},
author = {Faheem Ullah and Muhammad {Ali Babar}},
keywords = {Big data, Cybersecurity, Quality attribute, Architectural tactic},
abstract = {Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}
@article{LI2020106143,
title = {Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke},
journal = {Environment International},
volume = {145},
pages = {106143},
year = {2020},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2020.106143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412020320985},
author = {Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre},
keywords = {PM, Machine learning, Air pollution exposure, Wildfires, Remote sensing, California, High spatiotemporal resolution},
abstract = {Introduction
Estimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.
Methods
Using ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1 km × 1 km) and temporal (weekly) resolution for a 10-year time span (2008–2017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.
Results
Ensemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54 μg/m3 (R2: 0.94) and test RMSE of 2.29 μg/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 μg/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.
Conclusion
Our method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.}
}
@article{LOPEZROBLES2019729,
title = {The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis},
journal = {Procedia Computer Science},
volume = {162},
pages = {729-736},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320551},
author = {José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).}
}
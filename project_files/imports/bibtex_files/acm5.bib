@inproceedings{10.1145/3173574.3174043,
author = {Verma, Nitya and Dombrowski, Lynn},
title = {Confronting Social Criticisms: Challenges When Adopting Data-Driven Policing Strategies},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174043},
doi = {10.1145/3173574.3174043},
abstract = {Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {policing, data practices, challenges, metis, law enforcement, data-driven organizations},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3414752.3414800,
author = {Huang, Qibao and Huang, Yiqi},
title = {The Significance of Urban Cockpit for Urban Brain Construction},
year = {2020},
isbn = {9781450388016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414752.3414800},
doi = {10.1145/3414752.3414800},
abstract = {The urban cockpit will comprehensively perceive and process all kinds of data in the city operation, establish the data chassis of the smart city, objectively, comprehensively and multi dimensionally display the operation situation of the city, and carry out early warning, prediction and scientific disposal of outstanding problems and emergencies in the city operation. Moreover, in the near future, with the introduction and use of 5G, artificial intelligence, big data, data Luan Sheng and edge computing in the city brain project, the city cockpit will also give the city managers and visitors a better and more beautiful feeling in the display effect (such as immersion and three-dimensional), thus accelerating the promotion and landing of the city brain project and promoting social governance Intelligent and professional, improve the level of comprehensive city governance, and change the transformation and upgrading of the city from extensive to precise and refined.},
booktitle = {2020 The 11th International Conference on E-Business, Management and Economics},
pages = {70–73},
numpages = {4},
keywords = {Urban cockpit, Data, Urban brain},
location = {Beijing, China},
series = {ICEME 2020}
}

@inproceedings{10.1145/2330601.2330605,
author = {Siemens, George},
title = {Learning Analytics: Envisioning a Research Discipline and a Domain of Practice},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330605},
doi = {10.1145/2330601.2330605},
abstract = {Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging "big data" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., "intelligent" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {4–8},
numpages = {5},
keywords = {research, data integration, collaboration, learning analytics, theory, practice, ethics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2939672.2945388,
author = {Zhu, Qiang and Guo, Songtao and Ogilvie, Paul and Liu, Yan},
title = {Business Applications of Predictive Modeling at Scale},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945388},
doi = {10.1145/2939672.2945388},
abstract = {Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2139–2140},
numpages = {2},
keywords = {predictive modeling, business analytics, machine learning, machine learning platforms},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{10.1145/3124391,
author = {Santana, Eduardo Felipe Zambom and Chaves, Ana Paula and Gerosa, Marco Aurelio and Kon, Fabio and Milojicic, Dejan S.},
title = {Software Platforms for Smart Cities: Concepts, Requirements, Challenges, and a Unified Reference Architecture},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3124391},
doi = {10.1145/3124391},
abstract = {Information and communication technologies (ICT) can be instrumental in progressing towards smarter city environments, which improve city services, sustainability, and citizens’ quality of life. Smart City software platforms can support the development and integration of Smart City applications. However, the ICT community must overcome current technological and scientific challenges before these platforms can be widely adopted. This article surveys the state of the art in software platforms for Smart Cities. We analyzed 23 projects concerning the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges and discussed future opportunities. This survey provides important references to help application developers, city managers, system operators, end-users, and Smart City researchers make project, investment, and research decisions.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {78},
numpages = {37},
keywords = {software platforms, Wireless sensor networks}
}

@inproceedings{10.1145/3106426.3106434,
author = {Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque},
title = {Intelligent Decision Support for Data Purchase},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106434},
doi = {10.1145/3106426.3106434},
abstract = {The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called "gut feeling" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {396–402},
numpages = {7},
keywords = {data purchase, computational intelligence, personalization, decision support},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3377458.3377464,
author = {Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen},
title = {Picture Management of Power Supply Safety Management System Based on Deep Learning Technology},
year = {2019},
isbn = {9781450372640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377458.3377464},
doi = {10.1145/3377458.3377464},
abstract = {With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, "time" and "accuracy" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.},
booktitle = {Proceedings of the 2019 5th International Conference on Systems, Control and Communications},
pages = {71–75},
numpages = {5},
keywords = {deep learning, Picture management, convolutional neural network},
location = {Wuhan, China},
series = {ICSCC 2019}
}

@inproceedings{10.1145/2522848.2522892,
author = {Neumann, Alexander and Schnier, Christian and Hermann, Thomas and Pitsch, Karola},
title = {Interaction Analysis and Joint Attention Tracking in Augmented Reality},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2522892},
doi = {10.1145/2522848.2522892},
abstract = {Multimodal research in human interaction has to consider a variety of factors, ranging from local short-time phenomena to complex interaction patterns. As of today, no single discipline engaged in communication research offers the methods and tools to investigate the full complexity continuum in a time-efficient way. A synthesis of qualitative and quantitative analysis is required to merge insights about micro-sequential structures with big data patterns. Using the example of a co-present dyadic negotiation analysis to combine methods offered by Conversation Analysis and Data Mining, we show how such a partnership can benefit each discipline and lead to insights as well as new hypotheses evaluation opportunities.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {165–172},
numpages = {8},
keywords = {data mining, multimodality, conversation analysis, interaction studies},
location = {Sydney, Australia},
series = {ICMI '13}
}

@inproceedings{10.1145/3408877.3432457,
author = {Fekete, Alan and Kay, Judy and R\"{o}hm, Uwe},
title = {A Data-Centric Computing Curriculum for a Data Science Major},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432457},
doi = {10.1145/3408877.3432457},
abstract = {Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {865–871},
numpages = {7},
keywords = {curriculum, data science},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/2611040.2611086,
author = {Omitola, Tope and Davies, John and Duke, Alistair and Glaser, Hugh and Shadbolt, Nigel},
title = {Linking Social, Open, and Enterprise Data},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611086},
doi = {10.1145/2611040.2611086},
abstract = {The new world of big data, of the LOD cloud, of the app economy, and of social media means that organisations no longer own, much less control, all the data they need to make the best informed business decisions. In this paper, we describe how we built a system using Linked Data principles to bring in data from Web 2.0 sites (LinkedIn, Salesforce), and other external business sites such as OpenCorporates, linking these together with pertinent internal British Telecommunications enterprise data into that enterprise data space. We describe the challenges faced during the implementation, which include sourcing the datasets, finding the appropriate "join points" from the individual datasets, as well as developing the client application used for data publication. We describe our solutions to these challenges and discuss the design decisions made. We conclude by drawing some general principles from this work.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {41},
numpages = {8},
keywords = {Architectures, Hypertext/Hypermedia, Semantic networks, Navigation, User issues},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2331801.2331803,
author = {Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {ASTERIX: Scalable Warehouse-Style Web Data Integration},
year = {2012},
isbn = {9781450312394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2331801.2331803},
doi = {10.1145/2331801.2331803},
abstract = {A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of "Big Data" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.},
booktitle = {Proceedings of the Ninth International Workshop on Information Integration on the Web},
articleno = {2},
numpages = {4},
keywords = {data-intensive computing, semistructured data, cloud computing, hyracks, ASTERIX},
location = {Scottsdale, Arizona, USA},
series = {IIWeb '12}
}

@inproceedings{10.1145/3290605.3300561,
author = {Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy},
title = {Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300561},
doi = {10.1145/3290605.3300561},
abstract = {In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {law enforcement, human trafficking, qualitative, needs analysis},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inbook{10.1145/3487664.3487719,
author = {Caruccio, Loredana and Cirillo, Stefano and Deufemia, Vincenzo and Polese, Giuseppe},
title = {Efficient Discovery of Functional Dependencies from Incremental Databases},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487719},
abstract = { With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {400–409},
numpages = {10}
}

@inproceedings{10.1145/3035918.3058740,
author = {Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
title = {A Demo of the Data Civilizer System},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058740},
doi = {10.1145/3035918.3058740},
abstract = {Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data "in the wild". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1639–1642},
numpages = {4},
keywords = {data integration, polystore queries, join path discovery, data discovery, data stitching, data cleaning},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3347146.3359090,
author = {Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong},
title = {DeepMM: Deep Learning Based Map Matching with Data Augmentation},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359090},
doi = {10.1145/3347146.3359090},
abstract = {Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {452–455},
numpages = {4},
keywords = {deep learning, data driven system, map matching},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@inbook{10.1145/3429889.3429938,
author = {Chen, Juan and Lu, Yan and Zhang, Ting and Ouyang, Zhaolian},
title = {Artificial Intelligence in Medicine in the United States, China and India},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429938},
abstract = {Objective: To compare the development status of artificial intelligence (AI) in medicine among the United States (US), China and India with bibliometric analysis. Methods: Articles involving AI in medicine published from 2015 to 2019 were retrieved on March 30, 2020 from Web of Science Core Collection. The country-level and the institution-level performance of the US, China and India in the field of AI in medicine were compared with indicators including the amount of papers, 5-year Compound Annual Growth Rate (CAGR) of the amount of papers, the amount of highly-cited papers, the proportion of highly-cited papers and the average citations per paper. In addition, the research hotspots and international cooperation of the three countries in recent 5 years were compared by conducting keywords co-occurrence analysis and co-authorship analysis in VOSviewer. Results: From 2015 to 2019, The US has published 7838 papers and 154 highly-cited papers in the field of AI in medicine, with an average citations per paper to be 9.3, and the proportion of highly-cited papers to be 2.0 %. China has output 6635 papers and 73 highly-cited papers in this field, with an average citations per paper to be 5.3, and the proportion of highly-cited papers to be 1.1%. India has output 3895 papers and 22 highly-cited papers in this field, with an average citations per paper to be 3.6, and the proportion of highly-cited papers to be 0.6%. The 5-year CAGR of the US, China and India in the period of 2015~2019 were 16.0%, 25.4% and 2.4%, respectively. At the institutional level, most of these indicators were significantly better for the US institutions than for Chinese and Indian ones. There were four research hotspots in this field, namely medical imaging technology, health big data mining, disease prediction with biomarkers and genetic information, and early diagnosis of neurological disease. The three countries focused on different hotspots, with China focusing relatively less on health big data mining, while the US and India being complementary to each other. As to international cooperation, the average links per paper to other countries were 0.60, 0.40 and 0.20, respectively, for the US, China and India. Conclusions: In the field of AI in medicine, the US, with a number of competitive institutions in AI and medical researches, is taking a definitely leading role, having conducted many innovative researches and cooperated extensively with other countries. China is taking the second leading role at the country level, with top institutions somewhat less productive than those in the US. India is the third productive country, with top institutions obvious less productive than those in the US, and with research hotspots exactly complementary to the US.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {257–264},
numpages = {8}
}

@inproceedings{10.1145/3291064.3291074,
author = {H G, Monika Rani and R, Sapna and Mishra, Shakti},
title = {An Investigative Study on the Quality Aspects of Linked Open Data},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291074},
doi = {10.1145/3291064.3291074},
abstract = {Linked Open Data refers to a set of best practices that empowers enterprises to publish and interlink their data using existing ontologies on the Semantic Web. The focus of linked open data is to move from document-based Web to a Web of interlinked data, created by typed links between data from different data sources. Linked open data expert group has taken cognizance of data quality importance, as the amount of linked data publications grown on the Web substantially. Measures have been taken to check the linked data quality. But, these measures are diverse in nature with respect to quality terms. This makes the comparison and evaluation difficult, leading to an incorrect selection of accurate data sources based on quality requirements. In this paper, we carried out an analysis on linked data, the quality of linked data, the frameworks to assess the quality of linked data and the challenges to achieve the quality of linked open data.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {33–39},
numpages = {7},
keywords = {Quality of linked open data, Linked open data, Semantic Web},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3486611.3491133,
author = {Chowdhury, Tahiya and Ding, Qizhen and Mandel, Ilan and Ju, Wendy and Ortiz, Jorge},
title = {Tracking Urban Heartbeat and Policy Compliance through Vision and Language-Based Sensing},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3491133},
doi = {10.1145/3486611.3491133},
abstract = {Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {302–306},
numpages = {5},
keywords = {urban sensing, computer vision and language, COVID-19},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3274192.3274219,
author = {Strey, Mateus Rambo and Pereira, Roberto and de Castro Salgado, Luciana C.},
title = {Human Data-Interaction: A Systematic Mapping},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274219},
doi = {10.1145/3274192.3274219},
abstract = {Big Data, e-Science and Internet of Things have contributed to increase the production, processing and storage of data, changing the way people deal and live with data. Although the problem is not new, the "human aspect" of data and the possible impact of Human-Data Interaction (HDI) in human life have been explored and discussed as an emerging research area. On the one hand, HDI offers plenty of opportunities for research and development, and on the other hand it demands characterization, grounding, critical discussions, empirical results and thinking tools to support research and practice. This paper presents a Systematic Mapping of Literature on HDI in Computer Science, identifying the different definitions for the area, elements or objects of investigation, contexts of application, stakeholders, etc. Based on 28 selected papers, results point out to a lack of definition or agreement on what HDI is, but suggest that there are different aspects that can characterize it, and allow identifying concerns and objects of study, such as privacy, ownership and transparency. Results suggest a demand for theoretical and methodological frameworks to support the understanding, design and evaluation of HDI via computing systems.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {27},
numpages = {12},
keywords = {Systematic Mapping Review, Human-Data Interaction, Human-Computer Interaction},
location = {Bel\'{e}m, Brazil},
series = {IHC 2018}
}

@inproceedings{10.1145/2791347.2791371,
author = {Efros, Pavel and Buchmann, Erik and Englhardt, Adrian and B\"{o}hm, Klemens},
title = {How to Quantify the Impact of Lossy Transformations on Change Detection},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791371},
doi = {10.1145/2791347.2791371},
abstract = {To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data, such as changes in the case of time series. Changes however are important for subsequent analyses. The impact of those modifications depends on the application scenario, and quantifying it is far from trivial. This is because a transformation can shift or modify existing changes or introduce new ones. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent change detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose change-detection approach. We have evaluated it with three real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {17},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@article{10.1145/3353401.3353406,
author = {Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel},
title = {Recognizing Experts on Social Media: A Heuristics-Based Approach},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3353401.3353406},
doi = {10.1145/3353401.3353406},
abstract = {Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.},
journal = {SIGMIS Database},
month = {jul},
pages = {66–84},
numpages = {19},
keywords = {data analytics., social media, expertise location}
}

@article{10.14778/2824032.2824070,
author = {Dasu, Tamraparni and Shkapenyuk, Vladislav and Srivastava, Divesh and Swayne, Deborah F.},
title = {FIT to Monitor Feed Quality},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824070},
doi = {10.14778/2824032.2824070},
abstract = {While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations.We make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival.FIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1728–1739},
numpages = {12}
}

@inproceedings{10.1145/2442952.2442955,
author = {Mehta, Paras and Voisard, Agn\`{e}s},
title = {Analysis of User Mobility Data Sources for Multi-User Context Modeling},
year = {2012},
isbn = {9781450316941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442952.2442955},
doi = {10.1145/2442952.2442955},
abstract = {Finding the right data source for research is a challenge that many of us face. Although we live in times where 'Open Data' and 'Big Data' have become buzzwords, getting hold of a reasonable size and quality dataset is often hard. When it comes to user data such as mobility data, this becomes even tougher due to privacy-related concerns. This paper briefly explains our research in the area of multi-user context modeling and presents some criteria that we believe are important while selecting a dataset for testing different approaches in this domain. To find the right dataset, some relevant publicly available human mobility datasets are examined using these criteria. The following are the datasets that have been analyzed: Microsoft Research GeoLife Trajectory Dataset, Tracking Delft I Pedestrian Trajectory Dataset, MIT Media Lab Reality Mining Dataset and LifeMap Dataset. Besides these, some other useful data sources for researchers have been cited.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
pages = {9–14},
numpages = {6},
keywords = {multi-user, dataset, mobility, context, model, situation},
location = {Redondo Beach, California},
series = {GEOCROWD '12}
}

@inproceedings{10.1145/2525314.2525455,
author = {McKenzie, Grant and Janowicz, Krzysztof and Adams, Benjamin},
title = {Weighted Multi-Attribute Matching of User-Generated Points of Interest},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525455},
doi = {10.1145/2525314.2525455},
abstract = {To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {440–443},
numpages = {4},
keywords = {similarity, POI, location-based services, conflation, point of interest, volunteered geographic information},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@inproceedings{10.1145/3442381.3449956,
author = {Yang, Longqi and Zhang, Liangliang and Tang, Yuhua},
title = {Scalable Auto-Weighted Discrete Multi-View Clustering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449956},
doi = {10.1145/3442381.3449956},
abstract = { Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches’ high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples’ local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3269–3278},
numpages = {10},
keywords = {multi-view clustering, binary coding, parameter selection, graph regularization},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/2685352,
author = {Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan},
title = {Editorial: “Business Process Intelligence: Connecting Data and Processes”},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2685352},
doi = {10.1145/2685352},
abstract = {This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {18e},
numpages = {7},
keywords = {business process intelligence, Process mining, performance analysis, compliance checking, process modeling}
}

@article{10.1145/3145623,
author = {Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping},
title = {A Dependable Time Series Analytic Framework for Cyber-Physical Systems of IoT-Based Smart Grid},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3145623},
doi = {10.1145/3145623},
abstract = {With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {aug},
articleno = {7},
numpages = {18},
keywords = {dependable time series analytics, IoT-based smart grid, sensor-network-regularization-based matrix factorization, cyber-physical-systems}
}

@inproceedings{10.1145/3447568.3448537,
author = {Sang, Go Muan and Xu, Lai and de Vrieze, Paul and Bai, Yuewei and Pan, Fangyu},
title = {Predictive Maintenance in Industry 4.0},
year = {2020},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448537},
doi = {10.1145/3447568.3448537},
abstract = {In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {29},
numpages = {11},
keywords = {Collaborative business process, Industry 4.0, FIWARE, Industrial data space, Predictive maintenance, Blockchain},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3396868.3402495,
author = {Kumar, Santosh},
title = {Sensitivity, Specificity, Generalizability, and Reusability Aspirations for Machine Learning (ML) Models in MHealth},
year = {2020},
isbn = {9781450380126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396868.3402495},
doi = {10.1145/3396868.3402495},
abstract = {Mobile sensor big data collected from smartphones, smartwatches, fitness trackers, and other wearables can be mined for signatures (called mHealth biomarkers) of subtle changes in daily behaviors (e.g., mobility, gait, sleep, etc.) and/or physiology (e.g., heart function, breathing, sweating, etc.). Clinical adoption of these mHealth biomarkers can lead to potent temporally-precise interventions, enabling patients to initiate and sustain the healthy lifestyle choices and treatment regimes that are necessary to prevent and/or successfully manage the growing burden of multiple chronic conditions.However, for any new biomarker to be successfully used for clinical diagnosis or treatment, its clinical utility must be established. mHealth biomarkers are usually derived by training a machine learning (ML) algorithm on mobile sensor data. The published models differ in feature construction (domain-derived features fed to a supervised ML model vs. data-driven features discovered by a deep learning (DL) model), data collection setting (lab vs. field), data selection and preparation (covering all twenty-four hours of the day vs. awake hours, vs. only when performing certain tasks), data labeling (retrospective self-reported aggregate labels vs. time-synchronized labels from first-person video), data size and diversity (e.g., number of participants, gender, ethnicity, age group, number of days, hours per day, etc.), experiment design (cross-validation vs. cross-subject validation), and performance (e.g., accuracy, F1 score, confusion matrix, AUC, etc.). As a result, there is wide diversity in published models on their potential for reusability, generalizability, and eventual clinical utility.This talk will describe an aspirational framework for specificity, sensitivity, generalizability, and reusability of mHealth biomarkers with some concrete performance targets so that they have a higher chance of widespread clinical utility. It will draw upon the presenter's decade-long transdisciplinary research experience in developing machine learning models to detect a wide variety of daily behaviors such as stress, speaking, smoking, and brushing from wearable physiological and inertial sensors.The talk will use the analogy of five nines (i.e., 99.999%) paradigm in the area of service-level agreements to quantify high-availability. Similar to how these managed services are expected to be available 24-7-365, with the downtime limited to 5.26 minutes per year, we can express the performance requirements of mHealth biomarkers that are expected to detect subtle signs of health and behaviour deterioration anytime and anywhere. Five nines guarantee for the detection of a health event (e.g., fall, stress) translates to one false positive every 100 person-days, if the model runs on 1,000 minutes of sensor data collected each day. Achieving five nines to claim the detection of non-event (e.g., smoking abstinence) is even more challenging, as there are several other failure scenarios for missing an event, in addition to model failure, such as the non-wearing of sensors when performing the event of interest, poor data quality, mismatch of model to where (on the body) and how the sensor is worn (e.g., smartwatch on non-dominant hand), battery failure, and data loss. To achieve generalizability, the model performance must be achieved on independent test data that covers all aspects of daily life, without any data selection. Finally, for the model to be used by others for real-life, the model should not only be accessible to the community, it should also be possible to train and test the model on different datasets by independent non-ML-expert researchers.},
booktitle = {Proceedings of Deep Learning for Wellbeing Applications Leveraging Mobile Devices and Edge Computing},
pages = {1},
numpages = {1},
keywords = {Machine Learning Models, Mobile Health (mHealth)},
location = {Toronto, ON, Canada},
series = {HealthDL'20}
}

@article{10.1145/3522591,
author = {Xiao, Houping and Wang, Shiyu},
title = {Toward Quality of Information Aware Distributed Machine Learning},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522591},
doi = {10.1145/3522591},
abstract = {In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this paper, we propose a novel consensus optimization framework for distributed machine learning that incorporates the crucial metric, quality of information. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
keywords = {distributed machine learning, quality of information}
}

@inproceedings{10.1145/3388176.3388210,
author = {Duan, Xuliang and Guo, Bing and Shen, Yan and Shen, Yuncheng and Dong, Xiangqian and Zhang, Hong},
title = {Research on Parallel Data Currency Rule Algorithms},
year = {2020},
isbn = {9781450377256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388176.3388210},
doi = {10.1145/3388176.3388210},
abstract = {Data currency is a temporal reference of data, which is related to the value of data and affects the results of data analysis and mining. The currency rules that reflect the time series features of data can be used not only for data repairing, but also for data quality evaluation. However, with the rapid growth and dynamic update of data volume, both the forms and algorithms of basic currency rule are facing severe challenges in application. Therefore, based on the research on data currency repairing, we extended the basic currency rule form, and proposed rule extraction and incremental updating algorithms that can run in parallel on dynamic data set. The experimental results show that, compared with non-parallel methods, the efficiency of parallel algorithms is significantly improved.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Information Science and System},
pages = {24–28},
numpages = {5},
keywords = {parallel algorithm, Data currency, data currency rule, dynamic data},
location = {Cambridge, United Kingdom},
series = {ICISS 2020}
}

@article{10.1145/3176648,
author = {Skorin-Kapov, Lea and Varela, Mart\'{\i}n and Ho\ss{}feld, Tobias and Chen, Kuan-Ta},
title = {A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3176648},
doi = {10.1145/3176648},
abstract = {Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {29},
numpages = {29},
keywords = {NFV, monitoring probes, encrypted traffic, data analytics, crowdsourcing, SDN, QoE modeling, QoE monitoring, QoE management}
}

@inproceedings{10.1145/2882903.2904442,
author = {Zhang, Ce and Shin, Jaeho and R\'{e}, Christopher and Cafarella, Michael and Niu, Feng},
title = {Extracting Databases from Dark Data with DeepDive},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2904442},
doi = {10.1145/2882903.2904442},
abstract = {DeepDive is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data --- scientific papers, Web classified ads, customer service notes, and so on --- were instead in a relational database, it would give analysts access to a massive and highly-valuable new set of "big data" to exploit.DeepDive is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used DeepDive to create databases with accuracy that meets that of human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by DeepDive represents a massive opportunity for industry, government, and scientific researchers.DeepDive is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {847–859},
numpages = {13},
keywords = {dark data, information extraction, data integration, knowledge base construction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {polystore, heterogeneous provenance data integration, Workflows interoperability},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@article{10.1145/2893482,
author = {Aiken, Peter},
title = {EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2893482},
doi = {10.1145/2893482},
abstract = {In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {35},
keywords = {data, business intelligence, information systems, chief data officer, chief information officer, BigCo, CDO, data governance, data architecture, policy, enterprise architecture, organizational design, data warehousing, CIO, Data management, data stewardship, data integration, conceptual modeling, strategy, enterprise data executive, IT management, analytics}
}

@inproceedings{10.1145/3357777.3357781,
author = {Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan},
title = {Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning},
year = {2019},
isbn = {9781450372312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357777.3357781},
doi = {10.1145/3357777.3357781},
abstract = {Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.},
booktitle = {Proceedings of the 2019 the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {7–11},
numpages = {5},
keywords = {Power monitoring, Proximity data averaging, Chebyshev theory, Data cleaning},
location = {Wenzhou, China},
series = {PRAI '19}
}

@article{10.1145/3461839,
author = {Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin},
title = {Incremental Group-Level Popularity Prediction in Online Social Networks},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3461839},
doi = {10.1145/3461839},
abstract = {Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {20},
numpages = {26},
keywords = {popularity prediction, tensor analysis, information diffusion, online social networks, Group level, incremental approach}
}

@article{10.14778/3282495.3282496,
author = {Bleifu\ss{}, Tobias and Bornemann, Leon and Johnson, Theodore and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh},
title = {Exploring Change: A New Dimension of Data Analytics},
year = {2018},
issue_date = {October 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3282495.3282496},
doi = {10.14778/3282495.3282496},
abstract = {Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.We envision a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {85–98},
numpages = {14}
}

@article{10.14778/3407790.3407802,
author = {Fan, Ju and Chen, Junyou and Liu, Tongyu and Shen, Yuwei and Li, Guoliang and Du, Xiaoyong},
title = {Relational Data Synthesis Using Generative Adversarial Networks: A Design Space Exploration},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407802},
doi = {10.14778/3407790.3407802},
abstract = {The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1962–1975},
numpages = {14}
}

@inproceedings{10.1145/2559206.2560469,
author = {Meyer, Jochen and Simske, Steven and Siek, Katie A. and Gurrin, Cathal G. and Hermens, Hermie},
title = {Beyond Quantified Self: Data for Wellbeing},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2560469},
doi = {10.1145/2559206.2560469},
abstract = {Sustaining our health and wellbeing requires lifelong efforts for prevention and healthy living. Continuously observing ourselves is one of the fundamental measures to be taken. While many devices support monitoring and quantifying our health behavior and health state, they all are facing the same trade-off: the higher the data quality is the higher are the efforts of data acquisition. However, for lifelong use, minimizing efforts for the user is crucial. Nowadays, few devices find a good balance between cost and value. In this interdisciplinary workshop we discuss how this trade-off can be approached by addressing three topics: understanding the user's information needs, exploring options for data acquisition, and discussing potential designs for life-long use.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {95–98},
numpages = {4},
keywords = {user oriented design, data analysis, wellbeing},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@article{10.1145/3275520,
author = {Sangogboye, Fisayo Caleb and Jia, Ruoxi and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {A Framework for Privacy-Preserving Data Publishing with Enhanced Utility for Cyber-Physical Systems},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3–4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3275520},
doi = {10.1145/3275520},
abstract = {Cyber-physical systems have enabled the collection of massive amounts of data in an unprecedented level of spatial and temporal granularity. Publishing these data can prosper big data research, which, in turn, helps improve overall system efficiency and resiliency. The main challenge in data publishing is to ensure the usefulness of published data while providing necessary privacy protection. In our previous work&nbsp;(Jia et al. 2017a), we presented a privacy-preserving data publishing framework (referred to as PAD hereinafter), which can guarantee k-anonymity while achieving better data utility than traditional anonymization techniques. PAD learns the information of interest to data users or features from their interactions with the data publishing system and then customizes data publishing processes to the intended use of data. However, our previous work is only applicable to the case where the desired features are linear in the original data record. In this article, we extend PAD to nonlinear features. Our experiments demonstrate that for various data-driven applications, PAD can achieve enhanced utility while remaining highly resilient to privacy threats.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {30},
numpages = {22},
keywords = {smart buildings, Privacy preservation, k-anonymity, deep learning, cyber-physical systems}
}

@inproceedings{10.1145/2964284.2976761,
author = {Tang, Mengfan and Pongpaichet, Siripen and Jain, Ramesh},
title = {Research Challenges in Developing Multimedia Systems for Managing Emergency Situations},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2976761},
doi = {10.1145/2964284.2976761},
abstract = {With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {938–947},
numpages = {10},
keywords = {micro-reports, eventshop, disaster, situation prediction, situation recognition},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.5555/3204979.3204980,
author = {Nie, Yu and Talburt, John and Li, Xinming and Xiao, Zhongdong},
title = {Chief Data Officer (CDO) Role and Responsibility Analysis},
year = {2018},
issue_date = {May 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {5},
issn = {1937-4771},
abstract = {While the number of organizations creating the role of Chief Data Officer (CDO) is increasing each year, the nature of the role is still emerging. CDO management responsibilities can vary widely from company to company. The study focuses on the various management responsibilities of the CDO role and their commonalities across organizations. After collecting and analyzing CDO job description from 411 organizations, we came to the following conclusions. Data analytics and business management are the most often cited and thus the most important management responsibilities for the CDO. Second is the management of data quality and data governance programs. Third, the CDO should keep abreast of new information technologies that could help firms design and execute an enterprise data strategy that coordinates the firm's business intelligence processes, leads to the development of new products, and acquires new customers through new data media.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {4–12},
numpages = {9},
keywords = {chief data officer (CDO), business management, role and responsibility analysis, data analytics}
}

@article{10.1145/3470918,
author = {Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan},
title = {AutoML to Date and Beyond: Challenges and Opportunities},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470918},
doi = {10.1145/3470918},
abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {175},
numpages = {36},
keywords = {Automated machine learning, interactive data science, predictive analytics, democratization of artificial intelligence}
}

@inproceedings{10.1145/2939672.2945365,
author = {Mierswa, Ingo},
title = {The Wisdom of Crowds: Best Practices for Data Prep &amp; Machine Learning Derived from Millions of Data Science Workflows},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945365},
doi = {10.1145/2939672.2945365},
abstract = {With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {411},
numpages = {1},
keywords = {wisdom of the crowds, machine learning tools, analytics, visual workflow, data visualization},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2832087.2832090,
author = {Lopez, M. Graham and Young, Jeffrey and Meredith, Jeremy S. and Roth, Philip C. and Horton, Mitchel and Vetter, Jeffrey S.},
title = {Examining Recent Many-Core Architectures and Programming Models Using SHOC},
year = {2015},
isbn = {9781450340090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832087.2832090},
doi = {10.1145/2832087.2832090},
abstract = {The Scalable HeterOgeneous Computing (SHOC) benchmark suite was released in 2010 as a tool to evaluate the stability and performance of emerging heterogeneous architectures and to compare different programming models for compute devices used in those architectures. Since then, high-performance computing (HPC) system architectures have increasingly incorporated both discrete and fused multi-core and many-core processors. The TOP500 list illustrates this trend: heterogeneous systems grew from a 3.4% to 18.0% share of the list between June 2010 and June 2015. Not only are there more heterogeneous systems on the TOP500 list today, those machines are responsible for a disproportionately large percentage of list's aggregate performance: as of June 2015, the performance share for heterogeneous systems has grown to 33.7%.Part of this shift toward heterogeneous architectures has stemmed from new products in the hardware accelerator market, such as Intel's Xeon Phi coprocessor, and improvements in the approaches for programming such accelerators. Existing approaches such as CUDA and OpenCL have become more powerful and easy to use, and directive-based programming models such as OpenACC, OpenMP 4.0, and Intel's Language Extensions for Offload (LEO) are rapidly gaining user acceptance. The benefits of these hardware and software advances are not limited to HPC; other problem domains such as "big data" are reaping the rewards also.The original SHOC benchmarks had adequate support for CUDA and OpenCL for graphics processing units, but did not support more recent programming models and devices. We extended SHOC to support evaluation of recent heterogeneous architectures and programming models such as OpenACC and LEO, and we added new benchmarks to increase SHOC's application domain coverage. In this paper, we describe our modifications to the stock SHOC distribution and present several examples of using our augmented version of SHOC for evaluation of recent heterogeneous architectures and programming models.},
booktitle = {Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems},
articleno = {3},
numpages = {12},
keywords = {performance, accelerators, benchmarking},
location = {Austin, Texas},
series = {PMBS '15}
}

@inproceedings{10.1145/2882903.2903736,
author = {Rheinl\"{a}nder, Astrid and Lehmann, Mario and Kunkel, Anja and Meier, J\"{o}rg and Leser, Ulf},
title = {Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903736},
doi = {10.1145/2882903.2903736},
abstract = {In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge. In this paper, we report our experiences from building such a system for comparing the "web view" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {759–771},
numpages = {13},
keywords = {focused crawling, massively parallel data analysis, information extraction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3469213.3470246,
author = {Zhou, Han and Zou, Wentao and Jiang, Yan and Shao, Qizhuan and Wu, Yang and Liu, Shuangquan},
title = {Rule-Based Data Verification Method in Electricity Spot Market},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470246},
doi = {10.1145/3469213.3470246},
abstract = {In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {46},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3334480.3382864,
author = {Sharbatdar, Nasim and Lamine, Yassine and Milord, Brigitte and Morency, Catherine and Cheng, Jinghui},
title = {Capturing the Practices, Challenges, and Needs of Transportation Decision-Makers},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382864},
doi = {10.1145/3334480.3382864},
abstract = {Transportation decision-makers from government agencies play an important role in addressing the traffic network conditions, which in turn, have a major impact on the well-being of citizens. The practices, challenges, and needs of this group of practitioners are less represented in the HCI literature. We address this gap through an interview study with 19 practitioners from Transports Qu\'{e}bec, a government agency responsible for transportation infrastructures in Qu\'{e}bec, Canada. We found that this group of decision-makers can most benefit from research about data analysis tools and platforms that (1) provide information to support data quality awareness, (2) are interoperable with other tools in the complex workflow of the practitioners, and (3) support intuitive and customizable visual analytics. These implications can also be informative to the design of tools supporting other decision-making tasks and domains.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {decision-maker, user study, decision-making, transportation management and planning, persona},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{10.1145/3439873,
author = {Neto, Nelson Novaes and Madnick, Stuart and Paula, Anchises Moraes G. De and Borges, Natasha Malara},
title = {Developing a Global Data Breach Database and the Challenges Encountered},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3439873},
doi = {10.1145/3439873},
abstract = {If the mantra “data is the new oil” of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {3},
numpages = {33},
keywords = {data aggregation, Cyber security, privacy, data breach, semantics of data}
}

@inproceedings{10.1145/3511716.3511730,
author = {Yang, Jie and Cao, Yong},
title = {The Classification of Gene Sequencer Based on Machine Learning},
year = {2022},
isbn = {9781450395687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511716.3511730},
doi = {10.1145/3511716.3511730},
abstract = {Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.},
booktitle = {2021 4th International Conference on E-Business, Information Management and Computer Science},
pages = {84–88},
numpages = {5},
keywords = {Machine learning, Classification of gene Sequencer, Quality of sequencing},
location = {Hong Kong, China},
series = {EBIMCS 2021}
}

@inbook{10.1145/3336191.3371871,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371871},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {877–880},
numpages = {4}
}

@inproceedings{10.1145/3312714.3312717,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Application of Attribute Correlation in Unsupervised Data Cleaning},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312714.3312717},
doi = {10.1145/3312714.3312717},
abstract = {Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.},
booktitle = {Proceedings of the 2019 the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {45–51},
numpages = {7},
keywords = {weak logic errors, Unsupervised data cleaning, attribute correlation, machine learning, minimum repair cost},
location = {Vienna, Austria},
series = {ICSLT 2019}
}

@inproceedings{10.1145/3110025.3110161,
author = {Ahmadov, Ahmad and Thiele, Maik and Lehner, Wolfgang and Wrembel, Robert},
title = {Context Similarity for Retrieval-Based Imputation},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110161},
doi = {10.1145/3110025.3110161},
abstract = {Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation.We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1017–1024},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@article{10.1145/3530991,
author = {Killeen, Patrick and Kiringa, Iluju and Yeap, Tet},
title = {Unsupervised Dynamic Sensor Selection for IoT-Based Predictive Maintenance of a Fleet of Public Transport Buses},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2691-1914},
url = {https://doi.org/10.1145/3530991},
doi = {10.1145/3530991},
abstract = {In recent years, big data produced by the Internet of Things (IoT) has enabled new kinds of useful applications. One such application is monitoring a fleet of vehicles in real-time to predict their remaining useful life. Consensus self-organized models (COSMO) approach is an example of a predictive maintenance system. The present work proposes a novel IoT-based architecture for predictive maintenance that consists of three primary nodes: namely, the vehicle node (VN), the server leader node (SLN), and the root node (RN), which enable on-board vehicle data processing, heavy-duty data processing, and fleet administration, respectively. A minimally viable prototype (MVP) of the proposed architecture was implemented and deployed to a local bus garage in Gatineau, Canada. The present work proposes an improved COSMO (ICOSMO), a fleet-wide unsupervised dynamic sensor selection algorithm. To analyze the performance of ICOSMO, a fleet simulation was implemented. The J1939 data gathered from a hybrid bus was used to generate synthetic data in the simulations. Simulation results that compared the performance of the COSMO and ICOSMO approaches revealed that in general ICOSMO improves the average area under the curve of COSMO by approximately 1.5% when using the Cosine distance and 0.6% when using the Hellinger distance.},
note = {Just Accepted},
journal = {ACM Trans. Internet Things},
month = {apr},
keywords = {J1939, machine learning, Design, predictive maintenance, controller area network, internet of things, fleet management, sensor selection, Performance, Algorithms, predictive analytics}
}

@article{10.14778/3317315.3317318,
author = {Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren},
title = {Deducing Certain Fixes to Graphs},
year = {2019},
issue_date = {March 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3317315.3317318},
doi = {10.14778/3317315.3317318},
abstract = {This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and "incrementally" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {752–765},
numpages = {14}
}

@article{10.14778/2536274.2536304,
author = {Civili, Cristina and Console, Marco and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio},
title = {Mastro Studio: Managing Ontology-Based Data Access Applications},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536304},
doi = {10.14778/2536274.2536304},
abstract = {Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1314–1317},
numpages = {4}
}

@inbook{10.1145/3366424.3383117,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383117},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {317–319},
numpages = {3}
}

@article{10.1145/3187009.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3187009.3177739},
doi = {10.1145/3187009.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {635–647},
numpages = {13}
}

@article{10.1145/3177732.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3177732.3177739},
doi = {10.1145/3177732.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {635–647},
numpages = {13}
}

@inproceedings{10.1145/3129416.3129441,
author = {Cohen, L.},
title = {Impacts of Business Intelligence on Population Health: A Systematic Literature Review},
year = {2017},
isbn = {9781450352505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129416.3129441},
doi = {10.1145/3129416.3129441},
abstract = {"Business Intelligence" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists},
articleno = {9},
numpages = {9},
keywords = {population health, systematic literature review, business intelligence},
location = {Thaba 'Nchu, South Africa},
series = {SAICSIT '17}
}

@inproceedings{10.1145/3284179.3284322,
author = {Dorn, Amelie and Wandl-Vogt, Eveline and Palfinger, Thomas and D\'{\i}az, Jos\'{e} Luis Preza and Piringer, Barbara and Schatek, Alexander and Zoubek, Rainer},
title = {Applying Commercial Computer Vision Tools to Cope with Uncertainties in a Citizen-Driven Archive: The Case Study Topothek@exploreAT!},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284322},
doi = {10.1145/3284179.3284322},
abstract = {Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {845–851},
numpages = {7},
keywords = {computer vision, Digital Humanities, uncertainty, citizen-driven archive, AI},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@article{10.5555/3344081.3344082,
author = {Ye, Yumeng and Talburt, John R.},
title = {Generating Synthetic Data to Support Entity Resolution Education and Research},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {7},
issn = {1937-4771},
abstract = {Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {12–19},
numpages = {8}
}

@inproceedings{10.1145/3438872.3439085,
author = {Wu, Yi and Song, Yan and Yang, Hongshan},
title = {Intelligent Distributed Web Crawler Based on Attention Mechanism},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439085},
doi = {10.1145/3438872.3439085},
abstract = {With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {229–233},
numpages = {5},
keywords = {Deep Learning, Artificial Intelligence, Intelligent Web Crawler, Distributed Framework},
location = {Shanghai, China},
series = {RICAI 2020}
}

@inproceedings{10.1145/3491396.3506519,
author = {Li, Xiang and Zhang, Zhaoqian and Zhao, Zhigang and Wu, Lu and Huo, Jidong and Zhang, Jian and Wang, Yinglong},
title = {ECNN: One Online Deep Learning Model for Streaming Ocean Data Prediction},
year = {2022},
isbn = {9781450391603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491396.3506519},
doi = {10.1145/3491396.3506519},
abstract = {Despite been extensively explored, current techniques in sequential data modeling and prediction are generally designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient but also poorly scalable in real-world applications, especially for real-time intelligent ocean data quality control (QC), where the data arrives sequentially and the QC should be conducted in real time. This paper investigates the online learning for ocean data streams by resolving two main challenges: (i) how to develop a deep learning model to capture the complex ocean data distribution that could evolve dynamically, namely tackling the 'concept drift' problem for non-stationary time series; (ii) how to develop a deep learning model that can dynamically adapt its structure from shallow to deep with the inflow of the data to overcome under-fitting problem, namely tackling the 'model selection' problem. To tackle these challenges, we propose one Evolutive Convolutional Neural Network (ECNN) that dynamically re-weighting the sub-structure of the model from data streams in a sequential or online learning fashion, by which the capacity scalability and sustainability are introduced into the model. The experiments on real ocean observation data verify the effectiveness of our model. As far as we know, it is the first work that introduce online deep learning techniques into ocean data prediction research.},
booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
pages = {170–175},
numpages = {6},
keywords = {Ocean Data, CNN, Time Series Prediction, Online Learning, Attention Network},
location = {Jinan, China},
series = {ACM ICEA '21}
}

@inproceedings{10.1145/3292500.3332297,
author = {Shi, Xiaolin and Dmitriev, Pavel and Gupta, Somit and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3332297},
doi = {10.1145/3292500.3332297},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [18, 22]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3189–3190},
numpages = {2},
keywords = {online metrics, a/b testing, user experience evaluation, controlled experiments},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@book{10.1145/3310205,
author = {Ilyas, Ihab F. and Chu, Xu},
title = {Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.}
}

@article{10.1145/3326164,
author = {Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet},
title = {Efficient User Guidance for Validating Participatory Sensing Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326164},
doi = {10.1145/3326164},
abstract = {Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely &lt;u&gt;G&lt;/u&gt;eneralised &lt;u&gt;A&lt;/u&gt;uto &lt;u&gt;R&lt;/u&gt;egressive &lt;u&gt;C&lt;/u&gt;onditional &lt;u&gt;H&lt;/u&gt;eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {37},
numpages = {30},
keywords = {trust management, probabilistic database, Participatory sensing}
}

@inproceedings{10.1145/3447548.3469441,
author = {Zhu, Feida and Pei, Jian},
title = {The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD2021): Joint Workshop with SIGKDD 2021 Trust Day},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469441},
doi = {10.1145/3447548.3469441},
abstract = {Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed "Trust Day" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of "trust" in a highly interdisciplinary manner.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4185–4186},
numpages = {2},
keywords = {data auditing, data asset, data pricing, distributed ledger technology, privacy, data governance},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3307339.3342169,
author = {Perkins, Patrick and Heber, Steffen},
title = {Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to Identify Corrupted Ribo-Seq and RNA-Seq Samples},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3342169},
doi = {10.1145/3307339.3342169},
abstract = {RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {457–465},
numpages = {9},
keywords = {negative selection algorithm, ribosome profiling, machine learning, anomaly detection, rna-seq, sample quality},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/2846012.2846026,
author = {Lipuntsov, Yuri P.},
title = {On the Relationship Between the Information and Analytical Components in the Shared E-Government},
year = {2015},
isbn = {9781450340700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846012.2846026},
doi = {10.1145/2846012.2846026},
abstract = {Economic and mathematical models and information models are the two main components of the information environment. These two components perform different functions - the information models is responsible for the data quality, the data delivery, and the economic and mathematical models defines data mining and intelligence. This category of models is constantly being developed often independently of each other. The information models as methods of data presentation and data integration are considered as separate from economic and mathematical modeling area. This paper discuss the relationship between the two types of models as sequence of steps for models development with the horizontal and vertical traceability. The connection between two types of models presented as the reflection of the real word logic to the data layer and after that to the software layer, and the feedback from the application to the information and to the operation logic.},
booktitle = {Proceedings of the 2015 2nd International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {109–115},
numpages = {7},
keywords = {Economic and mathematical modeling, Data exchange, Shared environment, Information modeling, Simulation},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '15}
}

@inproceedings{10.1145/3491101.3503724,
author = {Pine, Kathleen and Bossen, Claus and Holten M\o{}ller, Naja and Miceli, Milagros and Lu, Alex Jiahong and Chen, Yunan and Horgan, Leah and Su, Zhaoyuan and Neff, Gina and Mazmanian, Melissa},
title = {Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503724},
doi = {10.1145/3491101.3503724},
abstract = {In the wake of the hype around big data, artificial intelligence, and “data-drivenness,” much attention has been paid to developing novel tools to capitalize upon the deluge of data being recorded and gathered automatically through IT systems. While much of this literature tends to overlook the data itself—sometimes even characterizing it as “data exhaust” that is readily available to be fed into algorithms, which will unlock the insights held within it—a growing body of literature has recently been directed at the (often intensive and skillful) work that goes into creating, collecting, managing, curating, analyzing, interpreting, and communicating data. These investigations detail the practices and processes involved in making data useful and meaningful so that aims of becoming ‘data-driven’ or ‘data-informed’ can become real. Further, In some cases, increased demands for data work have led to the formation of new occupations, whereas at other times data work has been added to the task portfolios of existing occupations and professions, occasionally affecting their core identity. Thus, the evolving forms of data work are requiring individual and organizational resources, new and re-tooled practices and tools, development of new competences and skills, and creation of new functions and roles. While differences exist across the global North and the global South experience of data work, such factors of data production remain paramount even as they exist largely for the benefit of the data-driven system [21, 32]. This one-day workshop will investigate existing and emerging tasks of data work. Further, participants will seek to understand data work as it impacts: individual data workers; occupations tasked with data work (existing and emerging); organizations (e.g. changing their skill-mix and infrastructuring to support data work); and teaching institutions (grappling with incorporation of data work into educational programs). Participants are required to submit a position paper or a case study drawn from their research to be reviewed and accepted by the organizing committee (submissions should be up to four pages in length). Upon acceptance, participants will read each other's paper, prepare to shortly present and respond to comments by two discussants and other participants. Subsequently, the workshop will focus on developing a set of core processes and tasks as well as an outline of a research agenda for a CHI-perspective on data work in the coming years.},
booktitle = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
articleno = {87},
numpages = {6},
keywords = {Datafication, Occupations, Labor, Data-Driven, Data Work},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/2944165.2944172,
author = {El-Atawy, Sameh S. and Khalefa, Mohamed E.},
title = {Building an Ontology-Based Electronic Health Record System},
year = {2016},
isbn = {9781450342933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2944165.2944172},
doi = {10.1145/2944165.2944172},
abstract = {Electronic health record (EHR) solutions are complex, spanning multiple specialties and domains of expertise. These systems need to handle clinical concepts, temporal data, documents, and financial transactions, which leads to a large code base that is tightly coupled with data models and inherently hard to maintain. These difficulties can greatly increase the cost of developing EHR systems, result in a high failure rate of implementation, and threaten investments in this sector. Moreover, due to the wide variance in the level of detail across different settings, data exchange is becoming a serious problem, further increasing the cost of development and maintenance.To overcome these issues, we adopt ontologies to model our proposed EHR solution, not only allowing code reuse; but also enabling later extension and customization. Adopting software factory techniques, we build tools to transform ontological models into deployment-ready code. This automatically provides handling of data persistence, access, and exchange. Business logic is expressed as ontology-based process flows and rules, ensuring data quality and supporting special needs. This logic is enforced transparently and can be modified on the fly. We optimized the user experience by facilitating fast data entry and retrieval.In this paper, we present the requirements of an effective EHR solution, explain the techniques we employed, describe the main modules of our proposed system, and discuss the technical decisions we made.},
booktitle = {Proceedings of the 2nd Africa and Middle East Conference on Software Engineering},
pages = {40–45},
numpages = {6},
keywords = {Ontology, Query Language, Electronic Health Record Management},
location = {Cairo, Egypt},
series = {AMECSE '16}
}

@inproceedings{10.1145/3219819.3219914,
author = {Samel, Karan and Miao, Xu},
title = {Active Deep Learning to Tune Down the Noise in Labels},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219914},
doi = {10.1145/3219819.3219914},
abstract = {The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {685–694},
numpages = {10},
keywords = {denoising, deep neural networks, classification, active learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3460000,
author = {Thirumuruganathan, Saravanan and Kunjir, Mayuresh and Ouzzani, Mourad and Chawla, Sanjay},
title = {Automated Annotations for AI Data and Model Transparency},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3460000},
doi = {10.1145/3460000},
abstract = {The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {9},
keywords = {data cleaning, machine learning, Data transparency}
}

@article{10.14778/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2809974.2809975},
doi = {10.14778/2809974.2809975},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1118–1129},
numpages = {12}
}

@article{10.5555/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1118–1129},
numpages = {12}
}

@article{10.14778/3297753.3297757,
author = {Li, Yanying and Sun, Haipei and Dong, Boxiang and Wang, Hui (Wendy)},
title = {Cost-Efficient Data Acquisition on Online Data Marketplaces for Correlation Analysis},
year = {2018},
issue_date = {December 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3297753.3297757},
doi = {10.14778/3297753.3297757},
abstract = {Incentivized by the enormous economic profits, the data marketplace platform has been proliferated recently. In this paper, we consider the data marketplace setting where a data shopper would like to buy data instances from the data marketplace for correlation analysis of certain attributes. We assume that the data in the marketplace is dirty and not free. The goal is to find the data instances from a large number of datasets in the marketplace whose join result not only is of high-quality and rich join informativeness, but also delivers the best correlation between the requested attributes. To achieve this goal, we design DANCE, a middleware that provides the desired data acquisition service. DANCE consists of two phases: (1) In the off-line phase, it constructs a two-layer join graph from samples. The join graph includes the information of the datasets in the marketplace at both schema and instance levels; (2) In the online phase, it searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {362–375},
numpages = {14}
}

@inproceedings{10.1145/3414274.3414505,
author = {Fu, Qingwen and Zhu, Jiahui and Chen, Yuepeng and Wan, Jintao and He, Bin},
title = {An Automatic Learning Model for Trajectory Outlier Detection},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414505},
doi = {10.1145/3414274.3414505},
abstract = {The rapid development of global positioning system has given birth to a large number of spatial-temporal data, and there are many outliers of points obviously in these trajectory data. It is very important to detect the outliers in the trajectory to improve the data quality and accuracy of trajectory mining. In this paper, we propose a trajectory outlier detection algorithm based on bi-directional long short-term memory model and attention mechanism. Firstly, an eight-dim eigenvector is extracted from each point of trajectory, and then a two-layer bi-directional long short-term memory model is constructed. Finally, representing the trajectory points in an interactive way which is called attention mechanism. The input of the model is the trajectory point with a certain length, and the output is the type of the trajectory point. The model can automatically learn the difference between the normal point and the adjacent abnormal point with motion features. Experimental dataset based on real trajectory data of taxi from Beijing, and results showed that the performance of this algorithm is significantly better than constant speed threshold method or classical machine learning classification. Especially the precision and recall reaches 0.93 and 0.90 separately, which proves the effectiveness of this algorithm.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {220–226},
numpages = {7},
keywords = {attention mechanism, spatial-temporal data, outlier detection, Bi-LSTM},
location = {Xiamen, China},
series = {DSIT 2020}
}

@article{10.1109/TNET.2021.3105427,
author = {Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming},
title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing},
year = {2022},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3105427},
doi = {10.1109/TNET.2021.3105427},
abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {176–189},
numpages = {14}
}

@inproceedings{10.1145/3184558.3192324,
author = {Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{e}s, Julien},
title = {TempWeb 2018 Chairs' Welcome and Organization},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3192324},
doi = {10.1145/3184558.3192324},
abstract = {Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1729–1730},
numpages = {2},
keywords = {systematic exploitation of web archives, large scale data storage, web science, web scale data analytics, data quality metrics, web trends, time aware web archiving, content evolution on the web, temporal web analytics, web spam evolution, web dynamics, terminology evolution, community detection and evolution, distributed data analytics, topic mining, data aggregation, large scale data processing},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3383783.3383793,
author = {Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou},
title = {Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing Data with Severe Dropout},
year = {2019},
isbn = {9781450372183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383783.3383793},
doi = {10.1145/3383783.3383793},
abstract = {As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.},
booktitle = {Proceedings of the 2019 6th International Conference on Bioinformatics Research and Applications},
pages = {55–62},
numpages = {8},
keywords = {model-free, directional dependency, single-cell sequencing},
location = {Seoul, Republic of Korea},
series = {ICBRA '19}
}

@inproceedings{10.1145/3137133.3137140,
author = {Jia, Ruoxi and Sangogboye, Fisayo Caleb and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {PAD: Protecting Anonymity in Publishing Building Related Datasets},
year = {2017},
isbn = {9781450355445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3137133.3137140},
doi = {10.1145/3137133.3137140},
abstract = {The diffusion of low-cost sensor network technologies in smart buildings has enabled the collection of massive amounts of data regarding indoor environments, energy use and occupants, which, in turn, creates opportunities for knowledge- and information-based building management. Driven by benefits mutual to occupants, building managers, and research communities, there is a demand for data publication to foster more sophisticated and robust models and algorithms. Data in the original form, however, contains sensitive information about occupants' behavioral patterns, and publishing such data will violate individuals' privacy. The current practice on publishing building-related datasets relies primarily on policies for dictating which types of data can be published and agreements on the use of published data. This approach alone provides insufficient protection as it does not prevent privacy breaches from occurring in the first place.In this paper, we present PAD, which to our knowledge is the first system that provides a technological solution for publishing building related datasets in a privacy-preserving manner while maintaining high data quality. PAD is able to offer a strong anonymity guarantee by perturbing data records. The unique feature of PAD is that it offers an interface to incorporate dataset users into the loop of data publication and customizes the perturbation such that useful information in the dataset can be better retained. We study the efficacy of PAD using occupancy and plug load data collected in real buildings. The experiments demonstrate that PAD can achieve high resilience to privacy threats without introducing any significant data fidelity penalties.},
booktitle = {Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments},
articleno = {4},
numpages = {10},
keywords = {occupancy privacy, k-anonymity, clustering, convex optimization},
location = {Delft, Netherlands},
series = {BuildSys '17}
}

@inproceedings{10.1145/2623330.2623685,
author = {Li, Furong and Lee, Mong Li and Hsu, Wynne},
title = {Entity Profiling with Varying Source Reliabilities},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623685},
doi = {10.1145/2623330.2623685},
abstract = {The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1146–1155},
numpages = {10},
keywords = {record linkage, truth discovery, source reliability, entity profiling},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/2808198,
author = {Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.},
title = {Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2808198},
doi = {10.1145/2808198},
abstract = {Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {18},
numpages = {22},
keywords = {trajectory, recruitment, Participatory sensing, tensor, deployment, DTA}
}

@inproceedings{10.1145/3012071.3012077,
author = {Madera, Cedrine and Laurent, Anne},
title = {The next Information Architecture Evolution: The Data Lake Wave},
year = {2016},
isbn = {9781450342674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012071.3012077},
doi = {10.1145/3012071.3012077},
abstract = {Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system.In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.},
booktitle = {Proceedings of the 8th International Conference on Management of Digital EcoSystems},
pages = {174–180},
numpages = {7},
keywords = {data lakes, data warehouses, data laboratory, internet of things, data governance, data reservoirs, digital transformation, data lab},
location = {Biarritz, France},
series = {MEDES}
}

@article{10.1145/2430456.2430472,
author = {Dong, Xin Luna and Dragut, Eduard Constantin},
title = {10th International Workshop on Quality in Databases: QDB 2012},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2430456.2430472},
doi = {10.1145/2430456.2430472},
journal = {SIGMOD Rec.},
month = {jan},
pages = {55–59},
numpages = {5}
}

@inproceedings{10.1145/2463676.2465337,
author = {Golab, Lukasz and Johnson, Theodore},
title = {Data Stream Warehousing},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465337},
doi = {10.1145/2463676.2465337},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {949–952},
numpages = {4},
keywords = {data streams, real-time analytics, data warehousing},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3041021.3053059,
author = {Tao, Shibo and Wang, Xiaorong and Huang, Weijing and Chen, Wei and Wang, Tengjiao and Lei, Kai},
title = {From Citation Network to Study Map: A Novel Model to Reorganize Academic Literatures},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053059},
doi = {10.1145/3041021.3053059},
abstract = {As the number of academic papers and new technologies soars, it has been increasingly difficult for researchers, especially beginners, to enter a new research field. Researchers often need to study a promising paper in depth to keep up with the forefront of technology. Traditional Query-Oriented study method is time-consuming and even tedious. For a given paper, existent academic search engines like Google Scholar tend to recommend relevant papers, failing to reveal the knowledge structure. The state-of-the-art Map-Oriented study methods such as AMiner and AceMap can structure scholar information, but they're too coarse-grained to dig into the underlying principles of a specific paper. To address this problem, we propose a Study-Map Oriented method and a novel model called RIDP (Reference Injection based Double-Damping PageRank) to help researchers study a given paper more efficiently and thoroughly. RIDP integrates newly designed Reference Injection based Topic Analysis method and Double-Damping PageRank algorithm to mine a Study Map out of massive academic papers in order to guide researchers to dig into the underlying principles of a specific paper. Experiment results on real datasets and pilot user studies indicate that our method can help researchers acquire knowledge more efficiently, and grasp knowledge structure systematically.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1225–1232},
numpages = {8},
keywords = {study map, double-damping pagerank, reference injection, topic analysis, academic papers},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3333165.3333180,
author = {Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind},
title = {Open Government Data: Towards a Comparison of Data Lifecycle Models},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333180},
doi = {10.1145/3333165.3333180},
abstract = {Government, through Open Government Data "OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {15},
numpages = {6},
keywords = {data lifecycle, Open Government Data, data value creation},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@inproceedings{10.1145/3482632.3484077,
author = {Wang, Chunxia and Xie, Jian},
title = {Constructing a Computer Model for Discipline Data Governance Using the Contingency Theory and Data Mining},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484077},
doi = {10.1145/3482632.3484077},
abstract = {Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the "precision" and "science" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1967–1970},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3035918.3054772,
author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
title = {Data Profiling: A Tutorial},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054772},
doi = {10.1145/3035918.3054772},
abstract = {is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1747–1751},
numpages = {5},
keywords = {dependency discovery, data profiling, data exploration},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3444370.3444575,
author = {Zhang, Bo and Kong, Dehua},
title = {Dynamic Estimation Model of Insurance Product Recommendation Based on Naive Bayesian Model},
year = {2020},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444575},
doi = {10.1145/3444370.3444575},
abstract = {Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {219–224},
numpages = {6},
keywords = {recommendation, insurance products, dynamic estimation, Naive Bayes},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3384544.3384588,
author = {Nugroho, Heru and Gumilang, Soni Fajar Surya},
title = {Recommendations for Improving Data Management Process in Government of Bandung Regency Using COBIT 4.1 Framework},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384588},
doi = {10.1145/3384544.3384588},
abstract = {Data is an valuable asset that potentially provides substantial benefits for the government and society. To make the performance of local government apparatus runs optimally and the public gets the best service, the government of Bandung Regency strives to improve data management. The initial stage of optimizing data management is the assessment of the maturity level in managing data (DS-11) using COBIT 4.1. Base on the assessment maturity level for DS-11, the government of Bandung Regency needs to raise the level from 2.46 (Repeatable but Intuitive) to 3.0 (Defined). Recommendations given to improve data management in Government with focuses on maintaining the completeness, accuracy, availability, and protection of data.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {57–61},
numpages = {5},
keywords = {DS-11, Recommendations, Maturity, COBIT 4.1, Data},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/2737817.2737831,
author = {Pedersen, Torben Bach and Castellanos, Malu and Dayal, Umesh},
title = {Report on the Seventh International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE 2013)},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2737817.2737831},
doi = {10.1145/2737817.2737831},
journal = {SIGMOD Rec.},
month = {feb},
pages = {55–58},
numpages = {4}
}

@article{10.1145/3190579,
author = {Bertino, Elisa and Jahanshahi, Mohammad R.},
title = {Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190579},
doi = {10.1145/3190579},
journal = {J. Data and Information Quality},
month = {may},
articleno = {1},
numpages = {6},
keywords = {Civil engineering, device swarms, edge computing}
}

@inproceedings{10.1145/3368756.3368965,
author = {Rhazal, Oumaima El and Tomader, Mazri},
title = {Study of Smart City Data: Categories and Quality Challenges},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3368965},
doi = {10.1145/3368756.3368965},
abstract = {Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {4},
numpages = {7},
keywords = {smart city, internet of things (IoT), quality of information (QoI)},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@book{10.1145/3453538,
author = {ACM Data Science Task Force},
title = {Computing Competencies for Undergraduate Data Science Curricula},
year = {2021},
isbn = {9781450390606},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@article{10.1145/3015456,
author = {Cao, Longbing},
title = {Data Science: Challenges and Directions},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3015456},
doi = {10.1145/3015456},
abstract = {While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.},
journal = {Commun. ACM},
month = {jul},
pages = {59–68},
numpages = {10}
}

@article{10.1145/2983463,
author = {Peek, Geerten and Taspinar, Ahmet},
title = {One Thousand Interviews},
year = {2016},
issue_date = {Fall 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2983463},
doi = {10.1145/2983463},
abstract = {How customer insights keep one company agile, and challenge these data scientist to stay ahead in an ever-changing world.},
journal = {XRDS},
month = {sep},
pages = {11–12},
numpages = {2}
}


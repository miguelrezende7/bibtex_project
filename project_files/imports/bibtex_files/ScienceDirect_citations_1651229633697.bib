@incollection{SIMON201577,
title = {Chapter 8 - Considerations for the Big Data Era},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Warehousing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {77-82},
year = {2015},
isbn = {978-0-12-801540-7},
doi = {https://doi.org/10.1016/B978-0-12-801540-7.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015407000081},
author = {Alan Simon},
keywords = {Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW},
abstract = {The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.}
}
@article{CHEN201798,
title = {Data quality of electricity consumption data in a smart grid environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {75},
pages = {98-105},
year = {2017},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.10.054},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116307109},
author = {Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu},
keywords = {Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid},
abstract = {With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.}
}
@article{WANG2015782,
title = {Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures},
journal = {Computer Law & Security Review},
volume = {31},
number = {6},
pages = {782-792},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001296},
author = {Zhong Wang and Qian Yu},
keywords = {Personal data, Privacy trust, Questionnaires, Interview, Big data},
abstract = {Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.}
}
@article{KNEPPER20151504,
title = {Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing},
journal = {Procedia Computer Science},
volume = {51},
pages = {1504-1513},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.340},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011485},
author = {Richard Knepper and Matthew Standish and Matthew Link},
keywords = {Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data},
abstract = {We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.}
}
@article{ZHAO20171085,
title = {An optimization model for green supply chain management by using a big data analytic approach},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1085-1097},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616300579},
author = {Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang},
keywords = {Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis},
abstract = {This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.}
}
@incollection{KRISHNAN2013199,
title = {Chapter 10 - Integration of Big Data and Data Warehousing},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {199-217},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000106},
author = {Krish Krishnan},
keywords = {Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework},
abstract = {The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?}
}
@article{HAZEN2016592,
title = {Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {592-598},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S036083521630225X},
author = {Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone},
keywords = {Big data, Predictive analytics, Supply chain management},
abstract = {Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.}
}
@incollection{EBBELS2019329,
title = {Chapter 11 - Big Data and Databases for Metabolic Phenotyping},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {329-367},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938000116},
author = {Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen},
keywords = {Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI},
abstract = {Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.}
}
@article{SIVARAJAH2017263,
title = {Critical analysis of Big Data challenges and analytical methods},
journal = {Journal of Business Research},
volume = {70},
pages = {263-286},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S014829631630488X},
author = {Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody},
keywords = {Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review},
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.}
}
@incollection{HUGHES2016293,
title = {Chapter 13 - Surface Solutions Using Data Virtualization and Big Data},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing for the Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {293-327},
year = {2016},
isbn = {978-0-12-396464-9},
doi = {https://doi.org/10.1016/B978-0-12-396464-9.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964649000138},
author = {Ralph Hughes},
keywords = {Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive},
abstract = {Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.}
}
@incollection{FIESCHI2018197,
title = {16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters.},
editor = {Marius Fieschi},
booktitle = {Health Data Processing},
publisher = {Elsevier},
pages = {197-212},
year = {2018},
isbn = {978-1-78548-287-8},
doi = {https://doi.org/10.1016/B978-1-78548-287-8.50016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548287850016X},
author = {Marius Fieschi},
keywords = {Data processing, Data-sharing, e-health, Epidemiology, Health security, Monitoring systems, Preventive action, Public health, SurSaUD system},
abstract = {Abstract:
The approaches used by epidemiologists are diverse: they range from “field studies” for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.}
}
@article{BARASH201510,
title = {Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points},
journal = {Applied & Translational Genomics},
volume = {4},
pages = {10-13},
year = {2015},
issn = {2212-0661},
doi = {https://doi.org/10.1016/j.atg.2015.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212066115000046},
author = {Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood},
abstract = {A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.}
}
@article{BJORNSDOTTIR20181195,
title = {Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry},
journal = {Research in Social and Administrative Pharmacy},
volume = {14},
number = {12},
pages = {1195-1202},
year = {2018},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S155174111830127X},
author = {Ingunn Björnsdottir and Guri Birgitte Verne},
abstract = {Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.}
}
@article{PONTORIERO2021106239,
title = {Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {208},
pages = {106239},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106239},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003138},
author = {Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese},
keywords = {FDOPA, PET, quality control, QC, convolutional neural networks},
abstract = {ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.}
}
@article{ROMERO2015336,
title = {Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem},
journal = {Information Systems},
volume = {54},
pages = {336-356},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001458},
author = {Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons},
keywords = {Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation},
abstract = {In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).}
}
@article{MAIER201797,
title = {Big data in large-scale systemic mouse phenotyping},
journal = {Current Opinion in Systems Biology},
volume = {4},
pages = {97-104},
year = {2017},
note = {Big data acquisition and analysis • Pharmacology and drug discovery},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300525},
author = {Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}},
abstract = {Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.}
}
@article{ABBASIAN201829,
title = {Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle},
journal = {Journal of Industrial Information Integration},
volume = {10},
pages = {29-38},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300869},
author = {Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett},
keywords = {External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering},
abstract = {Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.}
}
@article{DANIEL2019104804,
title = {Initializing a hospital-wide data quality program. The AP-HP experience.},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104804},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306242},
author = {Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon},
keywords = {Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic},
abstract = {Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.}
}
@incollection{KRISHNAN2013219,
title = {Chapter 11 - Data-Driven Architecture for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {219-240},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00011-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000118},
author = {Krish Krishnan},
keywords = {metadata, master data, machine learning, algorithms, semantic libraries, data governance},
abstract = {The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.}
}
@incollection{TALBURT20151,
title = {Chapter 1 - The Value Proposition for MDM and Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-16},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000016},
author = {John R. Talburt and Yinle Zhou},
keywords = {Master data, master data management, MDM, Big Data, reference data management, RDM},
abstract = {This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.}
}
@article{SEO201969,
title = {A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD},
journal = {Environmental Modelling & Software},
volume = {117},
pages = {69-75},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218307667},
author = {Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski},
keywords = {NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology},
abstract = {The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.}
}
@article{ALVAREZSANCHEZ2019104824,
title = {TAQIH, a tool for tabular data quality assessment and improvement in the context of health data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104824},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718304188},
author = {Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin},
keywords = {Data quality, Exploratory data analysis, Data pre-processing},
abstract = {Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.}
}
@article{BENDLE2016115,
title = {Uncovering the message from the mess of big data},
journal = {Business Horizons},
volume = {59},
number = {1},
pages = {115-124},
year = {2016},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681315001408},
author = {Neil T. Bendle and Xin (Shane) Wang},
keywords = {Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data},
abstract = {User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.}
}
@article{SCHULER2019191,
title = {Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets},
journal = {Advances in Radiation Oncology},
volume = {4},
number = {1},
pages = {191-200},
year = {2019},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S2452109418302240},
author = {Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego},
abstract = {Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.}
}
@article{LIU2018191,
title = {Steering data quality with visual analytics: The complexity challenge},
journal = {Visual Informatics},
volume = {2},
number = {4},
pages = {191-197},
year = {2018},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X18300573},
author = {Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong},
keywords = {Data quality management, Visual analytics, Data cleansing},
abstract = {Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.}
}
@incollection{MAYER201667,
title = {Chapter 5 - Big Data For Health Through Social Media},
editor = {Shabbir Syed-Abdul and Elia Gabarron and Annie Y.S. Lau},
booktitle = {Participatory Health Through Social Media},
publisher = {Academic Press},
pages = {67-82},
year = {2016},
isbn = {978-0-12-809269-9},
doi = {https://doi.org/10.1016/B978-0-12-809269-9.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092699000050},
author = {M.A. Mayer and L. Fernández-Luque and A. Leis},
keywords = {Big Data, social media, data analysis, public health, Internet},
abstract = {Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.}
}
@article{DREWER2017298,
title = {The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {298-308},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300699},
author = {Daniel Drewer and Vesela Miladinova},
keywords = {Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)},
abstract = {In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.}
}
@article{GUNTHER2019583,
title = {Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises},
journal = {Procedia Manufacturing},
volume = {29},
pages = {583-591},
year = {2019},
note = {“18th International Conference on Sheet Metal, SHEMET 2019”“New Trends and Developments in Sheet Metal Processing”},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.02.114},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919301477},
author = {Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer},
keywords = {Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control},
abstract = {Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.}
}
@incollection{PHAN2017253,
title = {9 - Big Data and Monitoring the Grid},
editor = {Brian W. D’Andrade},
booktitle = {The Power Grid},
publisher = {Academic Press},
pages = {253-285},
year = {2017},
isbn = {978-0-12-805321-8},
doi = {https://doi.org/10.1016/B978-0-12-805321-8.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053218000094},
author = {Sonal K. Phan and Cathy Chen},
keywords = {Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response},
abstract = {A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.}
}
@article{BIBAULT2016110,
title = {Big Data and machine learning in radiation oncology: State of the art and future prospects},
journal = {Cancer Letters},
volume = {382},
number = {1},
pages = {110-117},
year = {2016},
issn = {0304-3835},
doi = {https://doi.org/10.1016/j.canlet.2016.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0304383516303469},
author = {Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun},
keywords = {Radiation oncology, Big Data, Predictive model, Machine learning},
abstract = {Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.}
}
@article{SONG2019288,
title = {Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data},
journal = {Environmental Pollution},
volume = {253},
pages = {288-296},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930418X},
author = {Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood},
keywords = {Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health},
abstract = {In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.}
}
@article{MORANFERNANDEZ2022365,
title = {How important is data quality? Best classifiers vs best features},
journal = {Neurocomputing},
volume = {470},
pages = {365-375},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.107},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011127},
author = {Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos},
keywords = {Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis},
abstract = {The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.}
}
@article{BABAR2018155,
title = {Energy-harvesting based on internet of things and big data analytics for smart health monitoring},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {155-164},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917302238},
author = {Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon},
keywords = {Big data analytics, IoT, Energy harvesting},
abstract = {Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.}
}
@article{RAJAN2019193,
title = {Towards a content agnostic computable knowledge repository for data quality assessment},
journal = {Computer Methods and Programs in Biomedicine},
volume = {177},
pages = {193-201},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306254},
author = {Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli},
keywords = {Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework},
abstract = {Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.}
}
@article{SARAN2017713,
title = {The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”},
journal = {American Journal of Kidney Diseases},
volume = {69},
number = {6},
pages = {713-716},
year = {2017},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2017.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0272638617306340},
author = {Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham}
}
@article{CHALVATZIS2019381,
title = {Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {381-393},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315147},
author = {Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary},
keywords = {Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming},
abstract = {Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.}
}
@incollection{GUDIVADA2016169,
title = {Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {169-205},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300517},
author = {V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao},
keywords = {Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics},
abstract = {This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.}
}
@article{CALYAM20163,
title = {Synchronous Big Data analytics for personalized and remote physical therapy},
journal = {Pervasive and Mobile Computing},
volume = {28},
pages = {3-20},
year = {2016},
note = {Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119215001704},
author = {Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic},
keywords = {Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app},
abstract = {With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.}
}
@incollection{ANYA201699,
title = {Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges},
editor = {Dhiya Al-Jumeily and Abir Hussain and Conor Mallucci and Carol Oliver},
booktitle = {Applied Computing in Medicine and Health},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {99-124},
year = {2016},
series = {Emerging Topics in Computer Science and Applied Computing},
isbn = {978-0-12-803468-2},
doi = {https://doi.org/10.1016/B978-0-12-803468-2.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034682000059},
author = {Obinna Anya and Hissam Tawfik},
keywords = {Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework},
abstract = {Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.}
}
@article{GUETA2016139,
title = {Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models},
journal = {Ecological Informatics},
volume = {34},
pages = {139-145},
year = {2016},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574954116300577},
author = {Tomer Gueta and Yohay Carmel},
keywords = {Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data},
abstract = {The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.}
}
@incollection{KRISHNAN2013101,
title = {Chapter 5 - Big Data Driving Business Value},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-123},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000052},
author = {Krish Krishnan},
keywords = {sensor data, machine data, social media, compliance, safety},
abstract = {The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.}
}
@article{PFEIFFER2015213,
title = {Spatial and temporal epidemiological analysis in the Big Data era},
journal = {Preventive Veterinary Medicine},
volume = {122},
number = {1},
pages = {213-220},
year = {2015},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167587715002111},
author = {Dirk U. Pfeiffer and Kim B. Stevens},
keywords = {Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation},
abstract = {Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.}
}
@article{SALEM2021,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{DEBAUCHE2018112,
title = {Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments},
journal = {Procedia Computer Science},
volume = {141},
pages = {112-118},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318064},
author = {Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback},
keywords = {GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture},
abstract = {Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.}
}
@article{CHOI2019139,
title = {Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {131},
pages = {139-152},
year = {2019},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2019.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519311494},
author = {Tsan-Ming Choi and Suyuan Luo},
keywords = {Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare},
abstract = {In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.}
}
@article{FUMEO2015437,
title = {Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis},
journal = {Procedia Computer Science},
volume = {53},
pages = {437-446},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018244},
author = {Emanuele Fumeo and Luca Oneto and Davide Anguita},
keywords = {Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection},
abstract = {Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.}
}
@article{BIBRI2017449,
title = {ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts},
journal = {Sustainable Cities and Society},
volume = {32},
pages = {449-474},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716302475},
author = {Simon Elias Bibri and John Krogstie},
keywords = {Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing},
abstract = {Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.}
}
@article{SINGH2018652,
title = {Real world big data for clinical research and drug development},
journal = {Drug Discovery Today},
volume = {23},
number = {3},
pages = {652-660},
year = {2018},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617305950},
author = {Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra},
abstract = {The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.}
}
@incollection{LOSHIN2013105,
title = {Chapter 11 - Developing the Big Data Roadmap},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-120},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000119},
author = {David Loshin},
keywords = {Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment},
abstract = {This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.}
}
@incollection{TONG2020107,
title = {Chapter 5 - Machine learning for spatiotemporal big data in air pollution},
editor = {Lixin Li and Xiaolu Zhou and Weitian Tong},
booktitle = {Spatiotemporal Analysis of Air Pollution and Its Application in Public Health},
publisher = {Elsevier},
pages = {107-134},
year = {2020},
isbn = {978-0-12-815822-7},
doi = {https://doi.org/10.1016/B978-0-12-815822-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158227000054},
author = {Weitian Tong},
keywords = {Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning},
abstract = {An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.}
}
@article{MAYO2016260,
title = {The big data effort in radiation oncology: Data mining or data farming?},
journal = {Advances in Radiation Oncology},
volume = {1},
number = {4},
pages = {260-271},
year = {2016},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452109416300550},
author = {Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}},
abstract = {Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.}
}
@article{RAMOS20151031,
title = {Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis},
journal = {Procedia Computer Science},
volume = {55},
pages = {1031-1039},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015367},
author = {Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro},
keywords = {Big Data, Data Warehouse, Cluster, Education, IDEB},
abstract = {This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.}
}
@incollection{TALBURT2015161,
title = {Chapter 10 - CSRUD for Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-190},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000107},
author = {John R. Talburt and Yinle Zhou},
keywords = {Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component},
abstract = {This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.}
}
@article{CORIZZO201918,
title = {Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data},
journal = {Big Data Research},
volume = {16},
pages = {18-35},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302119},
author = {Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz},
keywords = {Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting},
abstract = {The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.}
}
@incollection{SAMPSON2015229,
title = {Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {229-237},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801967200015X},
author = {Fraser Sampson},
keywords = {Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation},
abstract = {This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.}
}
@incollection{SAHOO2019227,
title = {Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {227-246},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818146100009X},
author = {Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das},
keywords = {Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system},
abstract = {In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.}
}
@article{YE201965,
title = {A hybrid IT framework for identifying high-quality physicians using big data analytics},
journal = {International Journal of Information Management},
volume = {47},
pages = {65-75},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830834X},
author = {Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang},
keywords = {Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis},
abstract = {Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.}
}
@incollection{KRISHNAN2013257,
title = {Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-265},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000143},
author = {Krish Krishnan},
keywords = {Hadoop, RDBMS, NoSQL, transformation, architecture},
abstract = {This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.}
}
@article{ALGHAMDI2021462,
title = {Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {462-479},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033079X},
author = {Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros},
keywords = {Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making},
abstract = {An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.}
}
@article{SU201722,
title = {A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning},
journal = {Habitat International},
volume = {64},
pages = {22-40},
year = {2017},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0197397517300498},
author = {Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng},
keywords = {Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression},
abstract = {Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.}
}
@incollection{REEVE2013141,
title = {Chapter 21 - Big Data Integration},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {141-156},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000212},
author = {April Reeve}
}
@article{JESSE2016275,
title = {Internet of Things and Big Data – The Disruption of the Value Chain and the Rise of New Software Ecosystems},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {29},
pages = {275-282},
year = {2016},
note = {17th IFAC Conference on International Stability, Technology and Culture TECIS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.079},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316325174},
author = {Norbert Jesse},
keywords = {Internet of Things, Smart Factories, Big Data, Software Platforms, Data Science},
abstract = {Abstract:
IoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between “everything”, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract “sense” from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.}
}
@article{ANDREASEN201926,
title = {Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices},
journal = {Journal of Econometrics},
volume = {212},
number = {1},
pages = {26-46},
year = {2019},
note = {Big Data in Dynamic Predictive Econometric Modeling},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2019.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0304407619300740},
author = {Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch},
keywords = {Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model},
abstract = {Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.}
}
@incollection{BROWN2018277,
title = {Chapter Five - Big Data in Drug Discovery},
editor = {David R. Witty and Brian Cox},
series = {Progress in Medicinal Chemistry},
publisher = {Elsevier},
volume = {57},
pages = {277-356},
year = {2018},
issn = {0079-6468},
doi = {https://doi.org/10.1016/bs.pmch.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079646817300243},
author = {Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard},
keywords = {Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials},
abstract = {Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.}
}
@incollection{DHAESE2018137,
title = {Chapter 13 - Big Data and Deep Brain Stimulation},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {137-145},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053539000139},
author = {Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant},
keywords = {Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization},
abstract = {Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.}
}
@article{SOUIBGUI2019676,
title = {Data quality in ETL process: A preliminary study},
journal = {Procedia Computer Science},
volume = {159},
pages = {676-687},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314097},
author = {Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia},
keywords = {Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality},
abstract = {The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.}
}
@article{MATHEW201585,
title = {Big-data for building energy performance: Lessons from assembling a very large national database of building energy use},
journal = {Applied Energy},
volume = {140},
pages = {85-93},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914012112},
author = {Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter},
keywords = {Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support},
abstract = {Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.}
}
@article{RANJAN2017495,
title = {A note on exploration of IoT generated big data using semantics},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {495-498},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313912},
author = {Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya},
abstract = {Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.}
}
@article{DABEK2015265,
title = {Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion},
journal = {Procedia Computer Science},
volume = {53},
pages = {265-273},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018062},
author = {Filip Dabek and Jesus J. Caban},
keywords = {Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury},
abstract = {A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.}
}
@incollection{LEVIN2016317,
title = {Chapter 11 - From Databases to Big Data},
editor = {Elaine Holmes and Jeremy K. Nicholson and Ara W. Darzi and John C. Lindon},
booktitle = {Metabolic Phenotyping in Personalized and Public Healthcare},
publisher = {Academic Press},
address = {Boston},
pages = {317-331},
year = {2016},
isbn = {978-0-12-800344-2},
doi = {https://doi.org/10.1016/B978-0-12-800344-2.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128003442000112},
author = {Nadine Levin and Reza M. Salek and Christoph Steinbeck},
keywords = {Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing},
abstract = {This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.}
}
@article{ENRIQUEZ201714,
title = {Entity reconciliation in big data sources: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {80},
pages = {14-27},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417301550},
author = {J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples},
keywords = {Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data},
abstract = {The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.}
}
@article{MARKOWETZ2014405,
title = {Psycho-Informatics: Big Data shaping modern psychometrics},
journal = {Medical Hypotheses},
volume = {82},
number = {4},
pages = {405-411},
year = {2014},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0306987713005598},
author = {Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer},
abstract = {For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.}
}
@article{NIMMAGADDA20171871,
title = {Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting},
journal = {Procedia Computer Science},
volume = {112},
pages = {1871-1880},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316381},
author = {Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}},
keywords = {Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation},
abstract = {Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.}
}
@article{MACKIE2015189,
title = {Big data! Big deal?},
journal = {Public Health},
volume = {129},
number = {3},
pages = {189-190},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615000621},
author = {P. Mackie and F. Sim and C. Johnman}
}
@article{DUVIER2018196,
title = {Data quality challenges in the UK social housing sector},
journal = {International Journal of Information Management},
volume = {38},
number = {1},
pages = {196-200},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216308222},
author = {Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens},
keywords = {Social housing, Data quality},
abstract = {The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.}
}
@article{LIN2014532,
title = {A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era},
journal = {Procedia Computer Science},
volume = {31},
pages = {532-541},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004761},
author = {Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang},
keywords = {debt cost, big data, quality of accounting information, corporate governance, LASSO method},
abstract = {Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.}
}
@incollection{HOLLIN201514,
title = {Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {14-20},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019672000021},
author = {Rupert Hollin},
keywords = {Big Data, Fusion, High-performance analytics, Visualization},
abstract = {Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.}
}
@article{WONG201944,
title = {Artificial Intelligence for infectious disease Big Data Analytics},
journal = {Infection, Disease & Health},
volume = {24},
number = {1},
pages = {44-48},
year = {2019},
issn = {2468-0451},
doi = {https://doi.org/10.1016/j.idh.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468045118301445},
author = {Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang},
keywords = {Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning},
abstract = {Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.}
}
@incollection{SHEIKH2013185,
title = {Chapter 11 - Big Data, Hadoop, and Cloud Computing},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {185-197},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000116},
author = {Nauman Sheikh},
keywords = {Hadoop, Big Data, cloud computing},
abstract = {When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.}
}
@article{AZEROUAL201850,
title = {Analyzing data quality issues in research information systems via data profiling},
journal = {International Journal of Information Management},
volume = {41},
pages = {50-56},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218300975},
author = {Otmane Azeroual and Gunter Saake and Eike Schallehn},
keywords = {Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization},
abstract = {The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.}
}
@article{ELKASSAR2019483,
title = {Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {483-498},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315226},
author = {Abdul-Nasser El-Kassar and Sanjay Kumar Singh},
keywords = {Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance},
abstract = {Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.}
}
@article{201616,
title = {Clinical research and big data},
journal = {Dental Abstracts},
volume = {61},
number = {1},
pages = {16-17},
year = {2016},
issn = {0011-8486},
doi = {https://doi.org/10.1016/j.denabs.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0011848615010043}
}
@article{MCDERMOTT2015303,
title = {What are the implications of the big data paradigm shift for disability and health?},
journal = {Disability and Health Journal},
volume = {8},
number = {3},
pages = {303-304},
year = {2015},
issn = {1936-6574},
doi = {https://doi.org/10.1016/j.dhjo.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1936657415000515},
author = {Suzanne McDermott and Margaret A. Turk}
}
@article{YU20171,
title = {Data pricing strategy based on data quality},
journal = {Computers & Industrial Engineering},
volume = {112},
pages = {1-10},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217303509},
author = {Haifei Yu and Mengxiao Zhang},
keywords = {Big data, Data marketplace, Data pricing, Production management, Bi-level programming model},
abstract = {This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.}
}
@article{JEFFREYKUO2018120,
title = {Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach},
journal = {Energy and Buildings},
volume = {168},
pages = {120-136},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817334345},
author = {Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee},
keywords = {Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor},
abstract = {This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.}
}
@article{SUN20171,
title = {Special Issue on Scalable Computing Systems for Big Data Applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {108},
pages = {1-2},
year = {2017},
note = {Special Issue on Scalable Computing Systems for Big Data Applications},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301776},
author = {Xian-He Sun and Marc Frincu and Charalampos Chelmis}
}
@article{MARSDEN2018A1,
title = {Numerical data quality in IS research and the implications for replication},
journal = {Decision Support Systems},
volume = {115},
pages = {A1-A7},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301647},
author = {James R. Marsden and David E. Pingry},
abstract = {We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.}
}
@article{VITOLO2015185,
title = {Web technologies for environmental Big Data},
journal = {Environmental Modelling & Software},
volume = {63},
pages = {185-198},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002965},
author = {Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert},
keywords = {Web-based modelling, Big Data, Web services, OGC standards},
abstract = {Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.}
}
@article{SCHUH201943,
title = {Data quality program management for digital shadows of products},
journal = {Procedia CIRP},
volume = {86},
pages = {43-48},
year = {2019},
note = {7th CIRP Global Web Conference – Towards shifted production value stream patterns through inference of data, models, and technology (CIRPe 2019)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120300366},
author = {Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank},
keywords = {data quality program, digital shadow, data quality management},
abstract = {Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.}
}
@article{EYOB201927,
title = {Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives},
journal = {International Journal of Surgery},
volume = {72},
pages = {27-32},
year = {2019},
note = {Endoscopic Surgery},
issn = {1743-9191},
doi = {https://doi.org/10.1016/j.ijsu.2019.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S174391911930189X},
author = {Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger},
keywords = {Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data},
abstract = {Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.}
}
@article{GIL201696,
title = {Modeling and Management of Big Data: Challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {96-99},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002514},
author = {David Gil and Il-Yeol Song},
keywords = {Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize},
abstract = {The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.}
}
@incollection{KRISHNAN2013241,
title = {Chapter 12 - Information Management and Life Cycle for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {241-250},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405891000012X},
author = {Krish Krishnan},
keywords = {information life-cycle management, governance, program governance, data governance, data quality},
abstract = {This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.}
}
@incollection{KRISHNAN20133,
title = {Chapter 1 - Introduction to Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-14},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000015},
author = {Krish Krishnan},
keywords = {Big Data, data warehousing, sentiments, social media, machine data},
abstract = {Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.}
}
@article{OBRIEN2015442,
title = {‘Accounting’ for Data Quality in Enterprise Systems},
journal = {Procedia Computer Science},
volume = {64},
pages = {442-449},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.539},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915026745},
author = {Tony O’Brien},
keywords = {Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data},
abstract = {Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.}
}
@article{DEMIRKAN2013412,
title = {Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {412-421},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2012.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0167923612001595},
author = {Haluk Demirkan and Dursun Delen},
keywords = {Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data},
abstract = {Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.}
}
@article{CHANG201656,
title = {A model to compare cloud and non-cloud storage of Big Data},
journal = {Future Generation Computer Systems},
volume = {57},
pages = {56-76},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003167},
author = {Victor Chang and Gary Wills},
keywords = {Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization},
abstract = {When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.}
}
@article{XIAO2014594,
title = {Knowledge diffusion path analysis of data quality literature: A main path analysis},
journal = {Journal of Informetrics},
volume = {8},
number = {3},
pages = {594-605},
year = {2014},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2014.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751157714000492},
author = {Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou},
keywords = {Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data},
abstract = {This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.}
}
@incollection{LOSHIN201329,
title = {Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {29-37},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000041},
author = {David Loshin},
keywords = {Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration},
abstract = {This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.}
}
@incollection{CELKO2014119,
title = {Chapter 9 - Big Data and Cloud Computing},
editor = {Joe Celko},
booktitle = {Joe Celko’s Complete Guide to NoSQL},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {119-128},
year = {2014},
isbn = {978-0-12-407192-6},
doi = {https://doi.org/10.1016/B978-0-12-407192-6.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071926000091},
author = {Joe Celko},
keywords = {Forrester Research, V-list, cloud computing, Big Data, data mining},
abstract = {Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.}
}
@article{DEKHTIAR2018227,
title = {Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study},
journal = {Computers in Industry},
volume = {100},
pages = {227-243},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517305560},
author = {Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis},
keywords = {Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval},
abstract = {With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.}
}
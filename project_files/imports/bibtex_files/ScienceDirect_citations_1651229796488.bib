@article{SKRIPCAK2014303,
title = {Creating a data exchange strategy for radiotherapy research: Towards federated databases and anonymised public datasets},
journal = {Radiotherapy and Oncology},
volume = {113},
number = {3},
pages = {303-309},
year = {2014},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167814014004071},
author = {Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas Brunner and Volker Budach and Daniel Büttner and Jürgen Debus and Andre Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin Lühr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Björn Zackrisson and Daniel Zips and Michael Baumann},
keywords = {Data pooling, Interoperability, Data exchange, Large scale studies, Public data, Radiotherapy},
abstract = {Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within “Big Data”. Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.}
}
@article{SEMLALI2021107257,
title = {SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107257},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107257},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002421},
author = {Badr-Eddine Boudriki Semlali and Chaker El Amrani and Guadalupe Ortiz and Juan Boubeta-Puig and Alfonso Garcia-de-Prado},
keywords = {Remote sensing, Satellite sensors, Air quality, Complex event processing, Big data, Decision-making},
abstract = {Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.}
}
@article{PAGGI2021106907,
title = {Towards the definition of an information quality metric for information fusion models},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106907},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106907},
url = {https://www.sciencedirect.com/science/article/pii/S004579062030759X},
author = {Horacio Paggi and Javier Soriano and Juan A. Lara and Ernesto Damiani},
keywords = {Adaptive Peer-to-Peer systems, Information fusion, Uncertain information handling, Information quality metric},
abstract = {Managing information quality has become important in cyber-physical systems dealing with big data. In this regard, different models have been proposed, mainly in flat peer-to-peer networks, in which exchanging information efficiently is a key aspect due to scarce resources. However, little research has been conducted on information quality metrics for cyber-physical scenarios. In this paper, we propose an information quality metric and show its application to an information fusion model. It is a “model-oriented quality metric” since it allows non-predefined variants on its configuration depending on the application domain. The model was tested on several simulations using open datasets. The results obtained in the performance of the model confirm the validity of the information quality metric, proposed in this paper, on which the model is based. The model may have a wide variety of applications such as mobile recommendation or decision making in critical environments (emergencies, war, and so on).}
}
@article{WATT2016423,
title = {Privacy Matters – Issues within Mechatronics},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {21},
pages = {423-430},
year = {2016},
note = {7th IFAC Symposium on Mechatronic Systems MECHATRONICS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.641},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316322534},
author = {Steve Watt and Chris Milne and David Bradley and David Russell and Peter Hehenberger and Jorge Azorin-Lopez},
keywords = {Privacy, Users, Big Data, Security, Mechatronics, Cyber-Physical Systems, Internet of Things},
abstract = {Abstract:
As mechatronic devices and components become increasingly integrated with and within wider systems concepts such as Cyber-Physical Systems and the Internet of Things, designer engineers are faced with new sets of challenges in areas such as privacy. The paper looks at the current, and potential future, of privacy legislation, regulations and standards and considers how these are likely to impact on the way in which mechatronics is perceived and viewed. The emphasis is not therefore on technical issues, though these are brought into consideration where relevant, but on the soft, or human centred, issues associated with achieving user privacy.}
}
@incollection{HOVENGA2020355,
title = {Chapter 11 - Measuring health service quality},
editor = {Evelyn J.S. Hovenga and Cherrie Lowe},
booktitle = {Measuring Capacity to Care Using Nursing Data},
publisher = {Academic Press},
pages = {355-388},
year = {2020},
isbn = {978-0-12-816977-3},
doi = {https://doi.org/10.1016/B978-0-12-816977-3.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169773000113},
author = {Evelyn J.S. Hovenga and Cherrie Lowe},
keywords = {Performance measurement, Nursing ecosystem, Productivity, Nursing practice environments, Collegial culture, Accountability, Data quality, Data governance, Accreditation, Standards},
abstract = {Quality can be defined in multiple ways and is impacted by multiple factors. It applies to any operational process within health care and has a strong relationship with the performance of individual staff members as well as overall organizational performance outcomes. The characteristics of any nursing practice environment influence the quality of service provided. The ability to measure the quality of services provided is largely dependent upon the availability and type of data that can be accessed and processed. Meaningful measurement, trend analysis and monitoring to enable continuous improvements to be made, are only possible when governed data standards are used. This chapter has a strong focus on health and nursing, including acuity and clinical data use and provides global recommendations on health data, data standards and governance. Reference is made to other types of related standards, including accreditation standards and standards governance. The chapter concludes with an examination of various international and national outcomes research organizations, their comparative studies, and use of performance indicator data sets, clinical standards and guidelines, big data and secondary data use. Caring has been well defined yet doesn't appear to be routinely measured even though this is a major component directly impacting patient satisfaction.}
}
@article{CHEN2022102474,
title = {EVFL: An explainable vertical federated learning for data-oriented Artificial Intelligence systems},
journal = {Journal of Systems Architecture},
volume = {126},
pages = {102474},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102474},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122000583},
author = {Peng Chen and Xin Du and Zhihui Lu and Jie Wu and Patrick C.K. Hung},
keywords = {Data-oriented AI, Federated counterfactual explanation, Feature importance, Vertical federated learning, Data cleansing},
abstract = {Vertical federated learning (VFL), as one of the latest advances of security in the data-oriented Artificial Intelligence (AI) systems, facilitates better keeping the AI systems converge faster with higher performance and security. Since a large amount of data from these systems is often of low quality, the training data needs to be interpreted and evaluated. While there have been some research efforts, they still have significant shortcomings, such as high computational complexity and impracticality. Considering the characteristics of the data, the interpretation of machine learning models allows for data cleansing, which can improve data quality and help regulators understand the decision-making process. In this paper, we propose an explainable vertical federated learning (EVFL) framework, including the credibility assessment strategy, the federated counterfactual explanation and the importance rate (IR) metric. Furthermore, we initialize the knowledge-based counterfactual instance based on prior knowledge and retrain the federated counterfactual method for feasible counterfactual features. We report experimental results obtained on the Lending Club and Zhongyuan datasets for implementing our framework to show that our approach is significantly effective. Notably, on the Lending Club dataset, our method can have a +4.9% improvement over other selections.}
}
@article{KALANTARI20182,
title = {Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions},
journal = {Neurocomputing},
volume = {276},
pages = {2-22},
year = {2018},
note = {Machine Learning and Data Mining Techniques for Medical Complex Data Analysis},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315436},
author = {Ali Kalantari and Amirrudin Kamsin and Shahaboddin Shamshirband and Abdullah Gani and Hamid Alinejad-Rokny and Anthony T. Chronopoulos},
keywords = {Computational intelligence, Medical application, Big data, Detection, Ensemble algorithm},
abstract = {The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.}
}
@incollection{BERANGER2016167,
title = {3 - Management and Governance of Personal Health Data},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {167-236},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500038},
author = {Jérôme Béranger},
keywords = {Controlled regulation, Data lifecycle, Environmental digital ecosystem, Governance, Management, Quality control, Regulatory and organizational aspects, Relational and cultural aspects, Strategic and methodological aspects, Structural and technological aspects},
abstract = {Abstract:
Every company has its own culture, its organization, its governance mode and its project management models. Nevertheless, a number of significant and universal principles concerning governance can be identified, both at the approach level as well as that of actors and of responsibilities. Data governance is one of the key factors to success in the protection of information. It is one of the components that defines the rules, guides and charters of good practice, establishes references and policies (management, classification, storage, and conservation of personal data), and further describes the responsibilities and controls their application. Therefore, it becomes paramount to understand: how can the complexity around personal data management be apprehended and in particular in health fields? In addition, what are the possible mechanisms to process these data pools to turn them into consistent and relevant information? To this end, it is essential to have a detailed and an accurate understanding of algorithmic governance, of the environmental numerical ecosystem, of safety and protection, and of the lifecycle of Big Data.}
}
@article{YANG2019277,
title = {Ontology: Footstone for Strong Artificial Intelligence},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {4},
pages = {277-280},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003701},
url = {https://www.sciencedirect.com/science/article/pii/S1001929420300080},
author = {Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu},
keywords = {ontology, artificial intelligence, biomedicine, big data},
abstract = {Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.}
}
@article{WOODALL201972,
title = {Potential Problem Data Tagging: Augmenting information systems with the capability to deal with inaccuracies},
journal = {Decision Support Systems},
volume = {121},
pages = {72-83},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300740},
author = {Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane},
keywords = {Data quality, Information quality, Accuracy, Metadata, Data analytics, Data tags},
abstract = {Data quality tags are a means of informing decision makers about the quality of the data they use from information systems. Unfortunately, data quality tags have not been successfully adopted despite their potential to assist decision makers. One reason for the non-adoption is that maintaining the tags is expensive and time-consuming: having a tag that represents accuracy, for example, would be massively time-consuming to measure because it requires some physical observation of reality to check the true value. We argue that a useful surrogate tag for accuracy can be created—without having to physically measure it—by counting the number of times the data has been exposed to an event that could cause it to become inaccurate. Experimental results show that the tags can help to avoid problems caused by inaccuracies, and also to help find the inaccuracies themselves.}
}
@article{KHOURY2022,
title = {A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology and Education Committee},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
year = {2022},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2022.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S221321982200143X},
author = {Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider},
keywords = {Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education},
abstract = {Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.}
}
@article{GAI2018262,
title = {A survey on FinTech},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {262-273},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303247},
author = {Keke Gai and Meikang Qiu and Xiaotong Sun},
keywords = {FinTech, Cloud computing, Cyber security, Big data, Financial computing, Data-driven framework},
abstract = {As a new term in the financial industry, FinTech has become a popular term that describes novel technologies adopted by the financial service institutions. This term covers a large scope of techniques, from data security to financial service deliveries. An accurate and up-to-date awareness of FinTech has an urgent demand for both academics and professionals. This work aims to produce a survey of FinTech by collecting and reviewing contemporary achievements, by which a theoretical data-driven FinTech framework is proposed. Five technical aspects are summarized and involved, which include security and privacy, data techniques, hardware and infrastructure, applications and management, and service models. The main findings of this work are fundamentals of forming active FinTech solutions.}
}
@article{HEZEL20181,
title = {What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database},
journal = {Geochemistry},
volume = {78},
number = {1},
pages = {1-14},
year = {2018},
issn = {0009-2819},
doi = {https://doi.org/10.1016/j.chemer.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0009281916302896},
author = {Dominik C. Hezel and Markus Harak and Guy Libourel},
keywords = {Chondrules, Matrix, Elemental composition, ChondritedDB, Database},
abstract = {Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.}
}
@incollection{ZHOU2018423,
title = {5.11 Smart Energy Management},
editor = {Ibrahim Dincer},
booktitle = {Comprehensive Energy Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {423-456},
year = {2018},
isbn = {978-0-12-814925-6},
doi = {https://doi.org/10.1016/B978-0-12-809597-3.00525-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095973005253},
author = {Kaile Zhou and Shanlin Yang},
keywords = {Demand side management, Energy big data, Energy consumption behavior, Energy informatics, Energy social informatics, Smart energy management, Smart energy system},
abstract = {Smart energy management is the path to achieve the management and operational objectives of smart energy systems (SESs). First, some related concepts of smart energy management are introduced. The evolution of energy systems in four stages and the three dimensions of smart energy management are also proposed. Then the overall structure and key technologies of SESs are provided, followed by the introduction of the composition of energy big data and its application in demand side management (DSM). Furthermore, the Ubiquitous Energy Internet in China, the smart energy management in smart buildings, smart manufacturing, and smart transportation are discussed as case studies of smart energy management. Finally, the research paradigms of smart energy management are presented and future directions are pointed out.}
}
@article{YE2021102513,
title = {A feasible framework to downscale NPP-VIIRS nighttime light imagery using multi-source spatial variables and geographically weighted regression},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102513},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102513},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002208},
author = {Yang Ye and Linyan Huang and Qiming Zheng and Chenxin Liang and Baiyu Dong and Jinsong Deng and Xiuzhen Han},
keywords = {Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection},
abstract = {The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.}
}
@article{TAYLOR2022107492,
title = {An interdisciplinary research perspective on the future of multi-vector energy networks},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {135},
pages = {107492},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521007316},
author = {P.C. Taylor and M. Abeysekera and Y. Bian and D. Ćetenović and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu},
keywords = {Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk},
abstract = {Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.}
}
@article{JUNG202115,
title = {The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems},
journal = {Current Opinion in Biotechnology},
volume = {70},
pages = {15-22},
year = {2021},
note = {Food Biotechnology ● Plant Biotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0958166920301257},
author = {Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles},
abstract = {Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.}
}
@article{DELRIOCASTRO2021122204,
title = {Unleashing the convergence amid digitalization and sustainability towards pursuing the Sustainable Development Goals (SDGs): A holistic review},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {122204},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122204},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620322514},
author = {Gema {Del Río Castro} and María Camino {González Fernández} and Ángel {Uruburu Colsa}},
keywords = {Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence},
abstract = {The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.}
}
@article{PASICHNYI2019486,
title = {Energy performance certificates — New opportunities for data-enabled urban energy policy instruments?},
journal = {Energy Policy},
volume = {127},
pages = {486-499},
year = {2019},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2018.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0301421518307894},
author = {Oleksii Pasichnyi and Jörgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas},
keywords = {Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden},
abstract = {Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.}
}
@article{SCHOCK2021636,
title = {Data Acquisition and Preparation – Enabling Data Analytics Projects within Production},
journal = {Procedia CIRP},
volume = {104},
pages = {636-640},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010052},
author = {Christoph Schock and Jonas Dumler and Frank Doepper},
keywords = {Data Analytics, CRISP-DM, Data Acquisition, Data Preparation, Feature Engineering, Process Monitoring, Condition Monitoring},
abstract = {The increasing amount of available data in production systems is associated with great potential for process optimization. Due to lack of a data analytics methodology and low data quality within production these potentials often remain unused. Therefore, in this paper we present a model for data acquisition and data preparation including feature engineering for characteristic sensor signals of production machines. The model allows the extraction of relevant process information from the signal, which can be used for monitoring, KPI tracking, trend analysis and anomaly detection. The approach is evaluated on an industrial turning process.}
}
@article{LIU2020263,
title = {Super Resolution Perception for Smart Meter Data},
journal = {Information Sciences},
volume = {526},
pages = {263-273},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.088},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302681},
author = {Guolong Liu and Jinjin Gu and Junhua Zhao and Fushuan Wen and Gaoqi Liang},
keywords = {Super resolution perception, Smart meter data, High-frequency data, Big data analysis},
abstract = {In this paper, we present the problem formulation and methodology framework of Super Resolution Perception (SRP) on smart meter data. With the widespread use of smart meters, a massive amount of electricity consumption data can be obtained. Smart meter data is the basis of automated billing and pricing, appliance identification, demand response, etc. However, the provision of high-quality data may be expensive in many cases. In this paper, we propose a novel problem - the SRP problem as reconstructing high-quality data from unsatisfactory data in smart grids. Advanced generative models are then proposed to solve the problem. This technology makes it possible for empowering existing facilities without upgrading existing meters or deploying additional meters. We first mathematically formulate the SRP problem under the Maximum a Posteriori (MAP) estimation framework. The dataset namely Super Resolution Perception Dataset (SRPD) is designed for this problem and released. A case study is then presented, which performs SRP on smart meter data. A network namely Super Resolution Perception Convolutional Neural Network (SRPCNN) is proposed to generate high-frequency load data from low-frequency data. Experiments demonstrate that our SRP models can reconstruct high-frequency data effectively. Moreover, the reconstructed high-frequency data can lead to better appliance identification results.}
}
@article{KONGBOON2022130711,
title = {Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130711},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130711},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200350X},
author = {Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul},
keywords = {Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities},
abstract = {This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.}
}
@article{LNENICKA2019129,
title = {Big and open linked data analytics ecosystem: Theoretical background and essential elements},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {129-144},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302545},
author = {Martin Lnenicka and Jitka Komarkova},
keywords = {Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework},
abstract = {Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.}
}
@article{RODRIGUES2022101625,
title = {Species misidentification affects biodiversity metrics: Dealing with this issue using the new R package naturaList},
journal = {Ecological Informatics},
volume = {69},
pages = {101625},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101625},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000747},
author = {Arthur Vinicius Rodrigues and Gabriel Nakamura and Vanessa Graziele Staggemeier and Leandro Duarte},
keywords = {Occurrence records, Biodiversity, Species misidentification, Taxonomy, Ecological patterns},
abstract = {Biodiversity databases are increasingly available and have fostered accelerated advances in many disciplines within ecology and evolution. However, the quality of the evidence generated depends critically on the quality of the input data, and species misidentifications are present in virtually any occurrence dataset. Yet, the lack of automatized tools makes the assessment of the quality of species identification in big datasets time-consuming, which often induces researchers to assume that all species are reliably identified. In this study, we address this issue by evaluating how species misidentification can impact our ability to capture ecological patterns, and by presenting an R package, called naturaList, designed to classify species occurrence data according to identification reliability. naturaList allows the classification of species occurrences up to six confidence levels, in which the highest level is assigned to records identified by specialists. We obtained a list of specialists by using the species occurrence dataset itself, based on the identifier names within it, and by entering an independent list, obtained by contacting experts. Further, we evaluate the effects of filtering out occurrence records not identified by specialists on the estimations of species niche and diversity patterns. We used the tribe Myrteae (Myrtaceae) as a study model, which is a species-rich group in Central and South America and with challenging taxonomy. We found a significant change in species niche in 13% of species when using only occurrences identified by specialists. We found changes in patterns of alpha diversity in four genera and changes in beta diversity in all genera analyzed. We show how the uncertainty in species identification in occurrence datasets affects conclusions on macroecological patterns by generating bias or noise in different aspects of macroecological patterns (niche, alpha, and beta diversity). Therefore, to guarantee reliability in species identification in big data sets we recommend the use of automated tools such as the naturaList package, especially when analyzing variation in species composition. This study also represents a step forward to increasing the quality of large-scale studies that rely on species occurrence data.}
}
@article{JIN2020112412,
title = {Artificial intelligence biosensors: Challenges and prospects},
journal = {Biosensors and Bioelectronics},
volume = {165},
pages = {112412},
year = {2020},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2020.112412},
url = {https://www.sciencedirect.com/science/article/pii/S0956566320304061},
author = {Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang},
keywords = {Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare},
abstract = {Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.}
}
@incollection{ONIK2019197,
title = {Chapter 8 - Blockchain in Healthcare: Challenges and Solutions},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {197-226},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128181461000088},
author = {Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo Kim and Hee-Cheol Kim},
keywords = {Blockchain, Big data, Healthcare, Data privacy, EHR security},
abstract = {The main challenge in distributing electronic health records (EHRs) for patient-centered research, market analysis, medicine investigation, healthcare data mining etc., is data privacy. Handling the large-scale data and preserving the privacy of patients has been a challenge to researchers for a long period of time. On the contrary, blockchain technology has alleviated some of the problems by providing a protected and distributed platform. Sadly, existing electronic health record (EHR) management systems suffer from data manipulation, delayed communication, and trustless cooperation in data collection, storage, and distribution. This chapter discusses the current issues of healthcare data privacy and existing and upcoming regulations on this sector. This chapter also includes an overview of the architecture, existing issues, and future scope of blockchain technology for successfully handling privacy and management of current and future medical records. This chapter also presents few blockchain solutions that advocate the future research scopes in healthcare, big data, and blockchain.}
}
@article{FAN2018296,
title = {Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review},
journal = {Energy and Buildings},
volume = {159},
pages = {296-308},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817326671},
author = {Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang},
keywords = {Unsupervised data mining, Big data, Building operational performance, Building energy management, Building energy efficiency},
abstract = {Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.}
}
@article{LEPRINCE2021111195,
title = {Data mining cubes for buildings, a generic framework for multidimensional analytics of building performance data},
journal = {Energy and Buildings},
volume = {248},
pages = {111195},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821004795},
author = {Julien Leprince and Clayton Miller and Wim Zeiler},
keywords = {Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data},
abstract = {Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.}
}
@article{NASHAAT2019131,
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
journal = {Information and Software Technology},
volume = {113},
pages = {131-145},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301247},
author = {Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study},
abstract = {Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.}
}
@article{AMENGUALGUAL201931,
title = {Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients},
journal = {Seizure},
volume = {68},
pages = {31-37},
year = {2019},
note = {Pediatric Convulsive Status Epilepticus},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1059131118304059},
author = {Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper},
keywords = {Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection},
abstract = {Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.}
}
@article{LI2021692,
title = {Data science skills and domain knowledge requirements in the manufacturing industry: A gap analysis},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {692-706},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001448},
author = {Guoyan Li and Chenxi Yuan and Sagar Kamarthi and Mohsen Moghaddam and Xiaoning Jin},
keywords = {Industry 4.0, Labor market analysis, Skills gap, Data science},
abstract = {Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.}
}
@article{KOTSIOPOULOS2021100341,
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
journal = {Computer Science Review},
volume = {40},
pages = {100341},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100341},
url = {https://www.sciencedirect.com/science/article/pii/S157401372030441X},
author = {Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid},
abstract = {Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.}
}
@article{SUGDEN2020100014,
title = {Patterns of Reliability: Assessing the Reproducibility and Integrity of DNA Methylation Measurement},
journal = {Patterns},
volume = {1},
number = {2},
pages = {100014},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300143},
author = {Karen Sugden and Eilis J. Hannon and Louise Arseneault and Daniel W. Belsky and David L. Corcoran and Helen L. Fisher and Renate M. Houts and Radhika Kandaswamy and Terrie E. Moffitt and Richie Poulton and Joseph A. Prinz and Line J.H. Rasmussen and Benjamin S. Williams and Chloe C.Y. Wong and Jonathan Mill and Avshalom Caspi},
keywords = {DSML 3:  Data science output has been rolled out/validated across multiple domains/problems},
abstract = {Summary
DNA methylation plays an important role in both normal human development and risk of disease. The most utilized method of assessing DNA methylation uses BeadChips, generating an epigenome-wide “snapshot” of >450,000 observations (probe measurements) per assay. However, the reliability of each of these measurements is not equal, and little consideration is paid to consequences for research. We correlated repeat measurements of the same DNA samples using the Illumina HumanMethylation450K and the Infinium MethylationEPIC BeadChips in 350 blood DNA samples. Probes that were reliably measured were more heritable and showed consistent associations with environmental exposures, gene expression, and greater cross-tissue concordance. Unreliable probes were less replicable and generated an unknown volume of false negatives. This serves as a lesson for working with DNA methylation data, but the lessons are equally applicable to working with other data: as we advance toward generating increasingly greater volumes of data, failure to document reliability risks harming reproducibility.}
}
@article{MATE2016131,
title = {A hybrid integrated architecture for energy consumption prediction},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {131-147},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300644},
author = {Alejandro Maté and Jesús Peral and Antonio Ferrández and David Gil and Juan Trujillo},
keywords = {Data mining, Energy consumption, Information Extraction, Big data, Decision trees, Social networks},
abstract = {Irresponsible and negligent use of natural resources in the last five decades has made it an important priority to adopt more intelligent ways of managing existing resources, especially the ones related to energy. The main objective of this paper is to explore the opportunities of integrating internal data already stored in Data Warehouses together with external Big Data to improve energy consumption predictions. This paper presents a study in which we propose an architecture that makes use of already stored energy data and external unstructured information to improve knowledge acquisition and allow managers to make better decisions. This external knowledge is represented by a torrent of information that, in many cases, is hidden across heterogeneous and unstructured data sources, which are recuperated by an Information Extraction system. Alternatively, it is present in social networks expressed as user opinions. Furthermore, our approach applies data mining techniques to exploit the already integrated data. Our approach has been applied to a real case study and shows promising results. The experiments carried out in this work are twofold: (i) using and comparing diverse Artificial Intelligence methods, and (ii) validating our approach with data sources integration.}
}
@article{RISLING201789,
title = {Educating the nurses of 2025: Technology trends of the next decade},
journal = {Nurse Education in Practice},
volume = {22},
pages = {89-92},
year = {2017},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1471595316302748},
author = {Tracie Risling},
keywords = {Informatics, Curriculum development, Technology},
abstract = {The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.}
}
@article{NAM2019411,
title = {Business analytics adoption process: An innovation diffusion perspective},
journal = {International Journal of Information Management},
volume = {49},
pages = {411-423},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311435},
author = {Dalwoo Nam and Junyeong Lee and Heeseok Lee},
keywords = {Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization},
abstract = {Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.}
}
@incollection{DZIUBANY2019239,
title = {Chapter 11 - Machine learning-based artificial nose on a low-cost IoT-hardware},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {239-257},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000117},
author = {Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer},
keywords = {Artificial Nose, PCA, SVM, Feature selection, low cost},
abstract = {In order to make Internet of things applications easily available and cost-effective, we aim at using low-cost hardware for typical measurement tasks, and in return putting more effort into the signal processing and data analysis. By the example of beverage recognition with a low-cost temperature-modulated gas sensor, we demonstrate the benefits of processing techniques in big data such as feature selection and dimensionality reduction. Specifically, we determine a subset of temperatures that yields good support vector machine classification results and thereby shortens the measurement process.}
}
@article{CHUNG2020101837,
title = {Data science and analytics in aviation},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {134},
pages = {101837},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.101837},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520300077},
author = {Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi},
keywords = {Data science, Aviation, Analytics, Flight, Air logistics},
abstract = {Data science and analytics are attracting more and more attention from researchers and practitioners in recent years. Due to the rapid development of advanced technologies nowadays, a massive amount of real time data regarding flight information, flight performance, airport conditions, air traffic conditions, weather, ticket prices, passengers comments, crew comments, etc., are all available from a diverse set of sources, including flight performance monitoring systems, operational systems of airlines and airports, and social media platforms. Development of data analytics in aviation and related applications is also growing rapidly. This paper concisely examines data science and analytics in aviation studies in several critical areas, namely big data analysis, air transport network management, forecasting, and machine learning. The papers featured in this special issue are also introduced and reviewed, and future directions for data science and analytics in aviation are discussed.}
}
@article{DING2021,
title = {An Internet of Things Based Scalable Framework for Disaster Data Management},
journal = {Journal of Safety Science and Resilience},
year = {2021},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666449621000542},
author = {Zhiming Ding and Shan Jiang and Xinrun Xu and Yanbo Han},
keywords = {Disaster Data Management, IoT, Disaster Detection, Big data, Artificial Intelligence},
abstract = {In recent years, undesirable disasters attacked the cities frequently, leaving heavy casualties and serious economic losses. Meanwhile, disaster detection based on the Internet of Things(IoT) has become a hot spot benefited by the established development of smart city construction. And the IoT is visibly sensitive to the management and monitor of disasters, but massive amounts of monitoring data have brought huge challenges to data storage and data analysis. This article develops a new and much more general framework for disaster emergency management under the IoT environment. The framework is a bottom-up integration of highly scalable Raw Data Storages(RD-Stores) technology, hybrid indexing and queries technology, and machine learning technology for emergency disasters. Experimental results show that hybrid index and query technology have better performance under the condition of supporting multi-modal retrieval, and providing a better solution to offer real-time retrieval for the massive sensor sampling data in the IoT. In addition, further works to evaluate the top-level sub-application system in this framework were performed based on the GPS trajectory data of 35,000 Beijing taxis and the volumetric ground truth data of 7,500 images. The results show that the framework has desirable scalability and higher utility.}
}
@article{NILASHI2021102630,
title = {Big social data and customer decision making in vegetarian restaurants: A combined machine learning method},
journal = {Journal of Retailing and Consumer Services},
volume = {62},
pages = {102630},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102630},
url = {https://www.sciencedirect.com/science/article/pii/S096969892100196X},
author = {Mehrbakhsh Nilashi and Hossein Ahmadi and Goli Arji and Khalaf Okab Alsalem and Sarminah Samad and Fahad Ghabban and Ahmed Omar Alzahrani and Ali Ahani and Ala Abdulsalam Alarood},
keywords = {Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation},
abstract = {Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer’ satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers’ online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.}
}
@incollection{SEBASTIANCOLEMAN2022131,
title = {Chapter 7 - The People Challenge: Building Data Literacy},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {131-164},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000079},
author = {Laura Sebastian-Coleman},
keywords = {Data literacy, data visualization, analytics, metadata management, critical thinking, scientific thinking, data management},
abstract = {This chapter addresses the skills, knowledge, and experience people require to create, use, and interpret data. Data literacy is the ability to read, understand, interpret, and learn from data in different contexts and to communicate about data with other people. The people challenge is both a skills challenge and a knowledge challenge. No single individual can know everything about an organization’s data. But, together, people can solve more problems in better ways if they understand data as a construct, recognize the risks associated with data production and use, cultivate a level of skepticism about data, and develop skill in visualizing and interpreting data. They will solve even more problems if the organization supports these efforts through disciplined metadata management and data quality management.}
}
@article{YUAN201786,
title = {Exploring inter-country connection in mass media: A case study of China},
journal = {Computers, Environment and Urban Systems},
volume = {62},
pages = {86-96},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2016.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516303313},
author = {Yihong Yuan and Yu Liu and Guixing Wei},
keywords = {Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT},
abstract = {The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.}
}
@incollection{LARKIN2022283,
title = {Chapter Five - Connecting marine data to society},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {283-317},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000037},
author = {Kate E. Larkin and Andrée-Anne Marsan and Nathalie Tonné and Nathalie {Van Isacker} and Tim Collart and Conor Delaney and Mickaël Vasquez and Eleonora Manca and Helen Lillis and Jan-Bart Calewaert},
keywords = {Big data, Climate change, Data visualization, Digital ocean, Ecosystems, FAIR, Hackathon, Knowledge broker, Marine data, Marine map, Ocean, Ocean literacy, Open data, Science communication, Seabed habitats},
abstract = {This chapter looks at connecting marine data to society, with a focus on key developments in Europe, set in a global context. It presents the European Marine Observation and Data Network-EMODnet as an exemplar in marine domain. EMODnet has significantly advanced European capability for Findable, Accessible, Interoperable, and Reusable marine knowledge, offering access to standardized and harmonized in-situ marine data and added value data products across seven marine environmental themes. Open and free data, products and associated metadata, are available for discovery and access through a wide range of web/data services. These ensure that the wealth of existing ocean observations and marine data collected in Europe and beyond can be easily discovered and used by a growing, and diversifying, user community. Interoperability with key services is crucial toward a pan-European and global approach. Key partnerships include the Copernicus Marine Environment Monitoring Service and international initiatives, e.g., the International Oceanographic Data and Information Exchange. Looking at societal tools and applications, the chapter provides a case study of the European Atlas of the Seas, a web-mapping tool that communicates marine and other open-source data and information in an attractive and interactive way. The EU Atlas is a key tool for the European ocean literacy initiative EU4Ocean, contributing to engage citizens and drive the societal change that is required for Europe to meet the ambitious targets to be climate neutral by 2050. The paper introduces examples of emerging tools for data visualization and presents hackathons, a powerful method to cocreate and innovate applications for society. Finally, the chapter looks toward the digital era and addresses the emerging challenges and opportunities of marine data, e.g., big data and plans for a digital twin of the Ocean, as tools to enable a step-change in societal connection, understanding, and action regarding the ocean.}
}
@article{PIRI2020113339,
title = {Missing care: A framework to address the issue of frequent missing values;The case of a clinical decision support system for Parkinson's disease},
journal = {Decision Support Systems},
volume = {136},
pages = {113339},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113339},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300944},
author = {Saeed Piri},
keywords = {Electronic health records, Data missing values, Clinical decision support systems, Predictive healthcare analytics, Imbalanced data learning, Parkinson's disease},
abstract = {In recent decades, the implementation of electronic health record (EHR) systems has been evolving worldwide, leading to the creation of immense data volume in healthcare. Moreover, there has been a call for research studies to enhance personalized medicine and develop clinical decision support systems (CDSS) by analyzing the available EHR data. In EHR data, usually, there are millions of patients records with hundreds of features collected over a long period of time. This enormity of EHR data poses significant challenges, one of which is dealing with many variables with very high degrees of missing values. In this study, the data quality issue of incompleteness in EHR data is discussed, and a framework called ‘Missing Care’ is introduced to address this issue. Using Missing Care, researchers will be able to select the most important variables at an acceptable missing values degree to develop predictive models with high predictive power. Moreover, Missing Care is applied to analyze a unique, large EHR data to develop a CDSS for detecting Parkinson's disease. Parkinson is a complex disease, and even a specialist's diagnosis is not without error. Besides, there is a lack of access to specialists in more remote areas, and as a result, about half of the patients with Parkinson's disease in the US remain undiagnosed. The developed CDSS can be integrated into EHR systems or utilized as an independent tool by healthcare practitioners who are not necessarily specialists; therefore, making up for the limited access to specialized care in remote areas.}
}
@article{MA2020109941,
title = {A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data},
journal = {Energy and Buildings},
volume = {216},
pages = {109941},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109941},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819333717},
author = {Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai},
keywords = {Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning},
abstract = {Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.}
}
@incollection{CAVALCANTE2019265,
title = {Chapter 4-2 - Novel Bioinformatics Methods for Toxicoepigenetics},
editor = {Shaun D. McCullough and Dana C. Dolinoy},
booktitle = {Toxicoepigenetics},
publisher = {Academic Press},
pages = {265-288},
year = {2019},
isbn = {978-0-12-812433-8},
doi = {https://doi.org/10.1016/B978-0-12-812433-8.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124338000125},
author = {Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor},
keywords = {Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions},
abstract = {The use of high-throughput, genome-wide assays in toxicoepigenetics is rapidly developing and expanding. With recent advances in experimental technologies, a great amount of multiomics epigenomic data has been generated requiring the development of correspondingly advanced bioinformatics approaches to analyze and interpret such big data. This chapter discusses analysis methods for current epigenomic assays available for use in toxicoepigenetic and novel bioinformatics approaches to interpret, visualize, and integrate a variety of epigenomic data and data resources. The epigenomic features covered include DNA methylation, DNA hydroxymethylation, histone modification, chromatin accessibility, and chromatin interaction. For each type of assay used to interrogate those features, bioinformatics tools for data quality control, epigenetic mark detection, comparative analysis, data visualization, functional analysis, and integrative analysis are suggested. Looking forward, it is anticipated that researchers in toxicoepigenomics will adopt newer techniques such as single-cell assays and the bioinformatics methods will continue to evolve.}
}
@article{CHAKRABORTY2021662,
title = {The role of surrogate models in the development of digital twins of dynamic systems},
journal = {Applied Mathematical Modelling},
volume = {90},
pages = {662-681},
year = {2021},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2020.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X20305588},
author = {S. Chakraborty and S. Adhikari and R. Ganguli},
keywords = {Digital twin, Vibration, Response, Frequency, Surrogate},
abstract = {Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.}
}
@article{ZHAO2020264,
title = {The Value of the Surgeon Informatician},
journal = {Journal of Surgical Research},
volume = {252},
pages = {264-271},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022480420302079},
author = {Jane Zhao and Raquel Forsythe and Alexander Langerman and Genevieve B. Melton and David F. Schneider and Gretchen Purcell Jackson},
keywords = {Clinical informatics, Surgery, Health information technology, Interoperability, Telemedicine, Clinical decision support},
abstract = {Clinical informatics is an interdisciplinary specialty that leverages big data, health information technologies, and the science of biomedical informatics within clinical environments to improve quality and outcomes in the increasingly complex and often siloed health care systems. Core competencies of clinical informatics primarily focus on clinical decision making and care process improvement, health information systems, and leadership and change management. Although the broad relevance of clinical informatics is apparent, this review focuses on its application and pertinence to the discipline of surgery, which is less well defined. In doing so, we hope to highlight the importance of the surgeon informatician. Topics covered include electronic health records, clinical decision support systems, computerized order entry, data analytics, clinical documentation, information architectures, implementation science, quality improvement, simulation, education, and telemedicine. The formal pathway for surgeons to become clinical informaticians is also discussed.}
}
@article{KHATRI2016673,
title = {Managerial work in the realm of the digital universe: The role of the data triad},
journal = {Business Horizons},
volume = {59},
number = {6},
pages = {673-688},
year = {2016},
note = {CYBERSECURITY IN 2016: PEOPLE, TECHNOLOGY, AND PROCESSES},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681316300519},
author = {Vijay Khatri},
keywords = {Analytics, Big data, Managerial decision making, Managerial work, Digital universe},
abstract = {With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.}
}
@incollection{CINNAMON202057,
title = {Geographic Information Systems; Ethics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {57-62},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10554-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105542},
author = {Jonathan Cinnamon},
keywords = {Access, Big data, Cartography, Digital divide, Geodemographics, Mapping, Morals, Privacy, Profiling, Representation, Responsibilities, Spatial data, Surveillance, Values},
abstract = {The development and use of geographic information system (GIS) within particular sociopolitical contexts means that ethical issues can arise both from how GIS is used and also due to the affordances and constraints of the software, hardware, and data. Ethics is a longstanding concern in the field of geographic information science (GIScience), arising amid the critical cartography and GIS and Society debates beginning in the late 1980s, in which human geographers and GIS scholars began to call for more attention to the implications of maps, GIS, and spatial data. Ethics in GIS draws on normative frameworks including deontological (duties and obligations) and teleological (consequences and outcomes) ethical perspectives, as well as nonnormative critical ethics to understand concerns that arise with GIS and map-based representation, uneven access to spatial data and technologies, and the use of GIS in geodemographic profiling, location analytics, and war. Attention to ethics in GIS has led to the development of ethics education, guidance, and codes of conduct for GIS users. Recent advancements in the availability of geolocated personal data, wider societal use of geospatial technologies, and data analytics have pulled GIS ethics to the forefront of the larger domain of information ethics, as location becomes increasingly central to wider ethical debates in the era of big data, automation, and artificial intelligence.}
}
@incollection{LINSTEDT20161,
title = {Chapter 1 - Introduction to Data Warehousing},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-15},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000015},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {data, data warehouse, big data, decision support systems, scalability, business intelligence},
abstract = {This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.}
}
@article{LIM2018121,
title = {From data to value: A nine-factor framework for data-based value creation in information-intensive services},
journal = {International Journal of Information Management},
volume = {39},
pages = {121-135},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217300816},
author = {Chiehyeon Lim and Ki-Hun Kim and Min-Jun Kim and Jun-Yeon Heo and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Big data, Data-based value creation, Information-intensive service, Factor, Data–Value Chain},
abstract = {Service is a key context for the application of IT, as IT digitizes information interactions in service and facilitates value creation, thereby contributing to service innovation. The recent proliferation of big data provides numerous opportunities for information-intensive services (IISs), in which information interactions exert the greatest effect on value creation. In the modern data-rich economy, understanding mechanisms and related factors of data-based value creation in IISs is essential for using IT to improve such services. This study identified nine key factors that characterize this data-based value creation: (1) data source, (2) data collection, (3) data, (4) data analysis, (5) information on the data source, (6) information delivery, (7) customer (information user), (8) value in information use, and (9) provider network. These factors were identified and defined through six action research projects with industry and government that used specific datasets to design new IISs and by analyzing data usage in 149 IIS cases. This paper demonstrates the usefulness of these factors for describing, analyzing, and designing the entire value creation chain, from data collection to value creation, in IISs. The main contribution of this study is to provide a simple yet comprehensive and empirically tested basis for the use and management of data to facilitate service value creation.}
}
@article{HE2022100140,
title = {GARD: Gender difference analysis and recognition based on machine learning},
journal = {Array},
volume = {14},
pages = {100140},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100140},
url = {https://www.sciencedirect.com/science/article/pii/S259000562200011X},
author = {Shiwen He and Jian Song and Yeyu Ou and Yuanhong Yuan and Xiaojie Zhang and Xiaohua Xu},
keywords = {Gender difference analysis, Gender recognition, Medical examination data, Machine learning},
abstract = {In recent years, intelligent diagnosis and intelligent medical treatment based on big data of medical examinations have become the main trend of medical development in the future. In this paper, we propose a method for analyzing the difference between males and females in medical examination items (medical attributes) and find that males and females of different ages have differences in medical attributes. Then, the cluster analysis method is used to further analyze the differences between male and female in medical examination items, such that some common important attributes (CIAs) that can be used for gender recognition are found within a specific age range. Following, we propose two gender recognition models (GRMs) by using the found CIAs to identify the gender. A large number of experimental results are provided to validate the effectiveness of the proposed GRMs. Experimental results show that the medical attributes with a large value of difference really contribute to gender recognition. Within a certain age range, such as 17 to 51 years old, the proposed GRM can reach 92.8% accuracy using only six medical attributes.}
}
@article{SQUITIERI2020231,
title = {Deriving Evidence from Secondary Data in Hand Surgery: Strengths, Limitations, and Future Directions},
journal = {Hand Clinics},
volume = {36},
number = {2},
pages = {231-243},
year = {2020},
note = {Health Policy and Advocacy in Hand Surgery},
issn = {0749-0712},
doi = {https://doi.org/10.1016/j.hcl.2020.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0749071220300111},
author = {Lee Squitieri and Kevin C. Chung},
keywords = {Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis}
}
@article{LIU2020393,
title = {Wind speed forecasting using deep neural network with feature selection},
journal = {Neurocomputing},
volume = {397},
pages = {393-403},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.108},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304148},
author = {Xiangjie Liu and Hao Zhang and Xiaobing Kong and Kwang Y. Lee},
keywords = {Wind speed forecasting, Deep neural network, Mutual information, Stacked auto-encoder, Denoising, Long short-term memory network},
abstract = {With the rapid growth of wind power penetration into modern power grids, wind speed forecasting (WSF) becomes an increasing important task in the planning and operation of electric power and energy systems. However, WSF is quite challengeable due to its highly varying and complex features. In this paper, a novel hybrid deep neural network forecasting method is constituted. A feature selection method based on mutual information is developed in the WSF problem. With the real-time big data from the wind farm running log, the deep neural network model for WSF is established using a stacked denoising auto-encoder and long short-term memory network. The effectiveness of the deep neural network is evaluated by 10-minutes-ahead WSF. Comparing with the traditional multi-layer perceptron network, conventional long short-term memory network and stacked auto-encoder, the resulting deep neural network significantly improves the forecasting accuracy.}
}
@article{ZHOU2020e667,
title = {Artificial intelligence in COVID-19 drug repurposing},
journal = {The Lancet Digital Health},
volume = {2},
number = {12},
pages = {e667-e676},
year = {2020},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(20)30192-8},
url = {https://www.sciencedirect.com/science/article/pii/S2589750020301928},
author = {Yadi Zhou and Fei Wang and Jian Tang and Ruth Nussinov and Feixiong Cheng},
abstract = {Summary
Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.}
}
@incollection{MCGILVRAY202173,
title = {Chapter 4 - The Ten Steps Process},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {73-252},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000062},
author = {Danette McGilvray},
keywords = {business needs, information environment, information life cycle, data quality dimensions, business impact techniques, root causes, improvement, correction, prevention, controls, monitor, communicate, ethics, change management},
abstract = {This chapter contains the step-by-step guide for creating, assessing, improving, sustaining, and managing information and data quality. Concrete instructions, sample output and templates, and practical advice for executing every step of the Ten Steps Process are provided. A step summary table gives an at-a-glance overview of objectives, purpose, inputs and outputs, techniques and tools, communication, and checkpoints for each step. The Ten Steps Process was designed to be flexible. Suggestions are given to help the reader select and adjust the Ten Steps to various situations, business needs, and data quality issues. The layout allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, best practices, and warnings. The experience of actual clients and users of the Ten Steps are highlighted in callout boxes called Ten Steps in Action.}
}
@article{LIU202131,
title = {Objects detection toward complicated high remote basketball sports by leveraging deep CNN architecture},
journal = {Future Generation Computer Systems},
volume = {119},
pages = {31-36},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000303},
author = {Long Liu},
keywords = {Object detection, Sport action recognition, Image recognition, Basketball recognition},
abstract = {The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes’ skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.}
}
@article{JASEENA2020,
title = {Deterministic weather forecasting models based on intelligent predictors: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304729},
author = {K.U. Jaseena and Binsu C. Kovoor},
keywords = {Weather forecasting, Artificial neural networks, Deep learning, Autoencoders, Recurrent neural networks},
abstract = {Weather forecasting is the practice of predicting the state of the atmosphere for a given location based on different weather parameters. Weather forecasts are made by gathering data about the current state of the atmosphere. Accurate weather forecasting has proven to be a challenging task for meteorologists and researchers. Weather information is essential in every facet of life like agriculture, tourism, airport system, mining industry, and power generation. Weather forecasting has now entered the era of Big Data due to the advancement of climate observing systems like satellite meteorological observation and also because of the fast boom in the volume of weather data. So, the traditional computational intelligence models are not adequate to predict the weather accurately. Hence, deep learning-based techniques are employed to process massive datasets that can learn and make predictions more effectively based on past data. The effective implementation of deep learning in various domains has motivated its use in weather forecasting and is a significant development for the weather industry. This paper provides a thorough review of different weather forecasting approaches, along with some publicly available datasets. This paper delivers a precise classification of weather forecasting models and discusses potential future research directions in this area.}
}
@article{MOHDFAIZAL2021106190,
title = {A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106190},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106190},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721002649},
author = {Aizatul Shafiqah {Mohd Faizal} and T. Malathi Thevarajah and Sook Mei Khor and Siow-Wee Chang},
keywords = {Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning},
abstract = {Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.}
}
@article{MANTELERO2016238,
title = {Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection},
journal = {Computer Law & Security Review},
volume = {32},
number = {2},
pages = {238-255},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916300280},
author = {Alessandro Mantelero},
keywords = {Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment},
abstract = {In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.}
}
@article{DUIN2020102544,
title = {The Current State of Analytics: Implications for Learning Management System (LMS) Use in Writing Pedagogy},
journal = {Computers and Composition},
volume = {55},
pages = {102544},
year = {2020},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102544},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300050},
author = {Ann Hill Duin and Jason Tham},
keywords = {Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access},
abstract = {Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.}
}
@article{MONDEJAR2021148539,
title = {Digitalization to achieve sustainable development goals: Steps towards a Smart Green Planet},
journal = {Science of The Total Environment},
volume = {794},
pages = {148539},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.148539},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721036111},
author = {Maria E. Mondejar and Ram Avtar and Heyker Lellani Baños Diaz and Rama Kant Dubey and Jesús Esteban and Abigail Gómez-Morales and Brett Hallam and Nsilulu Tresor Mbungu and Chukwuebuka Christopher Okolo and Kumar Arun Prasad and Qianhong She and Sergi Garcia-Segura},
keywords = {Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development},
abstract = {Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.}
}
@article{LAMAAZI2022151,
title = {Smart-3DM: Data-driven decision making using smart edge computing in hetero-crowdsensing environment},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {151-165},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200022X},
author = {Hanane Lamaazi and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Ernesto Damiani},
keywords = {Smart edge computing, Crowdsensing, Distributed architecture, Data assessment, Data quality},
abstract = {Mobile Edge Computing (MEC) has recently emerged as a promising paradigm for Mobile Crowdsensing (MCS) environments. In a given Area of Interest (AoI), the sensing process is performed based on task requirements, which usually ask for a specific quality of the sensing outcome. In this work, a two-stage Data-Driven Decision-making Mechanism using smart edge computing (Smart-3DM) is proposed. It advocates the use of smart edge to better fulfill the data-related task requirements. Depending on the type of data to be collected, the minimum quality of the data required, and the heuristics to apply for each type of crowdsensing service, the smart edge orchestrates the selection of workers in MEC. Our approach relies on (a) smart-edge deployment: where a cluster-based distributed architecture using smart edge nodes is considered. Here, two entities are defined: the main edge node (MEN) and the local edge nodes (LENs); and (b) data management offloading where a two-layer re-selection strategy that considers data type and context-awareness is adopted, to reduce data computation complexity and to increase data quality while meeting the task target. The proposed Smart-3DM is evaluated using a real-life dataset and is compared to one-stage local and global approaches. The overall results show that by using two-stage re-selection strategies, better performance with lower processing power (CPU), less Storage(RAM), and improved execution time is achieved, when compared to the benchmarks.}
}
@article{SUN2022105034,
title = {A review of Earth Artificial Intelligence},
journal = {Computers & Geosciences},
volume = {159},
pages = {105034},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000036},
author = {Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John},
keywords = {Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure},
abstract = {In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.}
}
@article{ZHU202011283,
title = {Supervised Block-Aware Factorization Machine for Multi-Block Quality-Relevant Monitoring},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11283-11288},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320306546},
author = {Qinqin Zhu},
keywords = {Quality-relevant monitoring, block-aware factorization machine, supervised learning, multi-block processes},
abstract = {Multi-block multivariate statistical methods have been developed to extract useful information from process and quality data in the era of big data, where process variables are partitioned into several meaningful blocks. However, most of these methods did not consider cross-correlations among divided blocks, which leads to inferior monitoring performance. In this article, a block-aware factorization machine (BAFM) algorithm is proposed to exploit information from process and quality data. In BAFM, quality data are first classified into normal and abnormal labels with principal component analysis based quality monitoring framework. Afterwards, a block number is attached to each process variable, and the interactions among different variables (both within and cross blocks) are learned through latent variables, which is supervised by the classified quality labels. Apart from the variable relation within the same block, BAFM also incorporates the block information; thus, both inner and cross correlations are constructed. The monitoring framework based on BAFM is developed, and its effectiveness and superiority are demonstrated through the Tennessee Eastman process.}
}
@incollection{PEZOULAS202067,
title = {Chapter 3 - Medical data sharing},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {67-104},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000037},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Data curation, Data quality management, Data sharing, Data sharing frameworks, Standardization},
abstract = {This chapter introduces the primary step toward the realization of a strategically federated platform, namely data sharing. The rationale behind medical data sharing is related to the interlinking of medical cohorts with respect to data protection regulations and dealing with the unmet needs in various clinical domains. Emphasis is given on data quality management and especially on the existence of a data curation mechanism toward an effective data quality assessment procedure. Data curation and standardization methods are presented along with the latest advances in data sharing assessment and case studies on real data. Existing data sharing frameworks and global initiatives are extensively discussed. Crucial barriers toward data sharing are finally stated along with solutions and guidelines against the misuse of shared data.}
}
@article{CHAN2022112017,
title = {Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence},
journal = {Energy and Buildings},
volume = {262},
pages = {112017},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822001888},
author = {K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao},
keywords = {Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving},
abstract = {Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700 kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.}
}
@article{BENJELLOUN20211177,
title = {Improving outliers detection in data streams using LiCS and voting},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {10},
pages = {1177-1185},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819301454},
author = {Fatima-Zahra Benjelloun and Ahmed Oussous and Amine Bennani and Samir Belfkih and Ayoub {Ait Lahcen}},
keywords = {Data streams, Outlier detection, High-dimensional data, Big data mining, Intrusion detection},
abstract = {Detecting outliers in real-time is increasingly important for many real-world applications such as detecting abnormal heart activity, intrusions to systems, spams or abnormal credit card transactions. However, detecting outliers in data streams rises many challenges such as high-dimensionality, dynamic data distribution and unpredictable relationships. Our simulations demonstrate that some advanced solutions still show drawbacks. In this paper, first, we improve the capacity to detect outliers of both micro-clusters based algorithms (MCOD) and distance-based algorithms (Abstract-C and Exact-Storm) known for their performance. This is by adding a layer called LiCS that classifies online the K-nearest-neighbors (Knn) of each node based on their evolutionary status. This layer aggregates the results and uses a count threshold to better classify nodes. Experiments on SpamBase datasets confirmed that our technique enhances the accuracy and the precision of such algorithm and helps to reduce the unclassified nodes.Second, we propose a hybrid solution based on iterative majority voting and our LiCS. Experiments on real data proves that it outperforms discussed algorithms in terms of accuracy, precision and sensitivity in detecting outliers. It also minimizes the issue of unclassified instances and consolidate the different outputs of algorithms.}
}
@article{BAHLO2021106365,
title = {Livestock data – Is it there and is it FAIR? A systematic review of livestock farming datasets in Australia},
journal = {Computers and Electronics in Agriculture},
volume = {188},
pages = {106365},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106365},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003823},
author = {Christiane Bahlo and Peter Dahlhaus},
keywords = {Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming},
abstract = {The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.}
}
@article{JIN2021202,
title = {Lidar sheds new light on plant phenomics for plant breeding and management: Recent advances and future prospects},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {171},
pages = {202-223},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620303130},
author = {Shichao Jin and Xiliang Sun and Fangfang Wu and Yanjun Su and Yumei Li and Shiling Song and Kexin Xu and Qin Ma and Frédéric Baret and Dong Jiang and Yanfeng Ding and Qinghua Guo},
keywords = {Lidar, Traits, Phenomics, Breeding, Management, Multi-omics},
abstract = {Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial–temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial–temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.}
}
@article{PANG2021104454,
title = {Prediction of early childhood obesity with machine learning and electronic health record data},
journal = {International Journal of Medical Informatics},
volume = {150},
pages = {104454},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104454},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000800},
author = {Xueqin Pang and Christopher B. Forrest and Félice Lê-Scherban and Aaron J. Masino},
keywords = {Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction},
abstract = {Objective
This study compares seven machine learning models developed to predict childhood obesity from age > 2 to ≤ 7 years using Electronic Healthcare Record (EHR) data up to age 2 years.
Materials and methods
EHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children’s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing.
Results
XGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %).
Discussion and conclusion
Early childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.}
}
@article{BETTS202035,
title = {Predicting postpartum psychiatric admission using a machine learning approach},
journal = {Journal of Psychiatric Research},
volume = {130},
pages = {35-40},
year = {2020},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395620308669},
author = {Kim S. Betts and Steve Kisely and Rosa Alati},
keywords = {Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning},
abstract = {Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.}
}
@article{KRIEGER2021100511,
title = {Explaining the (non-) adoption of advanced data analytics in auditing: A process theory},
journal = {International Journal of Accounting Information Systems},
volume = {41},
pages = {100511},
year = {2021},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2021.100511},
url = {https://www.sciencedirect.com/science/article/pii/S1467089521000130},
author = {Felix Krieger and Paul Drews and Patrick Velte},
keywords = {Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation},
abstract = {Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client’s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.}
}
@article{CHAUHAN2022121508,
title = {Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {177},
pages = {121508},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121508},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522000403},
author = {Chetna Chauhan and Vinit Parida and Amandeep Dhir},
keywords = {Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things},
abstract = {The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.}
}
@article{GHASEMAGHAEI2018101,
title = {Data analytics competency for improving firm decision making performance},
journal = {The Journal of Strategic Information Systems},
volume = {27},
number = {1},
pages = {101-113},
year = {2018},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717300768},
author = {Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein},
keywords = {Data analytics competency, Data quality, Bigness of data, Analytical skills, Domain knowledge, Tools sophistication, Decision making performance},
abstract = {This study develops and validates the concept of Data Analytics Competency as a five multidimensional formative index (i.e., data quality, bigness of data, analytical skills, domain knowledge, and tools sophistication) and empirically examines its impact on firm decision making performance (i.e., decision quality and decision efficiency). The findings based on an empirical analysis of survey data from 151 Information Technology managers and data analysts demonstrate a large, significant, positive relationship between data analytics competency and firm decision making performance. The results reveal that all dimensions of data analytics competency significantly improve decision quality. Furthermore, interestingly, all dimensions, except bigness of data, significantly increase decision efficiency. This is the first known empirical study to conceptualize, operationalize and validate the concept of data analytics competency and to study its impact on decision making performance. The validity of the data analytics competency construct as conceived and operationalized, suggests the potential for future research evaluating its relationships with possible antecedents and consequences. For practitioners, the results provide important guidelines for increasing firm decision making performance through the use of data analytics.}
}
@article{RAHIMI2014768,
title = {Validating an ontology-based algorithm to identify patients with Type 2 Diabetes Mellitus in Electronic Health Records},
journal = {International Journal of Medical Informatics},
volume = {83},
number = {10},
pages = {768-778},
year = {2014},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505614001038},
author = {Alireza Rahimi and Siaw-Teng Liaw and Jane Taggart and Pradeep Ray and Hairong Yu},
keywords = {Ontology, SPARQL, Electronic Health Records, Diabetes Mellitus, Type 2, Validation studies},
abstract = {Background
Improving healthcare for people with chronic conditions requires clinical information systems that support integrated care and information exchange, emphasizing a semantic approach to support multiple and disparate Electronic Health Records (EHRs). Using a literature review, the Australian National Guidelines for Type 2 Diabetes Mellitus (T2DM), SNOMED-CT-AU and input from health professionals, we developed a Diabetes Mellitus Ontology (DMO) to diagnose and manage patients with diabetes. This paper describes the manual validation of the DMO-based approach using real world EHR data from a general practice (n=908 active patients) participating in the electronic Practice Based Research Network (ePBRN).
Method
The DMO-based algorithm to query, using Semantic Protocol and RDF Query Language (SPARQL), the structured fields in the ePBRN data repository were iteratively tested and refined. The accuracy of the final DMO-based algorithm was validated with a manual audit of the general practice EHR. Contingency tables were prepared and Sensitivity and Specificity (accuracy) of the algorithm to diagnose T2DM measured, using the T2DM cases found by manual EHR audit as the gold standard. Accuracy was determined with three attributes – reason for visit (RFV), medication (Rx) and pathology (path) – singly and in combination.
Results
The Sensitivity and Specificity of the algorithm were 100% and 99.88% with RFV; 96.55% and 98.97% with Rx; and 15.6% and 98.92% with Path. This suggests that Rx and Path data were not as complete or correct as the RFV for this general practice, which kept its RFV information complete and current for diabetes. However, the completeness is good enough for this purpose as confirmed by the very small relative deterioration of the accuracy (Sensitivity and Specificity of 97.67% and 99.18%) when calculated for the combination of RFV, Rx and Path. The manual EHR audit suggested that the accuracy of the algorithm was influenced by data quality such as incorrect data due to mistaken units of measurement and unavailable data due to non-documentation or documented in the wrong place or progress notes, problems with data extraction, encryption and data management errors.
Conclusion
This DMO-based algorithm is sufficiently accurate to support a semantic approach, using the RFV, Rx and Path to define patients with T2DM from EHR data. However, the accuracy can be compromised by incomplete or incorrect data. The extent of compromise requires further study, using ontology-based and other approaches.}
}
@article{BELLINI2014827,
title = {Km4City ontology building vs data harvesting and cleaning for smart-city services},
journal = {Journal of Visual Languages & Computing},
volume = {25},
number = {6},
pages = {827-839},
year = {2014},
note = {Distributed Multimedia Systems DMS2014 Part I},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2014.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X14001165},
author = {Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch},
keywords = {Smart city, Knowledge base construction, Reconciliation, Validation and verification of knowledge base, Smart city ontology, Linked open graph, Km4city},
abstract = {Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.}
}
@article{BOOMGARDZAGRODNIK2022106580,
title = {Machine learning imputation of missing Mesonet temperature observations},
journal = {Computers and Electronics in Agriculture},
volume = {192},
pages = {106580},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106580},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005974},
author = {Joseph P. Boomgard-Zagrodnik and David J. Brown},
keywords = {Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation},
abstract = {Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.}
}
@article{OLARU201854,
title = {Workshop Synthesis: Passive and sensor data - potential and application},
journal = {Transportation Research Procedia},
volume = {32},
pages = {54-61},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301613},
author = {Doina Olaru and Alejandro Tudela},
keywords = {passive data, big data, sensor, GPS, Wi-Fi, smartcard, transport, travel surveys},
abstract = {The workshop on technology, tools and applications around passive and sensor travel data is summarized in this paper. Such data requires protocols for collection, storing/retrieving, sharing and processing; as well as the need for validation methods and multi-disciplinary work. Traditional surveys can complement passive and sensor data to aid a deeper understanding of travel behaviour. Passive data is particularly beneficial for planning long-distance travel and freight transport. While access to massive data collected by licensed operators should be guaranteed, maintaining data privacy and cultural sensitivity is a priority.}
}
@incollection{MALCOM2014325,
title = {Chapter 14 - Analysis of Deep Sequencing Data: Insights and Challenges},
editor = {Carolina Simó and Alejandro Cifuentes and Virginia García-Cañas},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {63},
pages = {325-354},
year = {2014},
booktitle = {Fundamentals of Advanced Omics Technologies: From Genes to Metabolites},
issn = {0166-526X},
doi = {https://doi.org/10.1016/B978-0-444-62651-6.00015-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444626516000155},
author = {Jacob W. Malcom and John H. Malone},
keywords = {Deep sequencing, Seq, Big data, Statistical analysis},
abstract = {Modern biomedical research demands that investigators become familiar with deep sequencing data analysis, yet the vast nature of deep sequencing data creates a variety of roadblocks for biologists not familiar with the analysis of such large datasets. In this chapter, we provide an introduction to data analysis for biologists, review first principles, point out areas of concern, and suggest software tools that are becoming standards for analysis of deep sequencing data. Perhaps the biggest challenge in the analysis of deep sequencing data will be data management and storage and repeating complex, multitier computational analyses. The future of deep sequencing data analysis will be likely data-driven and rely on principles gleaned from “big data” analysis.}
}
@article{KIRTLEY2022243,
title = {Translating promise into practice: a review of machine learning in suicide research and prevention},
journal = {The Lancet Psychiatry},
volume = {9},
number = {3},
pages = {243-252},
year = {2022},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00254-6},
url = {https://www.sciencedirect.com/science/article/pii/S2215036621002546},
author = {Olivia J Kirtley and Kasper {van Mens} and Mark Hoogendoorn and Navneet Kapur and Derek {de Beurs}},
abstract = {Summary
In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need—ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.}
}
@incollection{FILCHEV2020103,
title = {Chapter 6 - Surveys, Catalogues, Databases/Archives, and State-of-the-Art Methods for Geoscience Data Processing},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {103-136},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00016-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000163},
author = {Lachezar Filchev and Lyubka Pashova and Vasil Kolev and Stuart Frye},
keywords = {Geosensor networks, Remote sensing, Earth observations, Geoinformation, Big Data, Databases/archives, Satellite image processing, Geographic information systems, Web geoportals, ICT, Decision analysis and technologies, Spectral imaging, Fourier analysis, Principal component analysis, Karhunen–Loève transform, Continuous and discrete wavelet, Multiwavelet transforms, Hyperspectral images, Classification methods, Image denoising},
abstract = {Recent years are marked with rapid growth in sources and availability of geospatial data and information providing new opportunities and challenges for scientific knowledge and technology solutions on time. This chapter represents a general overview of modern ICT tools and methods for acquiring Earth observation (EO) data storage, processing, analysis, and interpretation for many research and applied purposes. The main contribution to Big Data developments in EO is the space activities of the space and governmental agencies, such as CNES, CSA, CSIRO, DLR, ESA, INPE, ISRO, JAXA, NASA, RADI, and Roscosmos. Special attention is devoted to the international archives, catalogues, and databases of satellite EO, which already become an indispensable and crucial source of information in support of many sectors of social-economic activities and resolving environmental issues. Main technological and information products, geoportals, and services to deal with Big EO datasets are shortly discussed. Some advanced contemporary approaches for processing big EO data, compressing, clustering, and denoising, and hyperspectral images in the geoinformation science are outlined.}
}
@article{ZHANG2021103691,
title = {Perspectives of big experimental database and artificial intelligence in tunnel fire research},
journal = {Tunnelling and Underground Space Technology},
volume = {108},
pages = {103691},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103691},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820306453},
author = {Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani},
keywords = {Big data, Empirical model, Deep learning, Critical event, Smart firefighting},
abstract = {Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.}
}
@article{MIGLANI202137,
title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review},
journal = {Computer Communications},
volume = {178},
pages = {37-63},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002632},
author = {Arzoo Miglani and Neeraj Kumar},
keywords = {Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G},
abstract = {Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.}
}
@article{JI2020103459,
title = {Converting clinical document architecture documents to the common data model for incorporating health information exchange data in observational health studies: CDA to CDM},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103459},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103459},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300873},
author = {Hyerim Ji and Seok Kim and Soyoung Yi and Hee Hwang and Jeong-Whun Kim and Sooyoung Yoo},
keywords = {Clinical document architecture, Common data model, Observational Medical Outcomes Partnership, Referral documents},
abstract = {Background
Utilization of standard health information exchange (HIE) data is growing due to the high adoption rate and interoperability of electronic health record (EHR) systems. However, integration of HIE data into an EHR system is not yet fully adopted in clinical research. In addition, data quality should be verified for the secondary use of these data. Thus, the aims of this study were to convert referral documents in a Health Level 7 (HL7) clinical document architecture (CDA) to the common data model (CDM) to facilitate HIE data availability for longitudinal data analysis, and to identify data quality levels for application in future clinical studies.
Methods
A total of 21,492 referral CDA documents accumulated for over 10 years in a tertiary general hospital in South Korea were analyzed. To convert CDA documents to the Observational Medical Outcomes Partnership (OMOP) CDM, processes such as CDA parsing, data cleaning, standard vocabulary mapping, CDA-to-CDM mapping, and CDM conversion were performed. The quality of CDM data was then evaluated using the Achilles Heel and visualized with the Achilles tool.
Results
Mapping five CDA elements (document header, problem, medication, laboratory, and procedure) into an OMOP CDM table resulted in population of 9 CDM tables (person, visit_occurrence, condition_occurrence, drug_exposure, measurement, observation, procedure_occurrence, care_site, and provider). Three CDM tables (drug_era, condition_era, and observation_period) were derived from the converted table. From vocabulary mapping codes in CDA documents according to domain, 98.6% of conditions, 68.8% of drugs, 35.7% of measurements, 100% of observation, and 56.4% of procedures were mapped as standard concepts. The conversion rates of the CDA to the OMOP CDM were 96.3% for conditions, 97.2% for drug exposure, 98.1% for procedure occurrence, 55.1% for measurements, and 100% for observation.
Conclusions
We examined the possibility of CDM conversion by defining mapping rules for CDA-to-CDM conversion using the referral CDA documents collected from clinics in actual medical practice. Although mapping standard vocabulary for CDM conversion requires further improvement, the conversion could facilitate further research on the usage patterns of medical resources and referral patterns.}
}
@article{PLAZAS2022101971,
title = {Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {101971},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101971},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000926},
author = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
keywords = {Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things},
abstract = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.}
}
@incollection{NETTLETON2014105,
title = {Chapter 7 - Data Sampling and Partitioning},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-117},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000078},
author = {David Nettleton},
keywords = {sampling, data reduction, partitioning, business criteria, train, test, Big Data},
abstract = {This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.}
}
@article{ALSHAER2019792,
title = {IBRIDIA: A hybrid solution for processing big logistics data},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {792-804},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830606X},
author = {Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk},
keywords = {Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm},
abstract = {Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.}
}
@article{FRYE2021142,
title = {Production rescheduling through product quality prediction},
journal = {Procedia Manufacturing},
volume = {54},
pages = {142-147},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001578},
author = {Maik Frye and Dávid Gyulai and Júlia Bergmann and Robert H. Schmitt},
keywords = {Machine Learning, Production Scheduling, Product Quality Prediction, Data Quality},
abstract = {In production management, efficient scheduling is key towards smooth and balanced production. Scheduling can be well-supported by real-time data acquisition systems, resulting in decisions that rely on actual or predicted status of production environment and jobs in progress. Utilizing advanced monitoring systems, prediction-based rescheduling method is proposed that can react on in-process scrap predictions, performed by machine learning algorithms. Based on predictions, overall production can be rescheduled with higher efficiency, compared to rescheduling after completion of the whole machining process with realization of scrap. Series of numerical experiments are presented to demonstrate potentials in prediction-based rescheduling, with early-stage scrap detection.}
}
@article{PEER20212162,
title = {Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records},
journal = {Journal of Allergy and Clinical Immunology},
volume = {147},
number = {6},
pages = {2162-2170},
year = {2021},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2020.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0091674920324052},
author = {Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Renée Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian},
keywords = {Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics},
abstract = {Background
Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.
Objective
Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.
Methods
We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.
Results
The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.
Conclusion
We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.}
}
@article{MIETH2019868,
title = {Framework for the usage of data from real-time indoor localization systems to derive inputs for manufacturing simulation},
journal = {Procedia CIRP},
volume = {81},
pages = {868-873},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.216},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305219},
author = {Carina Mieth and Anne Meyer and Michael Henke},
keywords = {real-time indoor localization system, input data management, cyber-physical system, manufacturing simulation, digital twin},
abstract = {Discrete event simulation is becoming increasingly important in the planning and operation of complex manufacturing systems. A major problem with today’s approach to manufacturing simulation studies is the collection and processing of data from heterogeneous sources, because the data is often of poor quality and does not contain all the necessary information for a simulation. This work introduces a framework that uses a real-time indoor localization systems (RTILS) as a central main data harmonizer, that is designed to feed production data into a manufacturing simulation from a single source of truth. It is shown, based on different data quality dimensions, how this contributes to a better overall data quality in manufacturing simulation. Furthermore, a detailed overview on which simulation inputs can be derived from the RTILS data is given.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{GLENNON2021100516,
title = {Challenges in modeling the emergence of novel pathogens},
journal = {Epidemics},
volume = {37},
pages = {100516},
year = {2021},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2021.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1755436521000621},
author = {Emma E. Glennon and Marjolein Bruijning and Justin Lessler and Ian F. Miller and Benjamin L. Rice and Robin N. Thompson and Konstans Wells and C. Jessica E. Metcalf},
keywords = {Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning},
abstract = {The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.}
}
@article{YUE2018316,
title = {Using Taiwan National Health Insurance Database to model cancer incidence and mortality rates},
journal = {Insurance: Mathematics and Economics},
volume = {78},
pages = {316-324},
year = {2018},
note = {Longevity risk and capital markets: The 2015–16 update},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2017.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167668717304304},
author = {Jack C. Yue and Hsin-Chung Wang and Yin-Yee Leong and Wei-Ping Su},
keywords = {Cancer insurance, Longevity risk, Big data, Stochastic models, National Health Insurance},
abstract = {The increasing cancer incidence and decreasing mortality rates in Taiwan worsened the loss ratio of cancer insurance products and created a financial crisis for insurers. In general, the loss ratio of long-term health products seems to increase with the policy year. In the present study, we used the data from Taiwan National Health Insurance Research Database to evaluate the challenge of designing cancer products. We found that the Lee–Carter and APC models have the smallest estimation errors, and the CBD and Gompertz models are good alternatives to explore the trend of cancer incidence and mortality rates, especially for the elderly people. The loss ratio of Taiwan’s cancer products is to grow and this can be deemed as a form of longevity risk. The longevity risk of health products is necessary to face in the future, similar to the annuity products.}
}
@article{CANHOTO2021441,
title = {Leveraging machine learning in the global fight against money laundering and terrorism financing: An affordances perspective},
journal = {Journal of Business Research},
volume = {131},
pages = {441-452},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306640},
author = {Ana Isabel Canhoto},
keywords = {Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals},
abstract = {Financial services organisations facilitate the movement of money worldwide, and keep records of their clients’ identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.}
}
@article{WANG202183,
title = {The groundwater potential assessment system based on cloud computing: A case study in islands region},
journal = {Computer Communications},
volume = {178},
pages = {83-97},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002528},
author = {Daqing Wang and Haoli Xu and Yue Shi and Zhibin Ding and Zhengdong Deng and Zhixin Liu and Xingang Xu and Zhao Lu and Guangyuan Wang and Zijian Cheng and Xiaoning Zhao},
keywords = {Big data, Cloud computing, Remote sensing, Groundwater potential, Bedrock islands},
abstract = {Today’s intelligent system based on cloud computing platform can realize “unattended”, real-time monitoring observation and forecast by remote sensing. In order to import the development and efficiency of groundwater potential assessment(GPA) by remote sensing, the cloud computing platform was tried to use in the computing GPA. In this study, the Pearl River Estuary islands region(China) was selected as the study area. The slope, aspect, water-density(WD), land surface temperature(LST), NDVI and NDWI were used as the GPA indexes, which have been used before. Considering the similar geological and geomorphological conditions of the islands area, the analytic hierarchy process (AHP) method and these indexes can be used to assess GPA in the remote sensing cloud computing platform efficiently and conveniently. The results of the assessment were in good agreement with the actual hydrogeological map. Besides, the other intelligent algorithms can also be applied in this platform. Finally, this study realized the rapid “unattended” and “real-time monitoring” groundwater potential assessment, and carried out a multi-level GPA. It will be of certain reference significance to the exploitation of groundwater in the island area, which has realized convenient and efficient processing and analysis of data anytime and anywhere. At the same time, attention must be paid to the security of data and the maintenance of the system.}
}
@article{PRIYADARSHINI2015371,
title = {Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents},
journal = {Procedia Computer Science},
volume = {54},
pages = {371-379},
year = {2015},
note = {Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915013678},
author = {R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish},
keywords = {Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.},
abstract = {The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.}
}
@article{TIAN2020116335,
title = {Impact of water source mixture and population changes on the Al residue in megalopolitan drinking water},
journal = {Water Research},
volume = {186},
pages = {116335},
year = {2020},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2020.116335},
url = {https://www.sciencedirect.com/science/article/pii/S004313542030871X},
author = {Chenhao Tian and Chenghong Feng and Lei Chen and Qixuan Wang},
keywords = {Al residue, Mixed water sources, Big data analysis, Megalopolitan, Drinking water},
abstract = {This study establishes a new understanding of the contributions of Al residue in a megalopolitan drinking water supply system with mixed water sources. The different influences and contributions of foreign water source, resident migration and season changing to Al residue in drinking water were investigated. Especially, the role of Southern water transferred over 1200 km via the South-to-North Water Diversion Project in the Al residue of drinking water supply system of a northern megalopolitan were revealed for the first time. Comparisons of big data on Al residue in the water supply system with sole and mixed water sources showed that the introduction of Southern water enhanced the Al residue in drinking water by over 35%. The world's largest annual residents’ migration during Chinese Lunar New Year and the changes of season affect the water pipework hydrodynamics, which were embodied as the periodic changes of particulate aluminium and the relations with resident's temporal-spatial distribution in the megalopolitan. Because of the differences in water quality, Southern water promotes the release of historically deposited Al and facilitates the cleaning of old pipes.}
}
@article{JIANG2020161,
title = {Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach},
journal = {Information Sciences},
volume = {512},
pages = {161-174},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309089},
author = {Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu},
keywords = {Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior},
abstract = {In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.}
}
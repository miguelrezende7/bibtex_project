@inproceedings{10.1145/2905055.2905184,
author = {Jangra, Ajay and Singh, Niharika and Lakhina, Upasana},
title = {VIP: Verification and Identification Protective Data Handling Layer Implementation to Achieve MVCC in Cloud Computing},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905184},
doi = {10.1145/2905055.2905184},
abstract = {Over transactional database systems MultiVersion concurrency control is maintained for secure, fast and efficient access to the shared data file implementation scenario. An effective coordination is supposed to be set up between owners and users also the developers &amp; system operators, to maintain inter-cloud &amp; intra-cloud communication Most of the services &amp; application offered in cloud world are real-time, which entails optimized compatibility service environment between master and slave clusters. In the paper, offered methodology supports replication and triggering methods intended for data consistency and dynamicity. Where intercommunication between different clusters is processed through middleware besides slave intra-communication is handled by verification &amp; identification protection. The proposed approach incorporates resistive flow to handle high impact systems that identifies and verifies multiple processes. Results show that the new scheme reduces the overheads from different master and slave servers as they are co-located in clusters which allow increased horizontal and vertical scalability of resources.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {124},
numpages = {6},
keywords = {Serializability, MVCC, Verification &amp; Identification, Data version validation, Transaction Manager, Cloud computing},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3139958.3140013,
author = {Lin, Yijun and Chiang, Yao-Yi and Pan, Fan and Stripelis, Dimitrios and Ambite, Jose Luis and Eckel, Sandrah P. and Habre, Rima},
title = {Mining Public Datasets for Modeling Intra-City PM2.5 Concentrations at a Fine Spatial Resolution},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140013},
doi = {10.1145/3139958.3140013},
abstract = {Air quality models are important for studying the impact of air pollutant on health conditions at a fine spatiotemporal scale. Existing work typically relies on area-specific, expert-selected attributes of pollution emissions (e,g., transportation) and dispersion (e.g., meteorology) for building the model for each combination of study areas, pollutant types, and spatiotemporal scales. In this paper, we present a data mining approach that utilizes publicly available OpenStreetMap (OSM) data to automatically generate an air quality model for the concentrations of fine particulate matter less than 2.5 μm in aerodynamic diameter at various temporal scales. Our experiment shows that our (domain-) expert-free model could generate accurate PM2.5 concentration predictions, which can be used to improve air quality models that traditionally rely on expert-selected input. Our approach also quantifies the impact on air quality from a variety of geographic features (i.e., how various types of geographic features such as parking lots and commercial buildings affect air quality and from what distance) representing mobile, stationary and area natural and anthropogenic air pollution sources. This approach is particularly important for enabling the construction of context-specific spatiotemporal models of air pollution, allowing investigations of the impact of air pollution exposures on sensitive populations such as children with asthma at scale.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {25},
numpages = {10},
keywords = {Air Quality Modeling, Geospatial Data Mining, PM2.5 Concentration Prediction},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3428502.3428514,
author = {Papadopoulos, Theodoros and Charalabidis, Yannis},
title = {What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428514},
doi = {10.1145/3428502.3428514},
abstract = {The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {100–111},
numpages = {12},
keywords = {AI strategies, NLP, Automated Text Analysis, document similarity, topic modelling, machine learning},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/2600821.2600847,
author = {Moazeni, Ramin and Link, Daniel and Boehm, Barry},
title = {COCOMO II Parameters and IDPD: Bilateral Relevances},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600847},
doi = {10.1145/2600821.2600847},
abstract = { The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed. },
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {20–24},
numpages = {5},
keywords = {scale factors, cost drivers, Parametric cost estimation, IDPD, incremental development},
location = {Nanjing, China},
series = {ICSSP 2014}
}

@inproceedings{10.1145/2631775.2631806,
author = {Zhang, Kunpeng and Bhattacharyya, Siddhartha and Ram, Sudha},
title = {Empirical Analysis of Implicit Brand Networks on Social Media},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631806},
doi = {10.1145/2631775.2631806},
abstract = {This paper investigates characteristics of implicit brand networks extracted from a large dataset of user historical activities on a social media platform. To our knowledge, this is one of the first studies to comprehensively examine brands by incorporating user-generated social content and information about user interactions. This paper makes several important contributions. We build and normalize a weighted, undirected network representing interactions among users and brands. We then explore the structure of this network using modified network measures to understand its characteristics and implications. As a part of this exploration, we address three important research questions: (1) What is the structure of a brand-brand network? (2) Does an influential brand have a large number of fans? (3) Does an influential brand receive more positive or more negative comments from social users? Experiments conducted with Facebook data show that the influence of a brand has (a) high positive correlation with the size of a brand, meaning that an influential brand can attract more fans, and, (b) low negative correlation with the sentiment of comments made by users on that brand, which means that negative comments have a more powerful ability to generate awareness of a brand than positive comments. To process the large-scale datasets and networks, we implement MapReduce-based algorithms.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {190–199},
numpages = {10},
keywords = {network analysis, sentiment identification, mapreduce, marketing intelligence, social media},
location = {Santiago, Chile},
series = {HT '14}
}

@article{10.1145/3494582,
author = {Shen, Cong and Qian, Zhaozhi and Huyuk, Alihan and Van Der Schaar, Mihaela},
title = {MARS: Assisting Human with Information Processing Tasks Using Machine Learning},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2691-1957},
url = {https://doi.org/10.1145/3494582},
doi = {10.1145/3494582},
abstract = {This article studies the problem of automated information processing from large volumes of unstructured, heterogeneous, and sometimes untrustworthy data sources. The main contribution is a novel framework called Machine Assisted Record Selection (MARS). Instead of today’s standard practice of relying on human experts to manually decide the order of records for processing, MARS learns the optimal record selection via an online learning algorithm. It further integrates algorithm-based record selection and processing with human-based error resolution to achieve a balanced task allocation between machine and human. Both fixed and adaptive MARS algorithms are proposed, leveraging different statistical knowledge about the existence, quality, and cost associated with the records. Experiments using semi-synthetic data that are generated from real-world patients record processing in the UK national cancer registry are carried out, which demonstrate significant (3 to 4 fold) performance gain over the fixed-order processing. MARS represents one of the few examples demonstrating that machine learning can assist humans with complex jobs by automating complex triaging tasks.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {21},
numpages = {19},
keywords = {human-in-the-loop decision support system, Data entry, online learning}
}

@article{10.1145/2602204.2602217,
author = {Kenneally, Erin and Bailey, Michael},
title = {Cyber-Security Research Ethics Dialogue &amp; Strategy Workshop},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2602204.2602217},
doi = {10.1145/2602204.2602217},
abstract = {The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of "ethics-by-design" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {76–79},
numpages = {4},
keywords = {law, network measurement, ethics, trust, cyber security}
}

@inbook{10.1145/3313831.3376662,
author = {Shipman, Frank M. and Marshall, Catherine C.},
title = {Ownership, Privacy, and Control in the Wake of Cambridge Analytica: The Relationship between Attitudes and Awareness},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376662},
abstract = {Has widespread news of abuse changed the public's perceptions of how user-contributed content from social networking sites like Facebook and LinkedIn can be used? We collected two datasets that reflect participants' attitudes about content ownership, privacy, and control, one in April 2018, while Cambridge Analytica was still in the news, and another in February 2019, after the event had faded from the headlines, and aggregated the data according to participants' awareness of the story, contrasting the attitudes of those who reported the greatest awareness with those who reported the least. Participants with the greatest awareness of the news story's details have more polarized attitudes about reuse, especially the reuse of content as data. They express a heightened desire for data mobility, greater concern about networked privacy rights, increased skepticism of algorithmically targeted advertising and news, and more willingness for social media platforms to demand corrections of inaccurate or deceptive content.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}

@article{10.1145/3523059,
author = {Zhang, Lin and Fan, Lixin and Luo, Yong and Duan, Ling-Yu},
title = {Intrinsic Performance Influence Based Participant Contribution Estimation for Horizontal Federated Learning},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3523059},
doi = {10.1145/3523059},
abstract = {The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust and efficient manner. To achieve this goal, we propose a novel contribution estimation method - Intrinsic Performance Influence based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data, and thus prevent them from participating and deteriorating the learning ecosystem.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
keywords = {participant contribution estimation, neural network, federated learning}
}

@inproceedings{10.1145/3307772.3328285,
author = {Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno},
title = {Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328285},
doi = {10.1145/3307772.3328285},
abstract = {Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {58–67},
numpages = {10},
keywords = {Energy dataset, non-intrusive load monitoring, waveform compression, high sampling rate, file format, electricity aggregate},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@inproceedings{10.1145/3428502.3428548,
author = {Osorio-Sanabria, Mariutsi Alexandra and Amaya-Fern\'{a}ndez, Ferney and Gonz\'{a}lez-Zabala, Mayda Patricia},
title = {Developing a Model to Readiness Assessment of Open Government Data in Public Institutions in Colombia},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428548},
doi = {10.1145/3428502.3428548},
abstract = {Open data is a movement that has gained worldwide political relevance as a strategy that supports active transparency, access to public information, and the generation of public, social, and economic value. To know the progress and results of open data initiatives, governments, working groups, international organizations, and researchers have proposed indexes and evaluation models. These measurements focus on the evaluation of aspects of the preparation, implementation, and impact of open data initiatives. In Colombia, the national government within the framework of its digital government policy defined the open data project. The progress in data openings is monitored through international indexes and the open government index, which focuses solely on the publication and use of open government data. This research deals with the evaluation of the preparation for the opening of data, in public entities that have not implemented an open data initiative. The study gives a general description of the evaluation of open data at the international and national level, identifies aspects to be considered to measure the preparation, and proposes a conceptual model of evaluation to measure the preparation in open data of a public sector entity. This proposal can be considered as a tool that generates information that supports the design and implementation of an effective open data initiative.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {334–340},
numpages = {7},
keywords = {e-government, Open data, open government data, digital government, readiness assessment},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@article{10.1145/3422158,
author = {Kumar, Devender and Jeuris, Steven and Bardram, Jakob E. and Dragoni, Nicola},
title = {Mobile and Wearable Sensing Frameworks for MHealth Studies and Applications: A Systematic Review},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3422158},
doi = {10.1145/3422158},
abstract = {With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.},
journal = {ACM Trans. Comput. Healthcare},
month = {dec},
articleno = {8},
numpages = {28},
keywords = {mobile sensing, mobile sensing frameworks, mHealth frameworks, mHealth sensing, wearable sensing}
}

@article{10.1145/3093895,
author = {Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.},
title = {Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093895},
doi = {10.1145/3093895},
abstract = {A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {5},
numpages = {21},
keywords = {Mobile crowed sensing, smart cities, social sensing}
}

@article{10.1145/3426866,
author = {Meng, Linhao and Wei, Yating and Pan, Rusheng and Zhou, Shuyue and Zhang, Jianwei and Chen, Wei},
title = {VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3426866},
doi = {10.1145/3426866},
abstract = {Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {aug},
articleno = {26},
numpages = {23},
keywords = {visual analytics, anomaly detection, Federated learning}
}

@article{10.1145/3450518,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3450518},
doi = {10.1145/3450518},
abstract = {We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.},
journal = {ACM Trans. Database Syst.},
month = {may},
articleno = {7},
numpages = {46},
keywords = {updates, decomposition, Boyce-Codd normal form, synthesis, missing value, functional dependency, normal form, third normal form, key, database design, redundancy}
}

@inproceedings{10.1145/3397166.3413465,
author = {Bazzi, Alessandro and Campolo, Claudia and Masini, Barbara M. and Molinaro, Antonella},
title = {How to Deal with Data Hungry V2X Applications?},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3413465},
doi = {10.1145/3397166.3413465},
abstract = {Current vehicular communication technologies were designed for a so-called phase 1, where cars needed to advise of their presence. Several projects, research activities and field tests have proved their effectiveness to this scope. But entering the phase 2, where awareness needs to be improved with non-connected objects and vulnerable road users, and even more with phases 3 and 4, where also coordination is foreseen, the spectrum scarcity becomes a critical issue. In this work, we provide an overview of various 5G and beyond solutions currently under investigation that will be needed to tackle the challenge. We first recall the undergoing activities at the access layer aimed to satisfy capacity and bandwidth demands. We then discuss the role that emerging networking paradigms can play to improve vehicular data dissemination, while preventing congestion and better exploiting resources. Finally, we give a look into edge computing and machine learning techniques that will be determinant to efficiently process and mine the massive amounts of sensor data.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {333–338},
numpages = {6},
keywords = {vehicle-to-everything, 5G, connected and automated vehicles, cooperative sensing},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/3374587.3374631,
author = {Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu},
title = {A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous Power IoT},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374631},
doi = {10.1145/3374587.3374631},
abstract = {As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named "SSD" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {252–257},
numpages = {6},
keywords = {Ubiquitous Power IoT, Data Splitting, Information Security},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3183713.3197387,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3197387},
doi = {10.1145/3183713.3197387},
abstract = {There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1645–1650},
numpages = {6},
keywords = {data enrichment, machine learning, data integration},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3083187.3083189,
author = {Pogorelov, Konstantin and Eskeland, Sigrun Losada and de Lange, Thomas and Griwodz, Carsten and Randel, Kristin Ranheim and Stensland, H\r{a}kon Kvale and Dang-Nguyen, Duc-Tien and Spampinato, Concetto and Johansen, Dag and Riegler, Michael and Halvorsen, P\r{a}l},
title = {A Holistic Multimedia System for Gastrointestinal Tract Disease Detection},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3083189},
doi = {10.1145/3083187.3083189},
abstract = {Analysis of medical videos for detection of abnormalities and diseases requires both high precision and recall, but also real-time processing for live feedback and scalability for massive screening of entire populations. Existing work on this field does not provide the necessary combination of retrieval accuracy and performance.; AB@In this paper, a multimedia system is presented where the aim is to tackle automatic analysis of videos from the human gastrointestinal (GI) tract. The system includes the whole pipeline from data collection, processing and analysis, to visualization. The system combines filters using machine learning, image recognition and extraction of global and local image features. Furthermore, it is built in a modular way so that it can easily be extended. At the same time, it is developed for efficient processing in order to provide real-time feedback to the doctors. Our experimental evaluation proves that our system has detection and localisation accuracy at least as good as existing systems for polyp detection, it is capable of detecting a wider range of diseases, it can analyze video in real-time, and it has a low resource consumption for scalability.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {112–123},
numpages = {12},
keywords = {Gastrointestinal Tract, Evaluation, Performance, Medicine, Medical Multimedia System, Interactive},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@article{10.1145/2738210.2738211,
author = {Kenneally, Erin},
title = {How to Throw the Race to the Bottom: Revisiting Signals for Ethical and Legal Research Using Online Data},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0095-2737},
url = {https://doi.org/10.1145/2738210.2738211},
doi = {10.1145/2738210.2738211},
abstract = {With research using data available online, researcher conduct is not fully prescribed or proscribed by formal ethical codes of conduct or law because of ill-fitting "expectations signals" -- indicators of legal and ethical risk. This article describes where these ordering forces breakdown in the context of online research and suggests how to identify and respond to these grey areas by applying common legal and ethical tenets that run across evolving models. It is intended to advance the collective dialogue work-in-progress toward a path that revisits and harmonizes more appropriate ethical and legal signals for research using online data between and among researchers, oversight entities, policymakers and society.},
journal = {SIGCAS Comput. Soc.},
month = {feb},
pages = {4–10},
numpages = {7},
keywords = {law, security research ethics}
}

@article{10.1145/3409473,
author = {Maiolo, Sof\'{\i}a and Etcheverry, Lorena and Marotta, Adriana},
title = {Data Profiling in Property Graph Databases},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3409473},
doi = {10.1145/3409473},
abstract = {Property Graph databases are being increasingly used within the industry as a powerful and flexible way to model real-world scenarios. With this flexibility, a great challenge appears regarding profiling tasks due to the need of adapting them to these new models while taking advantage of the Property Graphs’ particularities. This article proposes a set of data profiling tasks by integrating existing methods and techniques and an taxonomy to classify them. In addition, an application pipeline is provided while a formal specification of some tasks is defined.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {20},
numpages = {27},
keywords = {data profiling, Property Graph}
}

@inproceedings{10.1145/3459955.3460617,
author = {Gosh, Saptarshi and EL Boudani, Brahim and Dagiuklas, Tasos and Iqbal, Muddesar},
title = {SO-KDN: A Self-Organised Knowledge Defined Networks Architecture for Reliable Routing},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460617},
doi = {10.1145/3459955.3460617},
abstract = {“When you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared”. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.},
booktitle = {2021 The 4th International Conference on Information Science and Systems},
pages = {160–166},
numpages = {7},
keywords = {SDN, Routing, Deep Learning, SON},
location = {Edinburgh, United Kingdom},
series = {ICISS 2021}
}

@inproceedings{10.1145/2961111.2962605,
author = {Sun, Yan and Wang, Qing and Li, Mingshu},
title = {Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962605},
doi = {10.1145/2961111.2962605},
abstract = {Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {39},
numpages = {10},
keywords = {Software Maintenance, Non-Source Documents, Missing Link Recovery, Mining Software Repositories},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3098954.3105822,
author = {Stupka, V\'{a}clav and Hor\'{a}k, Martin and Hus\'{a}k, Martin},
title = {Protection of Personal Data in Security Alert Sharing Platforms},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3105822},
doi = {10.1145/3098954.3105822},
abstract = {In order to ensure confidentiality, integrity and availability (so called CIA triad) of data within network infrastructure, it is necessary to be able to detect and handle cyber security incidents. For this purpose, it is vital for Computer Security Incident Response Teams (CSIRT) to have enough data on relevant security events and threats. That is why CSIRTs share security alerts and incidents data using various sharing platforms. Even though they do so primarily to protect data and privacy of users, their use also lead to additional processing of personal data, which may cause new privacy risks. European data protection law, especially with the adoption of the new General data protection regulation, sets out very strict rules on processing of personal data which on one hand leads to greater protection of individual's rights, but on the other creates great obstacles for those who need to share any personal data. This paper analyses the General Data Protection Regulation (GDPR), relevant case-law and analyses by the Article 29 Working Party to propose optimal methods and level of personal data processing necessary for effective use of security alert sharing platforms, which would be legally compliant and lead to appropriate balance between risks.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {8},
keywords = {Information sharing, Cyber security, Alert sharing platform, Privacy, Personal data, Intrusion detection},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/2750858.2807526,
author = {Hovsepian, Karen and al'Absi, Mustafa and Ertin, Emre and Kamarck, Thomas and Nakajima, Motohiro and Kumar, Santosh},
title = {CStress: Towards a Gold Standard for Continuous Stress Assessment in the Mobile Environment},
year = {2015},
isbn = {9781450335744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2750858.2807526},
doi = {10.1145/2750858.2807526},
abstract = {Recent advances in mobile health have produced several new models for inferring stress from wearable sensors. But, the lack of a gold standard is a major hurdle in making clinical use of continuous stress measurements derived from wearable sensors. In this paper, we present a stress model (called cStress) that has been carefully developed with attention to every step of computational modeling including data collection, screening, cleaning, filtering, feature computation, normalization, and model training. More importantly, cStress was trained using data collected from a rigorous lab study with 21 participants and validated on two independently collected data sets --- in a lab study on 26 participants and in a week-long field study with 20 participants. In testing, the model obtains a recall of 89% and a false positive rate of 5% on lab data. On field data, the model is able to predict each instantaneous self-report with an accuracy of 72%.},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {493–504},
numpages = {12},
keywords = {modeling, mobile health (mHealth), wearable sensors, stress},
location = {Osaka, Japan},
series = {UbiComp '15}
}

@inproceedings{10.1145/3274895.3274899,
author = {Oliver, Dev and Hoel, Erik G.},
title = {A Trace Framework for Analyzing Utility Networks: A Summary of Results (Industrial Paper)},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274899},
doi = {10.1145/3274895.3274899},
abstract = {Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {249–258},
numpages = {10},
keywords = {utility networks, graphs and networks, GIS, spatial databases, graph algorithms},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/3498851.3498929,
author = {Dautaras, Justas and Matskin, Mihhail},
title = {Mobile Crowdsensing with Imagery Tasks},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498929},
doi = {10.1145/3498851.3498929},
abstract = { The amount of gadgets connected to the internet has grown rapidly in the recent years. These human owned devices can potentially be used to gather sensor data without active involvement of their owners. One of the types of platforms that contribute to the utilisation of these devices are mobile crowdsensing systems. These systems can be used for different tasks including different types of community support. While these systems are quite widely used, yet little research has been done for integration of imagery data into them which require also human involvement. This paper considers a mobile crowdsensing system where gathering data from sensors is supported by crowdsourcing human intelligence for providing both textual and visual information. We also explore the best settings for such a system. Imagery processing is integrated into an already existing mobile crowdsensing platform CrowdS. The solution was evaluated both by a limited number of real life users and by conducting simulations. The simulations represent complex scenarios with multi-level variables. The results of simulation allow suggest an efficient configuration for the parameters and characteristics of the environment used in imagery integration.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {54–61},
numpages = {8},
keywords = {mobile crowdsensing, crowdsourcing, mobile sensing devices},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/3524104,
author = {Qu, Youyang and Uddin, Md Palash and Gan, Chenquan and Xiang, Yong and Gao, Longxiang and Yearwood, John},
title = {Blockchain-Enabled Federated Learning: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524104},
doi = {10.1145/3524104},
abstract = {Federated learning (FL) is experiencing fast booming in recent years, which is jointly promoted by the prosperity of machine learning and Artificial Intelligence along with the emerging privacy issues. In the FL paradigm, a central server and local end devices maintain the same model by exchanging model updates instead of raw data, with which the privacy of data stored on end devices is not directly revealed. In this way, the privacy violation caused by the growing collection of sensitive data can be mitigated. However, the performance of FL with a central server is reaching a bottleneck while new threats are emerging simultaneously. There are various reasons, among which the most significant ones are centralized processing, data falsification, and lack of incentives. To accelerate the proliferation of FL, blockchain-enabled FL has attracted substantial attention from both academia and industry. A considerable number of novel solutions are devised to meet the emerging demands of diverse scenarios. Blockchain-enabled FL provides both theories and techniques to improve the performances of FL from various perspectives. In this survey, we will comprehensively summarize and evaluate existing variants of blockchain-enabled FL, identify the emerging challenges, and propose potentially promising research directions in this under-explored domain.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {feb},
keywords = {Attacks, Federated Learning, Countermeasures., Blockchain}
}

@inproceedings{10.1145/3490099.3511115,
author = {Dodge, Jonathan and Anderson, Andrew A. and Olson, Matthew and Dikkala, Rupika and Burnett, Margaret},
title = {How Do People Rank Multiple Mutant Agents?},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511115},
doi = {10.1145/3490099.3511115},
abstract = { Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: “the Ranking Task”; and 3) a new strategy for inducing controllable agent variations—Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user’s perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent “test selection” strategies.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {191–211},
numpages = {21},
keywords = {Explainable AI, After-Action Review},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inbook{10.1145/3310205.3310210,
title = {Data Transformation},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310210},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/2935634.2935641,
author = {Bajpai, Vaibhav and Berger, Arthur W. and Eardley, Philip and Ott, J\"{o}rg and Sch\"{o}nw\"{a}lder, J\"{u}rgen},
title = {Global Measurements: Practice and Experience (Report on Dagstuhl Seminar #16012)},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935641},
doi = {10.1145/2935634.2935641},
abstract = {This article summarises a 2.5 day long Dagstuhl seminar on Global Measurements: Practice and Experience held in January 2016. This seminar was a followup of the seminar on Global Measurement Frameworks held in 2013, which focused on the development of global Internet measurement platforms and associated metrics. The second seminar aimed at discussing the practical experience gained with building these global Internet measurement platforms. It brought together people who are actively involved in the design and maintenance of global Internet measurement platforms and who do research on the data delivered by such platforms. Researchers in this seminar have used data derived from global Internet measurement platforms in order to manage networks or services or as input for regulatory decisions. The entire set of presentations delivered during the seminar is made publicly available at [1].},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {32–39},
numpages = {8},
keywords = {internet measurements, quality of experience, traffic engineering, network management}
}

@article{10.1109/TCBB.2019.2953908,
author = {Wang, Bing and Mei, Changqing and Wang, Yuanyuan and Zhou, Yuming and Cheng, Mu-Tian and Zheng, Chun-Hou and Wang, Lei and Zhang, Jun and Chen, Peng and Xiong, Yan},
title = {Imbalance Data Processing Strategy for Protein Interaction Sites Prediction},
year = {2021},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2953908},
doi = {10.1109/TCBB.2019.2953908},
abstract = {Protein-protein interactions play essential roles in various biological progresses. Identifying protein interaction sites can facilitate researchers to understand life activities and therefore will be helpful for drug design. However, the number of experimental determined protein interaction sites is far less than that of protein sites in protein-protein interaction or protein complexes. Therefore, the negative and positive samples are usually imbalanced, which is common but bring result bias on the prediction of protein interaction sites by computational approaches. In this work, we presented three imbalance data processing strategies to reconstruct the original dataset, and then extracted protein features from the evolutionary conservation of amino acids to build a predictor for identification of protein interaction sites. On a dataset with 10,430 surface residues but only 2,299 interface residues, the imbalance dataset processing strategies can obviously reduce the prediction bias, and therefore improve the prediction performance of protein interaction sites. The experimental results show that our prediction models can achieve a better prediction performance, such as a prediction accuracy of 0.758, or a high F-measure of 0.737, which demonstrated the effectiveness of our method.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {may},
pages = {985–994},
numpages = {10}
}

@inproceedings{10.1145/2390045.2390062,
author = {B\"{a}r, Arian and Golab, Lukasz},
title = {Towards Benchmarking Stream Data Warehouses},
year = {2012},
isbn = {9781450317214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390045.2390062},
doi = {10.1145/2390045.2390062},
abstract = {Data management systems are facing two challenges driven by the requirements of emerging data-intensive applications: more data and less time to process the data. Data volumes continue to increase as new sources and data collecting mechanisms appear. At the same time, these sources tend to be highly dynamic and generate data in the form of a stream, which requires quick reaction to newly arrived data. Traditional data warehouses enable scalable data storage and analytics, including the ability to define nested levels of materialized views. However, views are typically refreshed during downtimes---e.g., every night---which does not meet the latency requirements of many applications. Stream data warehousing is a new data management technology that allows nearly-continuous view refresh as new data arrive, which enables seamless integration of real-time monitoring and business intelligence with long-term data mining. In this paper, we argue that a new benchmark is required for stream warehouses, which should focus on measuring the property that determines the utility of these systems, namely how well they can keep up with the incoming data and guarantee the "freshness" of materialized views.},
booktitle = {Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP},
pages = {105–112},
numpages = {8},
keywords = {materialized view maintenance, data warehouse benchmarking, stream data warehousing},
location = {Maui, Hawaii, USA},
series = {DOLAP '12}
}

@article{10.14778/3342263.3342626,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342626},
doi = {10.14778/3342263.3342626},
abstract = {We establish a robust schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing many redundant data value occurrences. We establish axiomatic and algorithmic foundations for reasoning about embedded functional dependencies. These foundations allow us to establish generalizations of Boyce-Codd and Third normal forms that do not permit any redundancy in any future application data, or minimize their redundancy across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate our framework, and the effectiveness and efficiency of our algorithms, but also provide quantified insight into database schema design trade-offs.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1458–1470},
numpages = {13}
}

@inbook{10.1145/3447404.3447411,
author = {McMenemy, David},
title = {Ethics and Statistics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447411},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {101–103},
numpages = {3}
}

@inbook{10.1145/3461702.3462605,
author = {Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison},
title = {Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462605},
abstract = {As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {627–637},
numpages = {11}
}

@article{10.1145/3371906,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
title = {Toward Model-Driven Sustainability Evaluation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3371906},
doi = {10.1145/3371906},
abstract = {Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.},
journal = {Commun. ACM},
month = {feb},
pages = {80–91},
numpages = {12}
}

@inproceedings{10.1145/3323679.3326513,
author = {Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu},
title = {DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326513},
doi = {10.1145/3323679.3326513},
abstract = {In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique "view" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {151–160},
numpages = {10},
keywords = {Deep Learning, Internet of Things, Sensor Fusion},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.1145/3209281.3209309,
author = {Alarabiat, Ayman and Soares, Delfina and Ferreira, Luis and de S\'{a}-Soares, Filipe},
title = {Analyzing E-Governance Assessment Initiatives: An Exploratory Study},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209309},
doi = {10.1145/3209281.3209309},
abstract = {This paper presents an exploratory study aimed at identifying, exploring, and analyzing current EGOV assessment initiatives. We do so based on data obtained from a desktop research and from a worldwide questionnaire directed to the 193 countries that are part of the list used by the Statistics Division of the United Nations Department of Economic and Social Affairs (UNDESA). The study analyses 12 EGOV assessment initiatives: a) seven of them are international/regional EGOV assessment initiatives performed by the United Nations (UN), European Union (EU), Waseda-IAC, Organisation for Economic Co-operation and Development (OECD), World Bank (WB), WWW Foundation, and Open Knowledge Network (OKN); b) five of them are country-level EGOV assessment initiatives performed by Norway, Germany, India, Saudi Arabia, and the United Arab Emirates. Further, the study provides general results obtained from a questionnaire with participation from 18 countries: Afghanistan, Angola, Brazil, Cabo Verde, Denmark, Estonia, Finland, Germany, Ghana, Ireland, Latvia, the Netherlands, Norway, Oman, Pakistan, the Philippines, Portugal, and Slovenia. The findings show that there is no shortage of interest in assessing EGOV initiatives. However, the supply side of EGOV initiatives is the dominant perspective being assessed, particularly by regional and international organizations. While there is an increasing interest in assessing the users' perspective (demand side) by individual countries, such attempts still seem to be at an early stage. Additionally, the actual use and impact of various EGOV services and activities are rarely well identified and measured. This study represents a stepping stone for developing instruments for assessing EGOV initiatives in future works. For the current stage, the study presents several general suggestions to be considered during the assessment process.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {30},
numpages = {10},
keywords = {evaluation, assessment, e-governance, e-government},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inbook{10.1145/3448016.3457552,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457552},
abstract = {Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14}
}

@inproceedings{10.1145/3388440.3412475,
author = {Chen, Junjie and Mowlaei, Mohammad Erfan and Shi, Xinghua},
title = {Population-Scale Genomic Data Augmentation Based on Conditional Generative Adversarial Networks},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412475},
doi = {10.1145/3388440.3412475},
abstract = {Although next generation sequencing technologies have made it possible to quickly generate a large collection of sequences, current genomic data still suffer from small data sizes, imbalances, and biases due to various factors including disease rareness, test affordability, and concerns about privacy and security. In order to address these limitations of genomic data, we develop a Population-scale Genomic Data Augmentation based on Conditional Generative Adversarial Networks (PG-cGAN) to enhance the amount and diversity of genomic data by transforming samples already in the data rather than collecting new samples. Both the generator and discriminator in the PG-CGAN are stacked with convolutional layers to capture the underlying population structure. Our results for augmenting genotypes in human leukocyte antigen (HLA) regions showed that PC-cGAN can generate new genotypes with similar population structure, variant frequency distributions and LD patterns. Since the input for PC-cGAN is the original genomic data without assumptions about prior knowledge, it can be extended to enrich many other types of biomedical data and beyond.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {26},
numpages = {6},
keywords = {deep learning, generative adversarial networks, genomics, data augmentation, machine learning},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3366623.3368140,
author = {Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian},
title = {Serverless Workflows for Indexing Large Scientific Data},
year = {2019},
isbn = {9781450370387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366623.3368140},
doi = {10.1145/3366623.3368140},
abstract = {The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific "data lakes" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.},
booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
pages = {43–48},
numpages = {6},
keywords = {materials science, metadata extraction, serverless, data lakes, file systems},
location = {Davis, CA, USA},
series = {WOSC '19}
}

@inproceedings{10.5555/3242181.3242205,
author = {Brailsford, Sally C and Carter, Michael W and Jacobson, Sheldon H},
title = {Five Decades of Healthcare Simulation},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {23},
numpages = {20},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.5555/3242181.3242194,
author = {Cheng, Russell},
title = {History of Input Modeling},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In stochastic simulation, input modeling refers to the process of identifying and selecting the probability distributions, called input models, from which are generated the random variates that are the source of the stochastic variation in the simulation model when it is run. This article reviews the history of the development and use of such models with the main focus on discrete-event simulation (DES).},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {12},
numpages = {21},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3025453.3025777,
author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben},
title = {Finding Similar People to Guide Life Choices: Challenge, Design, and Evaluation},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025777},
doi = {10.1145/3025453.3025777},
abstract = {People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5498–5544},
numpages = {47},
keywords = {similarity, temporal visualization, temporal event analytics, visual analytics, decision making},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@article{10.1145/3449252,
author = {Van Kleunen, Lucy and Muller, Brian and Voida, Stephen},
title = {"Wiring a City": A Sociotechnical Perspective on Deploying Urban Sensor Networks},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449252},
doi = {10.1145/3449252},
abstract = {We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {178},
numpages = {22},
keywords = {sociotechnical system, civic data, urban sensor networks, smart cities}
}

@article{10.1109/TCBB.2015.2453944,
author = {Masseroli, Marco and Canakoglu, Arif and Ceri, Stefano},
title = {Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2453944},
doi = {10.1109/TCBB.2015.2453944},
abstract = {Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At  http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {209–219},
numpages = {11}
}

@article{10.1145/3232863,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms, and Fundamental Limits},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2167-8375},
url = {https://doi.org/10.1145/3232863},
doi = {10.1145/3232863},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.},
journal = {ACM Trans. Econ. Comput.},
month = {aug},
articleno = {8},
numpages = {26},
keywords = {differential privacy, randomized response, Data collection}
}

@inproceedings{10.1145/3147234.3148104,
author = {Jansen, Christoph and Beier, Maximilian and Witt, Michael and Frey, Sonja and Krefting, Dagmar},
title = {Towards Reproducible Research in a Biomedical Collaboration Platform Following the FAIR Guiding Principles},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148104},
doi = {10.1145/3147234.3148104},
abstract = {Replication of computational experiments is essential for verifiable research. However, it requires a comprehensive and unambiguous description of all employed digital artifacts, in particular data, code and the computational environment. Recently, the FAIR Guiding Principles have been published to support reproducible research. In this paper, a cloud-based biomedical collaboration platform has been evaluated regarding FAIR principles and has been extended to support reproducibility. The FAICE suite is presented, encompassing tools to thoroughly describe and reproduce a computational experiment within the original execution environment as well as within a dynamically configured VM.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {3–8},
numpages = {6},
keywords = {repeatability, cloud computing, reproducibility, medical data, docker, xnat},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A Retrospective Analysis of SAC Requirements: Engineering Track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {aug},
pages = {26–41},
numpages = {16},
keywords = {SAC, relevance, systematic mapping study, requirements engineering, trends, retrospective, scoping study, symposium on applied computing}
}

@inproceedings{10.1145/3410566.3410606,
author = {Seong, Younho and Nuamah, Joseph and Yi, Sun},
title = {Guidelines for Cybersecurity Visualization Design},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410606},
doi = {10.1145/3410566.3410606},
abstract = {Cyber security visualization designers can benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design. We survey human factors concepts and principles that have been applied in the past decade of human factors research. We highlight these concepts and relate them to cybersecurity visualization design. We provide guidelines to help cybersecurity visualization designers address some human factors challenges in the context of interface design. We use ecological interface design approach to present human factors-based principles of interface design for visualization. Cyber security visualization designers will benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering &amp; Applications},
articleno = {25},
numpages = {6},
keywords = {affordance, cognition, cybersecurity, ecological interface design, visualization},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@inproceedings{10.1145/3333165.3333185,
author = {Soufan, Ayah},
title = {Deep Learning for Sentiment Analysis of Arabic Text},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333185},
doi = {10.1145/3333165.3333185},
abstract = {Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {20},
numpages = {8},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@article{10.1145/3323334,
author = {Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun},
title = {A Survey on Big Multimedia Data Processing and Management in Smart Cities},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323334},
doi = {10.1145/3323334},
abstract = {Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {54},
numpages = {29},
keywords = {machine learning, management, IoMT, multimedia, smart cities}
}

@article{10.1145/3185048,
author = {Zhang, Han and Hill, Shawndra and Rothschild, David},
title = {Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3185048},
doi = {10.1145/3185048},
abstract = {Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {4},
numpages = {24},
keywords = {Twitter, geolocation, social media, non-response bias, panels, coverage bias, survey, selection bias}
}

@inproceedings{10.1145/2608029.2608030,
author = {Gannon, Dennis and Fay, Dan and Green, Daron and Takeda, Kenji and Yi, Wenming},
title = {Science in the Cloud: Lessons from Three Years of Research Projects on Microsoft Azure},
year = {2014},
isbn = {9781450329118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608029.2608030},
doi = {10.1145/2608029.2608030},
abstract = {Microsoft Research is now in its fourth year of awarding Windows Azure cloud resources to the academic community. As of April 2014, over 200 research projects have started. In this paper we review the results of this effort to date. We also characterize the computational paradigms that work well in public cloud environments and those that are usually disappointing. We also discuss many of the barriers to successfully using commercial cloud platforms in research and ways these problems can be overcome.},
booktitle = {Proceedings of the 5th ACM Workshop on Scientific Cloud Computing},
pages = {1–8},
numpages = {8},
keywords = {map reduce, cloud programming models, cloud computing, platform as a service, infrastructure as a service, scalable systems},
location = {Vancouver, BC, Canada},
series = {ScienceCloud '14}
}

@inproceedings{10.1145/3152178.3152186,
author = {Shivaprabhu, Vivek R. and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F.},
title = {Ontology-Based Instance Matching for Geospatial Urban Data Integration},
year = {2017},
isbn = {9781450354950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152178.3152186},
doi = {10.1145/3152178.3152186},
abstract = {To run a smart city, data is collected from disparate sources such as IoT devices, social media, private and public organizations, and government agencies. In the US, the City of Chicago has been a pioneer in the collection of data and in the development of a framework, called OpenGrid, to curate and analyze the collected data. OpenGrid is a geospatial situational awareness platform that allows policy makers, service providers, and the general public to explore city data and to perform advanced data analytics to enable planning of services, prediction of events and patterns, and identification of incidents across the city. This paper presents the instance matching module of GIVA, a Geospatial data Integration, Visualization, and Analytics platform, as applied to the integration of information related to businesses, which is spread across several datasets. In particular, we describe the integration of two datasets, Business Licenses and Food Inspections, so as to enable predictive analytics to determine which food establishments the city should inspect first. The paper describes semantic web-based instance matching mechanisms to compare the Business Names and Address fields.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
articleno = {8},
numpages = {8},
keywords = {Data Integration, Record Linkage, Geospatial Data, Instance Matching, Ontology},
location = {Redondo Beach, CA, USA},
series = {UrbanGIS'17}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@inproceedings{10.1145/3047273.3047299,
author = {Attard, Judie and Orlandi, Fabrizio and Auer, S\"{o}ren},
title = {Exploiting the Value of Data through Data Value Networks},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047299},
doi = {10.1145/3047273.3047299},
abstract = {Open data is increasingly permeating into all dimensions of our society and has become an indispensable commodity that serves as a basis for many products and services. Governments are generating a huge amount of data spanning different dimensions. This dataification shows the paramount need to identify the means and methods in which the value of data and knowledge can be exploited. While not restricted to the government domain, this dataification is certainly relevant in a government context, particularly due to the large volume of data generated by public institutions. In this paper we identify the various activities and roles within a data value chain, and hence proceed to provide our own definition of a Data Value Network. We specifically cater for non-tangible data products and characterise three dimensions that play a vital role within the Data Value Network. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we therefore project our vision of enhancing the process of open (government) data exploitation and innovation, with the aim of achieving the highest possible impact.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {475–484},
numpages = {10},
keywords = {innovation, data demand, open data, data supply, data value chain, value creation, data value network, exploitation, impacts},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3209415.3209481,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil},
title = {Exploiting Data Analytics for Social Services: On Searching for Profiles of Unlawful Use of Social Benefits},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209481},
doi = {10.1145/3209415.3209481},
abstract = {In this paper we present a data-driven profiling approach that we have adopted and implemented for a municipality. Our aim was to make profiles transparent and meaningful for citizens, policymakers and authorities so that they can validate, scrutinize and challenge the profiles. Our approach relies on a Genetic Algorithm (GA) that searches for useful and human understandable group profiles. Furthermore, we discuss some of the challenges encountered, show a selection of the profiles that were found by the GA, and discuss the necessity and a number of ways of validating these profiles in accordance with, e.g., privacy and non-discrimination laws and guidelines before using them in practice.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {550–559},
numpages = {10},
keywords = {Genetic Algorithm, Data analytics, profiling, social benefits},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/2889160.2889231,
author = {Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel},
title = {The Bones of the System: A Case Study of Logging and Telemetry at Microsoft},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889231},
doi = {10.1145/2889160.2889231},
abstract = {Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {92–101},
numpages = {10},
keywords = {telemetry, collaboration, logging, practices, boundary object, developer tools},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/3448119,
author = {Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John},
title = {ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3448119},
doi = {10.1145/3448119},
abstract = {Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {45},
numpages = {25},
keywords = {wearable devices, micro finger writing, text input, vibration intelligence}
}

@inproceedings{10.1145/3491102.3501998,
author = {Cambo, Scott Allen and Gergle, Darren},
title = {Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501998},
doi = {10.1145/3491102.3501998},
abstract = { Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {19},
keywords = {annotator fingerprinting, model positionality, position mining, human-centered data science, human-centered machine learning, Computational reflexivity, critical data studies, data science},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3447541,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh},
title = {TabReformer: Unsupervised Representation Learning for Erroneous Data Detection},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447541},
doi = {10.1145/3447541},
abstract = {Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {18},
numpages = {29},
keywords = {bidirectional encoder, data augmentation, Error detection, transformers}
}

@article{10.1145/2794400,
author = {Guo, Bin and Wang, Zhu and Yu, Zhiwen and Wang, Yu and Yen, Neil Y. and Huang, Runhe and Zhou, Xingshe},
title = {Mobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2794400},
doi = {10.1145/2794400},
abstract = {With the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {7},
numpages = {31},
keywords = {crowd intelligence, urban/community dynamics, cross-space sensing and mining, human-machine systems, Mobile phone sensing}
}

@article{10.1145/3342515,
author = {Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin},
title = {Towards Profit Optimization During Online Participant Selection in Compressive Mobile Crowdsensing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3342515},
doi = {10.1145/3342515},
abstract = {A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {38},
numpages = {29},
keywords = {online participant selection, data reconstruction, Compressive mobile crowdsensing}
}

@article{10.14778/2850583.2850587,
author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
title = {QuERy: A Framework for Integrating Entity Resolution with Query Processing},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850587},
doi = {10.14778/2850583.2850587},
abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/3424771.3424822,
author = {Zimmermann, Olaf and L\"{u}bke, Daniel and Zdun, Uwe and Pautasso, Cesare and Stocker, Mirko},
title = {Interface Responsibility Patterns: Processing Resources and Operation Responsibilities},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424822},
doi = {10.1145/3424771.3424822},
abstract = {Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {9},
numpages = {24},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@article{10.1145/3177852,
author = {Antunes, Rodolfo S. and Seewald, Lucas A. and Rodrigues, Vinicius F. and Costa, Cristiano A. Da and Jr., Luiz Gonzaga and Righi, Rodrigo R. and Maier, Andreas and Eskofier, Bj\"{o}rn and Ollenschl\"{a}ger, Malte and Naderi, Farzad and Fahrig, Rebecca and Bauer, Sebastian and Klein, Sigrun and Campanatti, Gelson},
title = {A Survey of Sensors in Healthcare Workflow Monitoring},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3177852},
doi = {10.1145/3177852},
abstract = {Activities of a clinical staff in healthcare environments must regularly be adapted to new treatment methods, medications, and technologies. This constant evolution requires the monitoring of the workflow, or the sequence of actions from actors involved in a procedure, to ensure quality of medical services. In this context, recent advances in sensing technologies, including Real-time Location Systems and Computer Vision, enable high-precision tracking of actors and equipment. The current state-of-the-art about healthcare workflow monitoring typically focuses on a single technology and does not discuss its integration with others. Such an integration can lead to better solutions to evaluate medical workflows. This study aims to fill the gap regarding the analysis of monitoring technologies with a systematic literature review about sensors for capturing the workflow of healthcare environments. Its main scientific contribution is to identify both current technologies used to track activities in a clinical environment and gaps on their combination to achieve better results. It also proposes a taxonomy to classify work regarding sensing technologies and methods. The literature review does not present proposals that combine data obtained from Real-time Location Systems and Computer Vision sensors. Further analysis shows that a multimodal analysis is more flexible and could yield better results.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {42},
numpages = {37},
keywords = {Computer Vision, Real-time Location Systems, healthcare, workflow monitoring}
}

@inproceedings{10.1145/3170358.3170367,
author = {Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan},
title = {SHEILA Policy Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170367},
doi = {10.1145/3170358.3170367},
abstract = {This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {320–329},
numpages = {10},
keywords = {ROMA model, strategy, higher education, learning analytics, policy},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inbook{10.1145/3485190.3485193,
author = {Li, Yonghan and Lv, Hongjiang},
title = {The Dilemma of Digital Transformation of China's Hotel Industry and the Construction of Technology Platform: A Survey of Hotels Industry in China},
year = {2021},
isbn = {9781450384278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485190.3485193},
abstract = {The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.},
booktitle = {2021 4th International Conference on Information Management and Management Science},
pages = {13–18},
numpages = {6}
}

@inbook{10.1145/3387940.3391466,
author = {Suni-Lopez, Franci and Condori-Fernandez, Nelly and Catala, Alejandro},
title = {Understanding Implicit User Feedback from Multisensorial and Physiological Data: A Case Study},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391466},
abstract = {Ensuring the quality of user experience is very important for increasing the acceptance likelihood of software applications, which can be affected by several contextual factors that continuously change over time (e.g., emotional state of end-user). Due to these changes in the context, software continually needs to adapt for delivering software services that can satisfy user needs. However, to achieve this adaptation, it is important to gather and understand the user feedback. In this paper, we mainly investigate whether physiological data can be considered and used as a form of implicit user feedback. To this end, we conducted a case study involving a tourist traveling abroad, who used a wearable device for monitoring his physiological data, and a smartphone with a mobile app for reminding him to take his medication on time during four days. Through the case study, we were able to identify some factors and activities as emotional triggers, which were used for understanding the user context. Our results highlight the importance of having a context analyzer, which can help the system to determine whether the detected stress could be considered as actionable and consequently as implicit user feedback.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {563–569},
numpages = {7}
}

@inproceedings{10.1145/2961111.2962628,
author = {Baltes, Sebastian and Diehl, Stephan},
title = {Worse Than Spam: Issues In Sampling Software Developers},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962628},
doi = {10.1145/2961111.2962628},
abstract = {Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {52},
numpages = {6},
keywords = {Software Developers, Sampling, Ethics, Empirical Research},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2536780.2536783,
author = {Hafen, Ryan and Gibson, Tara D. and van Dam, Kerstin Kleese and Critchlow, Terence},
title = {Large-Scale Exploratory Analysis, Cleaning, and Modeling for Event Detection in Real-World Power Systems Data},
year = {2013},
isbn = {9781450325103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536780.2536783},
doi = {10.1145/2536780.2536783},
abstract = {In this paper, we present an approach to large-scale data analysis, Divide and Recombine (D&amp;R), and describe a hardware and software implementation that supports this approach. We then illustrate the use of D&amp;R on large-scale power systems sensor data to perform initial exploration, discover multiple data integrity issues, build and validate algorithms to filter bad data, and construct statistical event detection algorithms. This paper also reports on experiences using a non-traditional Hadoop distributed computing setup on top of a HPC computing cluster.},
booktitle = {Proceedings of the 3rd International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
articleno = {4},
numpages = {9},
keywords = {exploratory data analysis, phasor measurement unit, power systems, Hadoop, R, divide and recombine},
location = {Denver, Colorado},
series = {HiPCNA-PG '13}
}

@inproceedings{10.1145/3234698.3234743,
author = {Nagaraja, Arun and Kumar, T. Satish},
title = {An Extensive Survey on Intrusion Detection- Past, Present, Future},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234743},
doi = {10.1145/3234698.3234743},
abstract = {Intrusion Detection is the most eminent fields in the network which can also be called as anomaly detection. Various methods used by early research tells that, the kind of measures used to detect the intrusion is not specified. Research has grown extensively in Anomaly intrusion detection by using different data mining techniques. Most researchers have not briefed on the kinds of distances measures used, the classification and feature selection techniques used in identifying intrusion detection. Intrusion detection is classified with problems as Outlier problems, Sparseness problem and Data Distribution. One of the important observations made is, High Dimensional Data Reduction is not performed, and conventional dataset is not used or maintained by any researchers. A survey is performed to identify the type of distance measures used and the type of datasets used in the early research. In this extended survey, the measures like Distance measure, pattern behaviors are used in identifying the Network Intrusion Detection. In this paper, we present the various methods used by authors to obtain feature selection methods. Also, the discussion is towards, Computation of High Dimensional Data, how to decide the Choice of Learning algorithm, Efficient Distance and similarity measures to identify the intrusion detection from different datasets.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {45},
numpages = {9},
keywords = {Intrusion Detection, Measures, Datasets, Feature Selection, Classification, Clustering, Anomaly Detection},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3144789.3144797,
author = {Zhang, Zhiqiang and Huang, Xiangbing and Iqbal, Muhammad Faisal Buland and Ye, Songtao},
title = {Better Weather Forecasting through Truth Discovery Analysis},
year = {2017},
isbn = {9781450352871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144789.3144797},
doi = {10.1145/3144789.3144797},
abstract = {In many real world applications, the same object or event may be described by multiple sources. As a result, conflicts among these sources are inevitable and these conflicts cause confusion as we have more than one value or outcome for each object. One significant problem is to resolve the confusion and to identify a piece of information which is trustworthy. This process of finding the truth from conflicting values of an object provided by multiple sources is called truth discovery or fact-finding. The main purpose of the truth discovery is to find more and more trustworthy information and reliable sources. Because the major assumption of truth discovery is on this intuitive principle, the source that provides trustworthy information is considered more reliable, and moreover, if the piece of information is from a reliable source, then it is more trustworthy. However, previously proposed truth discovery methods either do not conduct source reliability estimation at all (Voting Method), or even if they do, they do not model multiple properties of the object separately. This is the motivation for researchers to develop new techniques to tackle the problem of truth discovery in data with multiple properties. We present a method using an optimization framework which minimizes the overall weighted deviation between the truths and the multi-source observations. In this framework, different types of distance functions can be plugged in to capture the characteristics of different data types. We use weather datasets collected by four different platforms for extensive experiments and the results verify both the efficiency and precision of our methods for truth discovery.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent Information Processing},
articleno = {4},
numpages = {7},
keywords = {heterogeneous data, weather, truth discovery},
location = {Bangkok, Thailand},
series = {IIP'17}
}

@inproceedings{10.1145/2660114.2660119,
author = {Sulser, Fabio and Giangreco, Ivan and Schuldt, Heiko},
title = {Crowd-Based Semantic Event Detection and Video Annotation for Sports Videos},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660119},
doi = {10.1145/2660114.2660119},
abstract = {Recent developments in sport analytics have heightened the interest in collecting data on the behavior of individuals and of the entire team in sports events. Rather than using dedicated sensors for recording the data, the detection of semantic events reflecting a team's behavior and the subsequent annotation of video data is nowadays mostly performed by paid experts. In this paper, we present an approach to generating such annotations by leveraging the wisdom of the crowd. We present the CrowdSport application that allows to collect data for soccer games. It presents crowd workers short video snippets of soccer matches and allows them to annotate these snippets with event information. Finally, the various annotations collected from the crowd are automatically disambiguated and integrated into a coherent data set. To improve the quality of the data entered, we have implemented a rating system that assigns each worker a trustworthiness score denoting the confidence towards newly entered data. Using the DBSCAN clustering algorithm and the confidence score, the integration ensures that the generated event labels are of high quality, despite of the heterogeneity of the participating workers. These annotations finally serve as a basis for a video retrieval system that allows users to search for video sequences on the basis of a graphical specification of team behavior or motion of the individual player. Our evaluations of the crowd-based semantic event detection and video annotation using the Microworkers platform have shown the effectiveness of the approach and have led to results that are in most cases close to the ground truth and can successfully be used for various retrieval tasks.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {63–68},
numpages = {6},
keywords = {crowdsourcing, sports, multimedia retrieval, video annotation},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@article{10.1109/TCBB.2019.2937862,
author = {Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat},
title = {Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review},
year = {2021},
issue_date = {March-April 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2937862},
doi = {10.1109/TCBB.2019.2937862},
abstract = {Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {745–758},
numpages = {14}
}

@article{10.14778/2536222.2536238,
author = {Elmeleegy, Hazem and Li, Yinan and Qi, Yan and Wilmot, Peter and Wu, Mingxi and Kolay, Santanu and Dasdan, Ali and Chen, Songting},
title = {Overview of Turn Data Management Platform for Digital Advertising},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536238},
doi = {10.14778/2536222.2536238},
abstract = {This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1138–1149},
numpages = {12}
}

@inproceedings{10.1145/3302425.3302447,
author = {Qiao, Lin and Ran, Ran and Wu, He and Zhou, Qiaoni and Liu, Sai and Liu, Yunfei},
title = {Imputation Method of Missing Values for Dissolved Gas Analysis Data Based on Iterative KNN and XGBoost},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302447},
doi = {10.1145/3302425.3302447},
abstract = {Power transformers are an important part of the power system. Accurate monitoring of its operating status is particularly important for the normal and stable operation of the entire power system and the timely diagnosis of potential faults. Dissolved Gas Analysis (DGA) can detect and judge the oil-immersed power transformer failure by comparing the dissolved gas content of the power transformer in the normal operating state and the oil in the fault state. However, in the operation process of the grid transformer, the detection data is often missing. This paper proposes an effective method based on iterative KNN and XGBoost method for missing values. Firstly, according to the XGBoost integration tree, there are missing values. Information such as the number of attribute divisions obtained by data set training calculates the importance scores of different attributes to determine the priority of the attributes, and then performs interpolation on the missing values ?in an iterative manner. The experimental results in the case of DGA dataset and different missing rate show that the proposed method is superior to the existing similar methods in accuracy, and the dataset after interpolation has a significant improvement on the classification effect of the classifier.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {11},
numpages = {7},
keywords = {Dissolved Gas Analysis, Missing Values, Iterative KNN, Interpolation Priority},
location = {Sanya, China},
series = {ACAI 2018}
}

@inproceedings{10.1145/2503859.2503863,
author = {Bento, Fernando and Costa, Carlos J.},
title = {ERP Measure Success Model; a New Perspective},
year = {2013},
isbn = {9781450322997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503859.2503863},
doi = {10.1145/2503859.2503863},
abstract = {This paper addresses the problem of defining and evaluating the success of ERP throughout the life cycle of the information system. In order to solve this problem, many of the theoretical and empirical contributions on the success of the information system are analysed and discussed.This approach allows the development of a new model; especially in Delone &amp; Mclean supported research.This work will try to establish a different perspective on the success of the ERP and can be an encouragement to some organizations or the many researchers that will be engaging in these areas, in order to help achieve more clearly the expected performance in the acquisition phase of ERPs. Many times that performance does not always happen [1].},
booktitle = {Proceedings of the 2013 International Conference on Information Systems and Design of Communication},
pages = {16–26},
numpages = {11},
keywords = {success measuring models, performance, information systems, ERP's life cycle, ERP's},
location = {Lisboa, Portugal},
series = {ISDOC '13}
}

@inproceedings{10.1145/3468791.3469119,
author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
title = {Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3469119},
doi = {10.1145/3468791.3469119},
abstract = { The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets. },
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
keywords = {Ranking, Question Answering, Knowledge Graphs},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/3443467.3444711,
author = {Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang},
title = {A Survey of Personalized Recommendation Based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3444711},
doi = {10.1145/3443467.3444711},
abstract = {Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {602–610},
numpages = {9},
keywords = {Machine learning, Personalized recommendation, Graph Neural Networks, Sparse matrix},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3491102.3502140,
author = {Rixen, Jan Ole and Colley, Mark and Askari, Ali and Gugenheimer, Jan and Rukzio, Enrico},
title = {Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502140},
doi = {10.1145/3491102.3502140},
abstract = { Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.},
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {295},
numpages = {14},
keywords = {Disclosure, User Acceptance, Personal Information, Public Experiences, Augmented Reality, Data Glasses, Mixed Reality, Social Acceptability, Consent, Comfort},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3473856.3473879,
author = {Herbert, Franziska and Schmidbauer-Wolf, Gina Maria and Reuter, Christian},
title = {Who Should Get My Private Data in Which Case? Evidence in the Wild},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3473879},
doi = {10.1145/3473856.3473879},
abstract = { As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.},
booktitle = {Mensch Und Computer 2021},
pages = {281–293},
numpages = {13},
keywords = {data sharing, survey, awareness, privacy},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inbook{10.1145/3310205.3310212,
title = {Rule-Based Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310212},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3462764,
author = {Bellio, Maura and Furniss, Dominic and Oxtoby, Neil P. and Garbarino, Sara and Firth, Nicholas C. and Ribbens, Annemie and Alexander, Daniel C. and Blandford, Ann},
title = {Opportunities and Barriers for Adoption of a Decision-Support Tool for Alzheimer’s Disease},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3462764},
doi = {10.1145/3462764},
abstract = {Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer’s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {32},
numpages = {19},
keywords = {user-centred design, Diffusion of innovation, design-reality gap, healthcare, technology adoption}
}

@article{10.1145/3411824,
author = {Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil},
title = {SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3411824},
doi = {10.1145/3411824},
abstract = {The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {107},
numpages = {26},
keywords = {Automatic Synchronization, Temporal Drift, Time Synchronization, Wearable Camera, Video, Accelerometry, Wearable Sensor}
}

@inproceedings{10.1145/3300061.3345456,
author = {Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana},
title = {Extracting 3D Maps from Crowdsourced GNSS Skyview Data},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3345456},
doi = {10.1145/3300061.3345456},
abstract = {3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {55},
numpages = {15},
keywords = {gnss snr measurements, 3d mapping, crowdsensing},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inbook{10.1145/3310205.3310213,
title = {Machine Learning and Probabilistic Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310213},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/2487575.2488217,
author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
title = {Online Controlled Experiments at Large Scale},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488217},
doi = {10.1145/2487575.2488217},
abstract = {Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1168–1176},
numpages = {9},
keywords = {randomized experiments, a/b testing, controlled experiments},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2858036.2858445,
author = {West, Peter and Giordano, Richard and Van Kleek, Max and Shadbolt, Nigel},
title = {The Quantified Patient in the Doctor's Office: Challenges &amp; Opportunities},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858445},
doi = {10.1145/2858036.2858445},
abstract = {While the Quantified Self and personal informatics fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning. In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings. This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data. Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3066–3078},
numpages = {13},
keywords = {clinical decision making, self-tracking, quantified self},
location = {San Jose, California, USA},
series = {CHI '16}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {9},
numpages = {14},
keywords = {developing countries, Global development}
}

@inbook{10.1145/3448016.3457330,
author = {Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan},
title = {DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457330},
abstract = {Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2271–2280},
numpages = {10}
}

@inbook{10.1145/3447404.3447429,
author = {McMenemy, David},
title = {Ethics in Automotive User Interface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447429},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {431–432},
numpages = {2}
}

@inbook{10.1145/3450613.3459657,
author = {Amyrotos, Christos},
title = {Adaptive Visualizations for Enhanced Data Understanding and Interpretation},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3459657},
abstract = { In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user’s individual differences, needs or requirements, and thus may hinder the user’s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user’s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user’s expertise and experience.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {291–297},
numpages = {7}
}

@inbook{10.1145/3447404.3447409,
author = {McMenemy, David},
title = {The Internet of Everything—Introducing Privacy},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447409},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {57–58},
numpages = {2}
}

@inbook{10.1145/3447404.3447430,
title = {Authors’ Biographies/Index},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447430},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {433–455},
numpages = {23}
}

@inproceedings{10.1145/3377049.3377083,
author = {Chowdhury, S. M. Habibul Mursaleen and Jahan, Ferdous and Sara, Sarawat Murtaza and Nandi, Dip},
title = {Secured Blockchain Based Decentralised Internet: A Proposed New Internet},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377083},
doi = {10.1145/3377049.3377083},
abstract = {Throughout this paper, we try to describe with blockchain technology the decentralization of the internet. A decentralized network that encourages the internet to operate from the smartphone or tablet of anybody instead of centralized servers. A decentralized implementation would be based on a peer-to-peer network that is dependent on a user community. Their machines connected to the internet will host the network, not a community of more powerful servers. Each site would be distributed across thousands of nodes on various devices. The data is therefore not contained, owned by private storage facilities. There is therefore no central point to hack, and no way for an oligarchy of entities to take control of it. A proposed alternative was formed based on a systematic literature review that demonstrates that Internet decentralization is what this modern technology needs in order to address not only the weaknesses of current servers including server down issue, hacking and data manipulation or single point of failure, but also to prevent companies from monetizing the data of citizens through their server and to market them to the advertisers.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {8},
numpages = {7},
keywords = {Encryption, Ethereum, Data Privacy, DApp, Cryptography, Blockchain, Whisper, Web 3.0, Bitcoin, Server vulnerabilities, Smart Contracts, Decentralised Web, Peer-To-Peer Network},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inbook{10.1145/3447404.3447427,
author = {McMenemy, David},
title = {Ethics and Adaptive Touch Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447427},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {407–408},
numpages = {2}
}

@inbook{10.1145/3447404.3447413,
author = {McMenemy, David},
title = {Ethical Issues of Digital Signal Processing},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447413},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {141–142},
numpages = {2}
}


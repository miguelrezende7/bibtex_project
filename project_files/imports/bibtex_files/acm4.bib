@inproceedings{10.1145/3329189.3329204,
author = {Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong},
title = {Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital Biomarkers Using Mobile Sensors},
year = {2019},
isbn = {9781450361262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329189.3329204},
doi = {10.1145/3329189.3329204},
abstract = {Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.},
booktitle = {Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare},
pages = {179–188},
numpages = {10},
keywords = {Data Quality, Cough, Digital Biomarkers, Breathing, Crowdsourced Annotation, mHealth, Breathlessness},
location = {Trento, Italy},
series = {PervasiveHealth'19}
}

@inproceedings{10.1145/2494091.2499223,
author = {Romualdo-Suzuki, Larissa and Finkelstein, Anthony and Gann, David},
title = {A Middleware Framework for Urban Data Management},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499223},
doi = {10.1145/2494091.2499223},
abstract = {The domain of inquiry of this research is the collection, organization, integration, distribution and consumption of knowledge derived from urban open data, and how it can be best offered to application cities' stakeholders through a software middleware. We argue that the extensive investigation proposed in this research will contribute to a growing body of knowledge about data integration and application in smart cities, and offer opportunities to re-think an integrated urban infrastructure.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1359–1362},
numpages = {4},
keywords = {smart cities, big data, value chain., software architecture},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/3078081.3078109,
author = {Kir\'{a}ly, P\'{e}ter},
title = {Towards an Extensible Measurement of Metadata Quality},
year = {2017},
isbn = {9781450352659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078081.3078109},
doi = {10.1145/3078081.3078109},
abstract = {This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.},
booktitle = {Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage},
pages = {111–115},
numpages = {5},
keywords = {REST API, design patterns, metadata quality, big data},
location = {G\"{o}ttingen, Germany},
series = {DATeCH2017}
}

@inproceedings{10.1145/2790755.2790774,
author = {Hamdi, Sana and Bouazizi, Emna and Faiz, Sami},
title = {A New QoS Management Approach in Real-Time GIS with Heterogeneous Real-Time Geospatial Data Using a Feedback Control Scheduling},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790774},
doi = {10.1145/2790755.2790774},
abstract = {Geographic Information System (GIS) is a computer system designed to capture, store, manipulate, analyze, manage, and present all types of spatial data. Spatial data, whether captured through remote sensors or large scale simulations becomes more and big and heterogenous. As a result, structured data and unstructured content are simultaneously accessed via an integrated user interface. The issue of real-time and heterogeneity is extremely important for taking effective decision. Thus, heterogeneous real-time spatial data management is a very active research domain nowadays. Existing research are interested in querying of real-time spatial data and their updates without taking into account the heterogeneity of real-time geospatial data. In this paper, we propose the use of the real-time Spatial Big Data and we define a new architecture called FCSA-RTSBD (Feedback Control Scheduling Architecture for Real-Time Spatial Big Data). The main objectives of this architecture are the following: take in account the heterogeneity of data, guarantee the data freshness, enhance the deadline miss ratio even in the presence of conflicts and finally satisfy the requirements of users by the improving of the quality of service (QoS).},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {174–179},
numpages = {6},
keywords = {Heterogeneous Real-Time Geospatial Data, Quality of Service, Feedback Control Scheduling, Geographic Information System, Real-Time Spatial Big Data, Transaction},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/3286606.3286794,
author = {Noussair, Lazrak and Jihad, Zahir and Hajar, Mousannif},
title = {Responsive Cities and Data Gathering: Challenges and Opportunities},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286794},
doi = {10.1145/3286606.3286794},
abstract = {For the last two decades, data driven cities have emerged as an efficient way of improving the city performance, enhancing life quality, and providing more choices to city planners and decision makers. A significant change in data driven cities in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in city development, changing the city operation system from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent system. But with more data collected the more questions raised about the optimization and space saving methods, then the quality of data collected and the efficiency of the its treatment. In this paper, we provide a survey on the data driven cities requirements, and the tools made available for the responsive cities to maintain its data.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {17},
numpages = {8},
keywords = {Quality of data, Responsive cities, Big Data, Data gathering},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3485447.3512104,
author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
title = {Improving Graph Collaborative Filtering with Neighborhood-Enriched Contrastive Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512104},
doi = {10.1145/3485447.3512104},
abstract = { Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users&nbsp;(or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user&nbsp;(or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users&nbsp;(or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users&nbsp;(or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26% and 17% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets, respectively. Our implementation code is available at: https://github.com/RUCAIBox/NCL.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2320–2329},
numpages = {10},
keywords = {Collaborative Filtering, Graph Neural Network, Contrastive Learning, Recommender System},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3194696.3194700,
author = {Palacio, Ana Le\'{o}n and L\'{o}pez, \'{O}scar Pastor},
title = {Towards an Effective Medicine of Precision by Using Conceptual Modelling of the Genome: Short Paper},
year = {2018},
isbn = {9781450357340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194696.3194700},
doi = {10.1145/3194696.3194700},
abstract = {The continuous improvement in our understanding of the human genome is leading to an increasing viable and effective Precision Medicine. Its intention is to provide a personalized solution to any individual health problem. Nevertheless, three main issues must be considered to make Precision Medicine a reality: i) the understanding of the huge amount of genomic data, spread out in hundreds of genome data sources, with different formats and contents, whose semantic interoperability is a must; ii) the development of information systems intended to guide the search of relevant genomic repositories related with a disease, the identification of significant information for its prevention, diagnosis and/or treatment and its management in an efficient software platform; iii) the high variability in the quality of the publicly available information. This paper presents a conceptual framework for solving these problems by i) using a precise conceptual schema of the human genome, and ii) introducing a method to search, identify, load and adequately interpret the required data, assuring its quality during the entire process.},
booktitle = {Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
pages = {14–17},
numpages = {4},
keywords = {precision medicine, conceptual modelling, data quality},
location = {Gothenburg, Sweden},
series = {SEHS '18}
}

@article{10.1145/3428080,
author = {Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng},
title = {Pricing-Aware Real-Time Charging Scheduling and Charging Station Expansion for Large-Scale Electric Buses},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3428080},
doi = {10.1145/3428080},
abstract = {We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {nov},
articleno = {13},
numpages = {26},
keywords = {MDP, charging pattern, charging scheduling, data driven, Electric bus}
}

@article{10.1145/3131611,
author = {Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin},
title = {Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3131611},
doi = {10.1145/3131611},
abstract = {The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {17},
numpages = {27},
keywords = {databases, clustering, Deduplication, validation}
}

@inproceedings{10.1145/3437963.3441747,
author = {Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin},
title = {Federated Deep Knowledge Tracing},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441747},
doi = {10.1145/3437963.3441747},
abstract = {Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {662–670},
numpages = {9},
keywords = {data quality evaluation, knowledge tracing, federated learning, data isolation, intelligent education},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/2659532.2659594,
author = {Jaakkola, Hannu and M\"{a}kinen, Timo and Etel\"{a}aho, Anna},
title = {Open Data: Opportunities and Challenges},
year = {2014},
isbn = {9781450327534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659532.2659594},
doi = {10.1145/2659532.2659594},
abstract = {Open data is seen as a promising source of new business, especially in the SME sector, in the form of new products, services and innovative solutions. High importance is seen also in fostering citizens' participation in political and social life and increasing the transparency of public authorities. The forerunners of the open data movement in the public sector are the USA and the UK, which started to open their public data resources in 2009. The first European Union open data related directive was drawn up as early as 2003; however progress in putting the idea into practice has been slow and adoptions by the wider member states are placed in the early 2010s. The beneficial use of open data in real applications has progressed hand in hand with the improvement of other ICT-related technologies. The (raw) data itself has no high value. The economic value comes from a balanced combination of high quality open (data) resources combined with the related value chain. This paper builds up a "big picture" of the role of open data in current society. The approach is analytical and it clarifies the topic from the viewpoints of both opportunities and challenges. The paper covers both general aspects related to open data and results of the research and regional development project conducted by the authors.},
booktitle = {Proceedings of the 15th International Conference on Computer Systems and Technologies},
pages = {25–39},
numpages = {15},
keywords = {public data, data analysis, big data, open data, networking},
location = {Ruse, Bulgaria},
series = {CompSysTech '14}
}

@inproceedings{10.1145/3047273.3047296,
author = {Choudhury, Pranab Ranjan and Behera, Manoj Kumar},
title = {Using Administrative Data for Monitoring and Improving Land Policy and Governance in India},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047296},
doi = {10.1145/3047273.3047296},
abstract = {Demands for production and dissemination of reliable data is growing with increasing demand from public policies to monitor, compare and improve global and national developmental status and targets. Implementation of intentionally agreed commitments like Millennium Development Goals (MDGs), Sustainable development Goals (SDGs) are influencing data production and availability, and the development of national statistical capacities. They also trigger challenges and opportunities in production of internationally comparable data to induce fair comparability among nations. Being a signatory to major international treaties, India has considerably improved data production, accessibility and availability over the years to ensure proper alignment of national level statistics and induce international comparison. However, very little efforts have been made to assess India's progress around data production and dissemination around growingly important land governance. This assessment attempts to identify key opportunities and challenges at the country level to improve data availability, access, timeliness and quality.India has made many progressive reforms around land laws and institutions to make land governance more inclusive and equitable; however its assessment with respect to global best practices through World Bank's Land Governance Assessment Framework (LGAF) indicate the need of improvements around different land dimensions. Movement towards good land governance outcomes is incumbent upon robust and regular monitoring mechanism of land indicators across spatial (viz. administrative boundaries, land being a state subject in India) and temporal scales.India has traditions of collecting, maintaining and reporting land information through nation-wide surveys, census, administrative and judicial reports/ databases. Its flagship program Digital India Land Record Modernization Program (DILRMP), has been supporting universal digitization of spatial and textual land records by the states. Together, these administrative and survey-derived datasets provide seamless opportunity for routine generation of data on key land indicators at low cost on a regularbasis. Land is a state subject in India. Monitoring and reporting land-indicators at state levels would help in systematically discovering and identifying good practice that can then be documented and disseminated across states, manage change, and gradually move towards a more performance-based approach to improving land governance in India. However, there have been lack of institutionalized attempts, so far, to report land-indicators at national scale.We have tried to assess the state of data in India, particularly to track and report two critical land governance indicators viz. women land rights and forest rights, critical to ensure equity and sustainability in terms of public policy. With UN's SDG, defining similar indicators, we also attempt aligning them around SDG indicators. Status of these two parameters were analyzed using nation-wide datasets collecting whole population data, through legitimate institutions following robust processes and reporting them open access.Census (human population) data and Forest Survey of India (FSI) data were used to assess village-wise forest areas eligible for recognition of rights under India's historic Forest Rights Act, 2005. Using the FSI data and meta-analysis of census data, we calculated the estimated population (150 million including 90 million tribal) living in villages that have forest land within administrative revenue boundaries, potential area (40 million ha) that can be recognized under FRA and number of villages (0.17 million) that are eligible to initiate the claim. These data were made available across administrative boundaries of state, district and village, providing opportunities for relevant Government Ministries at Central and State level and civil society to expedite the forest rights recognition under India's largest land reform process.In order to assess women's land rights (WLR) in India in the context of the SDGs, after examining the existing data sets, we used Agricultural Census data, conducted by Government of India every fifth year following the guidelines of World Census on Agriculture (WCA). Using Agricultural census data, we have developed atlas of women land rights (based on operational holdings) in India with state and district wise granularity with further disaggregation across ethnicity (caste) and other socio-economic parameters. The study also attempted to analyze the link between the inter-regional and temporal variability of WLR and relevant policies and legal-institutional frameworks among the states to see if the correlations can better inform public policy and also induce healthy competition among states to appreciate and follow best practices. This paper presents the process, methodology and results of the data-analysis for these two land indicators while delving into the scope and challenges of dealing with existing and upcoming big datasets in India to report the land governance indicators and the potential policy spinoffs.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {127–135},
numpages = {9},
keywords = {Forest Rights, Women Land Rights, SDGs, Big Data, India},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3503928.3503930,
author = {Zeng, Xian and Han, Minglei and Li, Ning and Liu, Peng},
title = {Research on Real-Time Data Warehouse Technology for Sea Battlefield},
year = {2021},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503930},
doi = {10.1145/3503928.3503930},
abstract = {Aiming at the data governance problems in the sea battlefield, this paper proposes a real-time data warehouse construction method for naval battlefields, which realizes the functions of storage, analysis and mining of battle data. This paper completes the construction of the data warehouse from the aspects of real-time data life cycle, real-time data application scenarios, data warehouse real-time safeguard measures, and data warehouse theme design. It can effectively provide data support for naval combat forces and provide auxiliary decision-making for commanders.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {13–20},
numpages = {8},
keywords = {Sea Battlefield, Real-time, Data Warehouse, Big Data},
location = {Shanghai, China},
series = {ICISE 2021}
}

@inproceedings{10.5555/2857070.2857186,
author = {Blake, Catherine and Souden, Maria and Anderson, Caryn L. and Twidale, Michael and Stelmack, Jenifer E.},
title = {Online Question Answering Practices to Support Healthcare Data Re-Use},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Institutional data collection practices inevitably evolve over time, especially in a distributed clinical setting. Clinical and administrative data can improve health and healthcare, but only if researchers ensure that the data is well-aligned to their reuse goals and that they have adequately accounted for changes in data collection practices over time. Our goal is to understand information behaviors of health services data users as they bridge the gap between the historical data and their intended data reuse goals. This project leverages more than a decade of listserv posts related to the use of clinical and administrative data by US Department of Veterans Affairs (VA) employees, providing longitudinal insight into data reuse practices in both research and operational settings. In this paper we report the results of a pilot study that highlighted questions raised in the use of data and the knowledge engaged to answer them.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {116},
numpages = {4},
keywords = {big data, social question answering, forums, health, communities of practice},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.1145/3433996.3434008,
author = {Guo, Xusheng and Liang, Likeng and Liu, Yuanxia and Weng, Heng and Hao, Tianyong},
title = {The Construction of a Diabetes-Oriented Frequently Asked Question Corpus for Automated Question-Answering Services},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434008},
doi = {10.1145/3433996.3434008},
abstract = {In recent years, the prevalence of diabetes has been increasing rapidly worldwide. With the advancement of information technology, automated question-answering services for healthcare, which are commonly based on annotated corpus in health domain, have positive effects on health knowledge spread and daily health management for high-risk populations. This paper proposes to construct a large scale diabetes corpus of frequently-asked questions for automated question-answering services and evaluations. Concentrating on the characteristics of diabetes-related factors that reflect conditions of diabetes, this work establishes an annotated dataset containing professional question &amp; answer pairs about diabetes and their annotated question target categories. The corpus is applicable for various question-answering applications, supporting users to retrieve needed information, arrange diets, adhere to scientific medication as well as prevent and control disease complications.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {60–66},
numpages = {7},
keywords = {Diabetes, frequently-asked questions, visualization, corpus construction},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3411764.3445518,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
doi = {10.1145/3411764.3445518},
abstract = { AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {39},
numpages = {15},
keywords = {data quality, Ghana, Kenya, AI, India, high-stakes AI, data collectors, data politics, Data, application-domain experts, data cascades, USA, developers, ML, Uganda, raters, Nigeria},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3447513,
author = {Deng, Song and Chen, Fulin and Dong, Xia and Gao, Guangwei and Wu, Xindong},
title = {Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3447513},
doi = {10.1145/3447513},
abstract = {Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {95},
numpages = {28},
keywords = {abnormal load recognition, power load forecasting, probability distribution, adaptive evolution, Gene expression programming}
}

@inproceedings{10.1145/3356991.3365474,
author = {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
title = {SONET: A Semantic Ontological Network Graph for Managing Points of Interest Data Heterogeneity},
year = {2019},
isbn = {9781450369602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356991.3365474},
doi = {10.1145/3356991.3365474},
abstract = {Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
articleno = {6},
numpages = {6},
keywords = {points of interest, openstreetmap, big data, graph database, ontology},
location = {Chicago, Illinois},
series = {GeoHumanities '19}
}

@inproceedings{10.1145/3378539.3393864,
author = {Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna},
title = {Challenges of Applying Predictive Analytics in Transport Logistics},
year = {2020},
isbn = {9781450371308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378539.3393864},
doi = {10.1145/3378539.3393864},
abstract = {The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics.},
booktitle = {Proceedings of the 2020 on Computers and People Research Conference},
pages = {144–151},
numpages = {8},
keywords = {transport logistics, supply chain management, predictive analytics, challenges, big data},
location = {Nuremberg, Germany},
series = {SIGMIS-CPR'20}
}

@inproceedings{10.1145/2755492.2755494,
author = {Huang, Qunying and Cao, Guofeng and Wang, Caixia},
title = {From Where Do Tweets Originate? A GIS Approach for User Location Inference},
year = {2014},
isbn = {9781450331401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755492.2755494},
doi = {10.1145/2755492.2755494},
abstract = {A number of natural language processing and text-mining algorithms have been developed to extract the geospatial cues (e.g., place names) to infer locations of content creators from publicly available information, such as text content, online social profiles, and the behaviors or interactions of users from social networks. These studies, however, can only successfully infer user locations at city levels with relatively decent accuracy, while much higher resolution is required for meaningful spatiotemporal analysis in geospatial fields. Additionally, geographical cues exploited by current text-based approaches are hidden in the unreliable, unstructured, informal, ungrammatical, and multilingual data, and therefore are hard to extract and make meaningful correctly. Instead of using such hidden geographic cues, this paper develops a GIS approach that can infer the true origin of tweets down to the zip code level by using and mining spatial (geo-tags) and temporal (timestamps when a message was posted) information recorded on user digital footprints. Further, individual major daily activity zones and mobility can be successfully inferred and predicted. By integrating GIS data and spatiotemporal clustering methods, this proposed approach can infer individual daily physical activity zones with spatial resolution as high as 20 m by 20 m or even higher depending on the number of digit footprints collected for social media users. The research results with detailed spatial resolution are necessary and useful for various applications such as human mobility pattern analysis, business site selection, disease control, or transportation systems improvement.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Location-Based Social Networks},
pages = {1–8},
numpages = {8},
keywords = {spatiotemporal clustering, spatial clustering, human mobility, geography, big data},
location = {Dallas/Fort Worth, Texas},
series = {LBSN '14}
}

@inproceedings{10.1145/2882903.2912574,
author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
title = {Data Cleaning: Overview and Emerging Challenges},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2912574},
doi = {10.1145/2882903.2912574},
abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2201–2206},
numpages = {6},
keywords = {data cleaning, integrity constraints, statistical cleaning, data quality, sampling},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3345551,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Large-Scale Semantic Integration of Linked Data: A Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3345551},
doi = {10.1145/3345551},
abstract = {A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,&lt;?brk?&gt;(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {103},
numpages = {40},
keywords = {big data, RDF, Data integration, semantic web, data discovery}
}

@inproceedings{10.5555/2873021.2873031,
author = {Ferguson, Holly T. and Vardeman, Charles F. and Buccellato, Aimee P. C.},
title = {Capturing an Architectural Knowledge Base Utilizing Rules Engine Integration for Energy and Environmental Simulations},
year = {2015},
isbn = {9781510801042},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The era of "Big Data" presents new challenges and opportunities to impact how the built environment is designed and constructed. Modern design tools and material databases should be more scalable, reliable, and accessible to take full advantage of the quantity of available building data. New approaches providing well-structured information can lead to robust decision support for architectural simulations earlier in the design process; rule-based decision engines and knowledge bases are the link between current data and useful decision frameworks. Integrating distributed API-based systems means that material data silos existing in modern tools can become enriched and extensible for future use with additional data from building documents, other databases, and the minds of design professionals. The PyKE rules engine extension to the Green Scale (GS) Tool improves material searches, creates the opportunity for incorporating additional rules via a REST interface, and enables integration with the Semantic Web via Linked Data principles.},
booktitle = {Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design},
pages = {67–74},
numpages = {8},
keywords = {SPARAQL, semantic web, standardization, SPIN, experimentation, OWL, knowledge based rules, RIF, machine learning, ontological knowledge engine, REST, design, linked data, HCI, green scale tool, reliability, verification, sustainable data, PyKE, algorithms, big data, SWIRL, expert systems, performance},
location = {Alexandria, Virginia},
series = {SimAUD '15}
}

@inproceedings{10.1145/2882903.2899414,
author = {Agrawal, Divy and Ba, Lamine and Berti-Equille, Laure and Chawla, Sanjay and Elmagarmid, Ahmed and Hammady, Hossam and Idris, Yasser and Kaoudi, Zoi and Khayyat, Zuhair and Kruse, Sebastian and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan and Zaki, Mohammed J.},
title = {Rheem: Enabling Multi-Platform Task Execution},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899414},
doi = {10.1145/2882903.2899414},
abstract = {Many emerging applications, from domains such as healthcare and oil &amp; gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2069–2072},
numpages = {4},
keywords = {data analytics, big data, cross-platform execution},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3436817,
author = {Harley, Kelsey and Cooper, Rodney},
title = {Information Integrity: Are We There Yet?},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436817},
doi = {10.1145/3436817},
abstract = {The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {33},
numpages = {35},
keywords = {information trustworthiness, Clark-Wilson model, information security, Integrity, Biba’s model, quality assessment, security requirements, data quality, information flow, quality dimension, noninterference, information quality, information integrity}
}

@inproceedings{10.1145/3219819.3219916,
author = {Xin, SHEN and Yang, Hongxia and Xian, Weizhao and Ester, Martin and Bu, Jiajun and Wang, Zhongyao and Wang, Can},
title = {Mobile Access Record Resolution on Large-Scale Identifier-Linkage Graphs},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219916},
doi = {10.1145/3219819.3219916},
abstract = {The e-commerce era is witnessing a rapid increase of mobile Internet users. Major e-commerce companies nowadays see billions of mobile accesses every day. Hidden in these records are valuable user behavioral characteristics such as their shopping preferences and browsing patterns. And, to extract these knowledge from the huge dataset, we need to first link records to the corresponding mobile devices. This Mobile Access Records Resolution (MARR) problem is confronted with two major challenges: (1) device identifiers and other attributes in access records might be missing or unreliable; (2) the dataset contains billions of access records from millions of devices. To the best of our knowledge, as a novel challenge industrial problem of mobile Internet, no existing method has been developed to resolve entities using mobile device identifiers in such a massive scale. To address these issues, we propose a SParse Identifier-linkage Graph (SPI-Graph) accompanied with the abundant mobile device profiling data to accurately match mobile access records to devices. Furthermore, two versions (unsupervised and semi-supervised) of Parallel Graph-based Record Resolution (PGRR) algorithm are developed to effectively exploit the advantages of the large-scale server clusters comprising of more than 1,000 computing nodes. We empirically show superior performances of PGRR algorithms in a very challenging and sparse real data set containing 5.28 million nodes and 31.06 million edges from 2.15 billion access records compared to other state-of-the-arts methodologies.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {886–894},
numpages = {9},
keywords = {scalable algorithms, big data, mobile access record resolution, graph algorithms},
location = {London, United Kingdom},
series = {KDD '18}
}

@inbook{10.1145/3341105.3373989,
author = {Alaa, Mostafa and Bolock, Alia El and Abas, Mostafa and Abdennadher, Slim and Herbert, Cornelia},
title = {AppGen: A Framework for Automatic Generation of Data Collection Apps},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373989},
abstract = {Data, and its collection, is one core aspect of technology and research, nowadays. Various scientific disciplines are interested in collecting human data in practically any context (at home, at work, during leisure time). For example, experts from the field of Psychology design studies for reliable and valid data collection in the laboratory and in the wild. We propose a generic platform for data-collection software development to be used by scientists without a programming background. This is done by adapting a basic Unity project through a configuration file provided by the platform users through an easy to use user interface. The scientific user can adapt and rearrange pre-defined data collection modules targeting a desired research question, implement it as application within the data collection platform and use and manage the application for data collection and later data analysis. As a proof of concept, the platform was embedded with build-in application modules for wide-spread Psychology data collection experiments. The versatility of the platform was tested by creating three diverse prototypical applications. Finally, the usability of the proposed platform evaluated using the System Usability Scale obtained high usability results. The robust module-based nature of the platform architecture makes is possible to create a various range of of psychologically-proven applications with different features to be decided by the researcher. This holds true for both the development phase of the applications, as well as, for after deployment.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1906–1913},
numpages = {8}
}

@inproceedings{10.1145/3173574.3173710,
author = {Bowyer, Alex and Montague, Kyle and Wheater, Stuart and McGovern, Ruth and Lingam, Raghu and Balaam, Madeline},
title = {Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173710},
doi = {10.1145/3173574.3173710},
abstract = {Across social care, healthcare and public policy, enabled by the "big data" revolution (which has normalized large-scale data-based decision-making), there are moves to "join up" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {data privacy, ubicomp, personal data, user-centered design, social care, data security, civic data, big data, family research, healthcare, dynamic consent, boundary objects, design games, data sharing, family, ethnographic interviews, family design games},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3478905.3478920,
author = {Fei, Yiming and Yuan, Xiaoyue and Ren, Mengmeng and Fan, Shuhai},
title = {Research on Horizontal Integration Scheme for Mass Customization Data Quantity and Quality Problem: Horizontal Integration Scheme for MC},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478920},
doi = {10.1145/3478905.3478920},
abstract = {To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {69–73},
numpages = {5},
keywords = {Mass Customization, Horizontal Integration, LiDAR Camera Technology, Data Quality},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/2910674.2935861,
author = {Bj\"{o}rk, Kaj-Mikael and Eirola, Emil and Miche, Yoan and Lendasse, Amaury},
title = {A New Application of Machine Learning in Health Care},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910674.2935861},
doi = {10.1145/2910674.2935861},
abstract = {In our ever more complex world, the field of analytics has dramatically increased its importance. Gut feeling is no longer sufficient in decision making, but intuition has to be combined with support from the huge amount of data available today. Even if the amount of data is enormous, the quality of the data is not always good. Problems arise in at least two situations: i) the data is imprecise by nature and ii) the data is incomplete (or there are missing parts in the data set). Both situations are problematic and need to be addressed appropriately. If these problems are solved, applications are to be found in various interesting fields. We aim at achieving significant methodology development as well as creative solutions in the domain of medicine, information systems and risk management. This paper sets focus especially on missing data problems in the field of medicine when presenting a new project in its very first phase.},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {49},
numpages = {4},
keywords = {Missing values, Health care, Machine Learning, Big Data, Huntington's disease},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@article{10.1145/2822898,
author = {Coletti, Paolo and Murgia, Maurizio},
title = {Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2822898},
doi = {10.1145/2822898},
abstract = {This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {16},
numpages = {23},
keywords = {stock market, data quality, Financial database, data integration}
}

@inproceedings{10.1145/3340531.3414077,
author = {Ukil, Arijit and Marin, Leandro and Jara, Antonio and Farserotu, John},
title = {On the Knowledge-Driven Analytics and Systems Impacting Human Quality of Life},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414077},
doi = {10.1145/3340531.3414077},
abstract = {The present scenario of Covid-19 pandemic has disrupted the human life to a larger extent. In such context, human-centric applications and systems that endeavor to positively impact the human quality of life is of utmost importance. Knowledge-driven analytics that help to build such intelligent systems play important role to construct the required eco-system on the macro-scale. It is worth mentioning that Knowledge-Driven Analytics and Systems Impacting Human Quality of Life (KDAH) workshop in ACM International Conference on Information and Knowledge Management (CIKM), attempts to bring out the intricate research direction for enabling a sustainable human society through the positive co-existence of human beings and intelligent systems.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3539–3540},
numpages = {2},
keywords = {knowledge, big data, sensors, privacy, deep learning, artificial intelligence, security, human life},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3216122.3216148,
author = {Chabin, Jacques and Gomes-Jr., Luiz and Halfeld-Ferrari, Mirian},
title = {A Context-Driven Querying System for Urban Graph Analysis},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216148},
doi = {10.1145/3216122.3216148},
abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {297–301},
numpages = {5},
keywords = {data quality, constraints, data graph, Query language, smart city},
location = {Villa San Giovanni, Italy},
series = {IDEAS 2018}
}

@inproceedings{10.1145/2948992.2949009,
author = {Martins, Pedro and Cec\'{\i}lio, Jos\'{e} and Abbasi, Maryam and Furtado, Pedro},
title = {GPII: A Benchmark for Generic Purpose Image Information},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949009},
doi = {10.1145/2948992.2949009},
abstract = {The growing number of different models and approaches for Geographic Information Systems (GIS) brings high complexity when we want to develop new approaches and compare a new GIS algorithm. In order to test and compare different processing models and approaches, in a simple way, we identified the need of defining uniform testing methods, able to compare processing algorithms in terms of performance and accuracy regarding large image processing, algorithms for GIS pattern-detection.Taking into account, for instance, images collected during a done flight or a satellite, it is important to know the processing cost to extract data when applying different processing models and approaches, as well as their accuracy (compare execution time vs. extracted data quality). In this work, we propose a GIS Benchmark (GPII), a benchmark that allows evaluating different approaches to detect/extract selected features from a GIS dataset. Considering a given dataset (or two data-sets, from different years, of the same region), it provides linear methods to compare different performance parameters regarding GIS information, making possible to access the most relevant information in terms of features and processing efficiency. Moreover, our approach to test algorithms makes possible to change the data-set in order to support different purpose algorithms.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {119–122},
numpages = {4},
keywords = {Benchmark, Big-data, pattern-detection, GIS, algorithms, experimentation, performance, spatio-temporal databases},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/2939672.2939799,
author = {Xu, Tong and Zhu, Hengshu and Zhao, Xiangyu and Liu, Qi and Zhong, Hao and Chen, Enhong and Xiong, Hui},
title = {Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939799},
doi = {10.1145/2939672.2939799},
abstract = {With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1285–1294},
numpages = {10},
keywords = {mobile data mining, taxi trajectories, social influence},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3290607.3313002,
author = {Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
title = {MYND: A Platform for Large-Scale Neuroscientific Studies},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313002},
doi = {10.1145/3290607.3313002},
abstract = {We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {medical studies, smartphone application, electrophysiology, neuroscience, big data, wearable sensors, user-centered design},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@article{10.1007/s00779-017-1034-0,
author = {Shemshadi, Ali and Sheng, Quan Z. and Qin, Yongrui and Sun, Aixin and Zhang, Wei Emma and Yao, Lina},
title = {Searching for the Internet of Things: Where It is and What It Looks Like},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {6},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-017-1034-0},
doi = {10.1007/s00779-017-1034-0},
abstract = {The Internet of Things (IoT), in general, is a compelling paradigm that aims to connect everyday objects to the Internet. Nowadays, IoT is considered as one of the main technologies which contribute towards reshaping our daily lives in the next decade. IoT unlocks many exciting new opportunities in a variety of applications in research and industry domains. However, many have complained about the absence of the real-world IoT data. Unsurprisingly, a common question that arises regularly nowadays is "Does the IoT already exist?". So far, little has been known about the real-world situation on IoT, its attributes, the presentation of data, and user interests. To answer this question, in this work, we conduct an in-depth analytical investigation on real IoT data. More specifically, we identify IoT data sources over the Web and develop a crawler engine to collect large-scale real-world IoT data for the first time. We make the results of our work available to the public in order to assist the community in the future research. In particular, we collect the data of nearly two million Internet connected objects and study trends in IoT using a real-world query set from an IoT search engine. Based on the collected data and our analysis, we identify the typical characteristics of IoT data. The most intriguing finding of our study is that IoT data is mainly disseminated using Web Mapping while the emerging IoT solutions such as the Web of Things are currently not well adopted. On top of our findings, we further discuss future challenges and open research problems in the IoT area.},
journal = {Personal Ubiquitous Comput.},
month = {dec},
pages = {1097–1112},
numpages = {16},
keywords = {Internet of things, Web of things, Information retrieval, Web mapping, Big data}
}

@article{10.1145/3432247,
author = {Garriga, Martin and Aarns, Koen and Tsigkanos, Christos and Tamburri, Damian A. and Heuvel, Wjan Van Den},
title = {DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3432247},
doi = {10.1145/3432247},
abstract = {Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {36},
numpages = {25},
keywords = {cyber-physical systems, airport management, DataOps, big data, systems governance, Data-intensive systems}
}

@inproceedings{10.1145/3386723.3387850,
author = {Maqboul, Jaouad and Jaouad, Bouchaib Bounabat},
title = {Contribution of Artificial Neural Network in Predicting Completeness Through the Impact and Complexity of Its Improvement},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387850},
doi = {10.1145/3386723.3387850},
abstract = {The technological evolution and the immensity of the data produced, circulated into company makes these data, the real capital of the companies to the detriment of the customers. The erroneous data put the knockout to relationships with customers, the company must address this problem and identify the quality projects on which it must make an effort. In this article, we will present an approach based on qualitative and quantitative analysis to help the decision-makers to target data by its impacts and complexities of process improvement. The Qualitative study will be a survey and a quantitative to learn from survey data to decide the prediction and the completeness of data.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {31},
numpages = {8},
keywords = {Data quality improvement project, data quality assessment and improvement, cost of data quality, cost/benefit analysis, artificial neural network},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@inproceedings{10.1145/3338906.3338931,
author = {Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei},
title = {Robust Log-Based Anomaly Detection on Unstable Log Data},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338931},
doi = {10.1145/3338906.3338931},
abstract = {Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {807–817},
numpages = {11},
keywords = {Deep Learning, Anomaly Detection, Log Analysis, Log Instability, Data Quality},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3404555.3404621,
author = {Zhu, Ruyi},
title = {Traffic Condition Prediction of Urban Roads Based on Neural Network},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404621},
doi = {10.1145/3404555.3404621},
abstract = {Real-time and reliable traffic flow estimation is the basis of urban traffic management and control. However, the existing research focuses on how to use the historical data of surveillance intersection to predict future traffic conditions. As we know, there are few effective algorithms to infer the real-time traffic state of non-surveillance intersections from limited road surveillance by using traffic information in the urban road system. In this paper, we introduce a new solution to solve the prediction task of traffic flow analysis by using traffic data, especially taxi historical data, traffic network data and intersection historical data. The proposed solution takes advantage of GCN and CGAN, and we improved the Unet to realize an important part of the generator. Then, we capture the relationship between the intersections with surveillance and the intersections without surveillance by floating taxi-cabs covered in the whole city. The framework of CGAN can adjust the weights and enhance the inference ability to generate complete traffic status under current conditions. The experimental results show that our method is superior to other methods on the accuracy of traffic volume inference.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {30–36},
numpages = {7},
keywords = {real-time traffic condition, big data, Urban road system, forecasting, video surveillance system},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3495018.3495097,
author = {Chen, Zhangbin and Liu, Yang},
title = {Research and Construction of University Data Governance Platform Based on Smart Campus Environment},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495097},
doi = {10.1145/3495018.3495097},
abstract = {Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {450–455},
numpages = {6},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3328905.3332513,
author = {Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian},
title = {A Real-World Distributed Infrastructure for Processing Financial Data at Scale},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3332513},
doi = {10.1145/3328905.3332513},
abstract = {Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {254–255},
numpages = {2},
keywords = {stream-processing, financial data, Event-processing, infrastructure, publish/subscribe, big data, quality of information, broker network},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@inproceedings{10.1145/2757384.2757396,
author = {Cao, Paul Y. and Li, Gang and Chen, Guoxing and Chen, Biao},
title = {Mobile Data Collection Frameworks: A Survey},
year = {2015},
isbn = {9781450335249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757384.2757396},
doi = {10.1145/2757384.2757396},
abstract = {Mobile phones equipped with powerful sensors have become ubiquitous in recent years. Mobile sensing applications present an unprecedented opportunity to collect and analyze information from mobile devices. Much of the work in mobile sensing has been done on designing monolithic applications but inadequate attention has been paid to general mobile data collection frameworks. In this paper, we provide a survey on how to build a general purpose mobile data collection framework. We identify the basic requirements and present an architecture for such a framework. We survey existing works to summarize existing approaches to address the basic requirements. Eight major mobile data collection frameworks are compared with respect to the requirements as well as additional issues on privacy, energy and incentives.},
booktitle = {Proceedings of the 2015 Workshop on Mobile Big Data},
pages = {25–30},
numpages = {6},
keywords = {data collection framework, mobile data},
location = {Hangzhou, China},
series = {Mobidata '15}
}

@article{10.1145/2906149,
author = {Khan, Suleman and Gani, Abdullah and Wahab, Ainuddin Wahid Abdul and Bagiwa, Mustapha Aminu and Shiraz, Muhammad and Khan, Samee U. and Buyya, Rajkumar and Zomaya, Albert Y.},
title = {Cloud Log Forensics: Foundations, State of the Art, and Future Directions},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2906149},
doi = {10.1145/2906149},
abstract = {Cloud log forensics (CLF) mitigates the investigation process by identifying the malicious behavior of attackers through profound cloud log analysis. However, the accessibility attributes of cloud logs obstruct accomplishment of the goal to investigate cloud logs for various susceptibilities. Accessibility involves the issues of cloud log access, selection of proper cloud log file, cloud log data integrity, and trustworthiness of cloud logs. Therefore, forensic investigators of cloud log files are dependent on cloud service providers (CSPs) to get access of different cloud logs. Accessing cloud logs from outside the cloud without depending on the CSP is a challenging research area, whereas the increase in cloud attacks has increased the need for CLF to investigate the malicious activities of attackers. This paper reviews the state of the art of CLF and highlights different challenges and issues involved in investigating cloud log data. The logging mode, the importance of CLF, and cloud log-as-a-service are introduced. Moreover, case studies related to CLF are explained to highlight the practical implementation of cloud log investigation for analyzing malicious behaviors. The CLF security requirements, vulnerability points, and challenges are identified to tolerate different cloud log susceptibilities. We identify and introduce challenges and future directions to highlight open research areas of CLF for motivating investigators, academicians, and researchers to investigate them.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {7},
numpages = {42},
keywords = {confidentiality, authenticity, correlation of cloud logs, big data, integrity, cloud log forensics, Cloud computing}
}

@article{10.1145/3379445,
author = {Bonifati, Angela and Holubov\'{a}, Irena and Prat-P\'{e}rez, Arnau and Sakr, Sherif},
title = {Graph Generators: State of the Art and Open Challenges},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379445},
doi = {10.1145/3379445},
abstract = {The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {36},
numpages = {30},
keywords = {generators, Big data management, benchmarks, synthetic data, graph data}
}

@inbook{10.1145/3383583.3398539,
author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
title = {Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398539},
abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {235–242},
numpages = {8}
}

@inproceedings{10.1145/3340017.3340022,
author = {Wieczorkowski, Jundefineddrzej},
title = {Barriers to Using Open Government Data},
year = {2019},
isbn = {9781450362375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340017.3340022},
doi = {10.1145/3340017.3340022},
abstract = {The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.},
booktitle = {Proceedings of the 2019 3rd International Conference on E-Commerce, E-Business and E-Government},
pages = {15–20},
numpages = {6},
keywords = {Open Data, Central Repository for Public Information, OGD, LOD, Linked Open Data, Open Government Data, CRPI, E-government, Big Data, Linked Data},
location = {Lyon, France},
series = {ICEEG 2019}
}

@inproceedings{10.5555/2740769.2740814,
author = {Borgman, Christine L. and Darch, Peter T. and Sands, Ashley E. and Wallis, Jillian C. and Traweek, Sharon},
title = {The Ups and Downs of Knowledge Infrastructures in Science: Implications for Data Management},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {The promise of technology-enabled, data-intensive scholarship is predicated upon access to knowledge infrastructures that are not yet in place. Scientific data management requires expertise in the scientific domain and in organizing and retrieving complex research objects. The Knowledge Infrastructures project compares data management activities of four large, distributed, multidisciplinary scientific endeavors as they ramp their activities up or down; two are big science and two are small science. Research questions address digital library solutions, knowledge infrastructure concerns, issues specific to individual domains, and common problems across domains. Findings are based on interviews (n=113 to date), ethnography, and other analyses of these four cases, studied since 2002. Based on initial comparisons, we conclude that the roles of digital libraries in scientific data management often depend upon the scale of data, the scientific goals, and the temporal scale of the research projects being supported. Digital libraries serve immediate data management purposes in some projects and long-term stewardship in others. In small science projects, data management tools are selected, designed, and used by the same individuals. In the multi-decade time scale of some big science research, data management technologies, policies, and practices are designed for anticipated future uses and users. The need for library, archival, and digital library expertise is apparent throughout all four of these cases. Managing research data is a knowledge infrastructure problem beyond the scope of individual researchers or projects. The real challenges lie in designing digital libraries to assist in the capture, management, interpretation, use, reuse, and stewardship of research data.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {257–266},
numpages = {10},
keywords = {biology, sensor networks, data management, small science, big science, big data, little science, astronomy, knowledge infrastructures, digital libraries},
location = {London, United Kingdom},
series = {JCDL '14}
}

@article{10.14778/3007263.3007320,
author = {Chu, Xu and Ilyas, Ihab F.},
title = {Qualitative Data Cleaning},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007320},
doi = {10.14778/3007263.3007320},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning "big data" in terms of scale and distribution.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1605–1608},
numpages = {4}
}

@inproceedings{10.1145/3409501.3409542,
author = {Hongmeng, Zhang and Zhiqiang, Zhu and Lei, Sun and Xiuqing, Mao and Yuehan, Wang},
title = {A Detection Method for DeepFake Hard Compressed Videos Based on Super-Resolution Reconstruction Using CNN},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409542},
doi = {10.1145/3409501.3409542},
abstract = {The DeepFake video detection method based on convolutional neural networks has a poor performance in the dataset of hard compressed DeepFake video. And a large number of false tests will occur to the real data. To solve this problem, a networks model detection method for super-resolution reconstruction of DeepFake video is proposed. First of all, the face area of real data is processed by Gaussian blur, which is converted into negative data, and the real data and processing data are input into neural network for training. Then the residual network is used for super-resolution reconstruction of test data. Finally, the trained model is used to test the video after super-resolution reconstruction. Experiments show that the proposed method can reduce the false detection rate and improve the accuracy in detection of single frames.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {98–103},
numpages = {6},
keywords = {DeepFake detection, Super-resolution reconstruction, Hard compressed video, Deep Learning},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI 2020}
}

@article{10.1145/3301284,
author = {Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele},
title = {Urban Computing Leveraging Location-Based Social Network Data: A Survey},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3301284},
doi = {10.1145/3301284},
abstract = {Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {17},
numpages = {39},
keywords = {urban informatics, location-based social networks, big data, city dynamics, urban societies, urban sensing, Urban computing}
}

@inproceedings{10.1145/3447548.3467129,
author = {Deng, Alex and Li, Yicheng and Lu, Jiannan and Ramamurthy, Vivek},
title = {On Post-Selection Inference in A/B Testing},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467129},
doi = {10.1145/3447548.3467129},
abstract = {When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2743–2752},
numpages = {10},
keywords = {randomization, A/B testing, post-selection inference, winner's curse, big data, regression, bias correction, machine learning, empirical Bayes, online metrics},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3410992.3411027,
author = {Kamel, Mohammed B. M. and Wallis, Kevin and Ligeti, Peter and Reich, Christoph},
title = {Distributed Data Validation Network in IoT: A Decentralized Validator Selection Model},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3411027},
doi = {10.1145/3410992.3411027},
abstract = {The generated real-time data on the Internet of Things (IoT) and the ability to gather and manipulate them are positively affecting various fields. One of the main concerns in IoT is how to provide trustworthy data. The data validation network ensures that the generated data by data sources in the IoT are trustworthy. However, the existing data validation network depends on a centralized entity for the selection of data validators. In this paper, a decentralized validator selection model is proposed. The proposed model creates multiple clusters using the distributed hash table (DHT) technique. The selection process of data validators from different clusters in the model is done randomly in a decentralized scheme. It provides a global method of assignment, selection, and verification of the selected validators in the network.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {12},
numpages = {8},
keywords = {data validation network, cluster-based data \^{A}\u{a}Validation, big data, internet of things, industrial internet of things, distributed hash table, data validation},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@inproceedings{10.1145/3442555.3442579,
author = {Maziku, Hellen},
title = {Improved Data Accuracy Assessment Tool for Information Management Systems},
year = {2020},
isbn = {9781450388092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442555.3442579},
doi = {10.1145/3442555.3442579},
abstract = {Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.},
booktitle = {2020 the 6th International Conference on Communication and Information Processing},
pages = {148–152},
numpages = {5},
keywords = {Data Quality Assessment, Accuracy, Information Management Systems, Human Centered Design},
location = {Tokyo, Japan},
series = {ICCIP 2020}
}

@inproceedings{10.1145/2457317.2457382,
author = {Jiang, Yu and Deng, Dong and Wang, Jiannan and Li, Guoliang and Feng, Jianhua},
title = {Efficient Parallel Partition-Based Algorithms for Similarity Search and Join with Edit Distance Constraints},
year = {2013},
isbn = {9781450315999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457317.2457382},
doi = {10.1145/2457317.2457382},
abstract = {The quantity of data in real-world applications is growing significantly while the data quality is still a big problem. Similarity search and similarity join are two important operations to address the poor data quality problem. Although many similarity search and join algorithms have been proposed, they did not utilize the abilities of modern hardware with multi-core processors. It calls for new parallel algorithms to enable multi-core processors to meet the high performance requirement of similarity search and join on big data. To this end, in this paper we propose parallel algorithms to support efficient similarity search and join with edit-distance constraints. We adopt the partition-based framework and extend it to support parallel similarity search and join on multi-core processors. We also develop two novel pruning techniques. We have implemented our algorithms and the experimental results on two real datasets show that our parallel algorithms achieve high performance and obtain good speedup.},
booktitle = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
pages = {341–348},
numpages = {8},
keywords = {parallel algorithms, content filter, similarity search, similarity join},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1109/CCGrid.2015.24,
author = {Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing},
title = {Eliminating the Redundancy in MapReduce-Based Entity Resolution},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.24},
doi = {10.1109/CCGrid.2015.24},
abstract = {Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1233–1236},
numpages = {4},
keywords = {redundancy elimination, blocking, entity resolution, MapReduce},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3340531.3414073,
author = {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
title = {DataMod2020: 9th International Symposium "From Data to Models and Back"},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414073},
doi = {10.1145/3340531.3414073},
abstract = {DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3531–3532},
numpages = {2},
keywords = {machine learning, text mining, big data analytics, formal methods, processing mining, process calculi, deep learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3397536.3422232,
author = {Soliman, Aiman and Terstriep, Jeffrey},
title = {Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422232},
doi = {10.1145/3397536.3422232},
abstract = {Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {593–596},
numpages = {4},
keywords = {Deep Learning, Geospatial Big Data, Geospatial Data Gateway, Image Preprocessing, Remote Sensing, Scientific Reproducibility},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/2806416.2806418,
author = {Zhou, Xiaofang and Zheng, Kai and Jueng, Hoyoung and Xu, Jiajie and Sadiq, Shazia},
title = {Making Sense of Spatial Trajectories},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806418},
doi = {10.1145/2806416.2806418},
abstract = {Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. Daily increases of real-time trajectory data have also been phenomenal in recent years. More and more new applications have emerged to derive business values from both trajectory data warehouses and real-time trajectory data. Due to their very large volumes, their nature of streaming, their highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data becomes one of the crucial areas for big data analytics. In this paper we will present a review of the extensive work in spatiotemporal data management and trajectory mining, and discuss new challenges and new opportunities in the context of new applications, focusing on recent advances in trajectory data management and trajectory mining from their foundations to high performance processing with modern computing infrastructure.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {671–672},
numpages = {2},
keywords = {trajectory data management, trajectory mining, spatiotemporal database},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@article{10.1145/3328747,
author = {Colborne, Adrienne and Smit, Michael},
title = {Characterizing Disinformation Risk to Open Data in the Post-Truth Era},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328747},
doi = {10.1145/3328747},
abstract = {Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {13},
numpages = {13},
keywords = {post-truth, risk identification, data quality assurance, fake news, Open data, risk mitigation}
}

@article{10.1145/3316416.3316425,
author = {Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda},
title = {NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3316416.3316425},
doi = {10.1145/3316416.3316425},
abstract = {In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/2567574.2567582,
author = {Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.},
title = {Educational Data Sciences: Framing Emergent Practices for Analytics of Learning, Organizations, and Systems},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567582},
doi = {10.1145/2567574.2567582},
abstract = {In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {193–202},
numpages = {10},
keywords = {theories and theoretical concepts for understanding learning, data-driven decisions, learner analytics, learning analytics, educational data mining, tools for sense-making in learning analytics, analytic approaches, methods, educational data science, big data},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@article{10.14778/3415478.3415562,
author = {Whang, Steven Euijong and Lee, Jae-Gil},
title = {Data Collection and Quality Challenges for Deep Learning},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415562},
doi = {10.14778/3415478.3415562},
abstract = {Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3429–3432},
numpages = {4}
}

@inproceedings{10.1109/WI-IAT.2014.139,
author = {Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
title = {Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.139},
doi = {10.1109/WI-IAT.2014.139},
abstract = {In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 02},
pages = {495–502},
numpages = {8},
keywords = {privacy preserving data mining, algorithm, data publishing},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2335484.2335488,
author = {Artikis, Alexander and Etzion, Opher and Feldman, Zohar and Fournier, Fabiana},
title = {Event Processing under Uncertainty},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335488},
doi = {10.1145/2335484.2335488},
abstract = {Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity ("data in doubt"). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {32–43},
numpages = {12},
keywords = {pattern matching, uncertainty, event recognition, artificial intelligence, event processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/3442381.3450066,
author = {Fang, Minghong and Sun, Minghao and Li, Qi and Gong, Neil Zhenqiang and Tian, Jin and Liu, Jia},
title = {Data Poisoning Attacks and Defenses to Crowdsourcing Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450066},
doi = {10.1145/3442381.3450066},
abstract = { A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {969–980},
numpages = {12},
keywords = {Data poisoning attacks, crowdsourcing, truth discovery},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3520084.3520099,
author = {Li, Yunze and Wu, Yuxuan and Tang, Ruisen},
title = {Data Aggregation and Anomaly Detection System for Isomerism and Heterogeneous Data},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520099},
doi = {10.1145/3520084.3520099},
abstract = {With the development of big data technology, data accessed by big data platforms maintain the features of mass, isomerism, heterogeneous, and streaming. Therefore, how to access the varied data sources of isomerism and heterogeneous data and how to process and analyze the data become the current challenges. In this paper, we design and implement a data aggregation and anomaly detection system for isomerism and heterogeneous data. The system proposes a novel isomerism and heterogeneous data access sub-system. The sub-system applies improved Avro as the unified data description format and presents different storage algorithms for data serialization to raise the data adaption efficiency. The system adopts Kafka as the message middleware for data aggregation and distribution. Also, we design the anomaly detection and alarming sub-system for detecting the anomalies of streaming data on time and notifying the users. The data aggregation and anomaly detection system has passed all the tests and applied in small and medium-sized enterprises.},
booktitle = {2022 The 5th International Conference on Software Engineering and Information Management (ICSIM)},
pages = {95–99},
numpages = {5},
keywords = {serialization and deserialization, Kafka, anomaly detection, isomerism and heterogeneous data},
location = {Yokohama, Japan},
series = {ICSIM 2022}
}

@inproceedings{10.1145/3495018.3495398,
author = {Zhang, Mingjie and Sheng, Yan and Tian, Nuo and Liu, Wei and Wang, Hui and Zhu, Longzhu and Xu, Qing},
title = {Exploring and Analyzing Data Mining Algorithm Technology in Internet Customer Ranking},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495398},
doi = {10.1145/3495018.3495398},
abstract = {Based on the characteristics of online customers, such as user characteristics, interaction behavior, frequency of visits and business queries, this paper uses big data analysis mining algorithm to conduct exploratory analysis on each business data, and builds a weight division model (Entropy Value Method) to achieve online customer ranking.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1352–1360},
numpages = {9},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/2910896.2926735,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {WADL 2016: Third International Workshop on Web Archiving and Digital Libraries},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926735},
doi = {10.1145/2910896.2926735},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {293–294},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inbook{10.1145/3482632.3482675,
author = {Sun, Wen},
title = {Cloud Service Context and Feedback Fusion of Product Design Creative Demand Perception Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482675},
abstract = {After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {207–213},
numpages = {7}
}

@inproceedings{10.5555/3200334.3200410,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {352–353},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.1145/3423603.3424004,
author = {Darmont, J\'{e}r\^{o}me and Favre, C\'{e}cile and Loudcher, Sabine and No\^{u}s, Camille},
title = {Data Lakes for Digital Humanities},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424004},
doi = {10.1145/3423603.3424004},
abstract = {Traditional data in Digital Humanities projects bear various formats (structured, semi-structured, textual) and need substantial transformations (encoding and tagging, stemming, lemmatization, etc.) to be managed and analyzed. To fully master this process, we propose the use of data lakes as a solution to data siloing and big data variety problems. We describe data lake projects we currently run in close collaboration with researchers in humanities and social sciences and discuss the lessons learned running these projects.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {6},
numpages = {4},
keywords = {digital humanities, data lakes, metadata},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3144826.3145387,
author = {Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel},
title = {Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145387},
doi = {10.1145/3144826.3145387},
abstract = {Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {37},
numpages = {8},
keywords = {Governance, Learning Analytics, Higher Education Institutions, Disruption},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3183713.3183753,
author = {Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao},
title = {Catching Numeric Inconsistencies in Graphs},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183753},
doi = {10.1145/3183713.3183753},
abstract = {Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we propose to extend graph functional dependencies with linear arithmetic expressions and comparison predicates, referred to as NGDs. We study fundamental problems for NGDs. We show that their satisfiability, implication and validation problems are Σ 2 p-complete, ¶II2 p-complete and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity.To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs, in response to updates Δ G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {381–393},
numpages = {13},
keywords = {graph dependencies, numeric errors, incremental validation},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3093241.3093268,
author = {Smith, Jeffrey and Rege, Manjeet},
title = {The Data Warehousing (R) Evolution: Where's It Headed Next?},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093268},
doi = {10.1145/3093241.3093268},
abstract = {This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {104–108},
numpages = {5},
keywords = {business, Data, ETL, warehouse, intelligence},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {statistical quality control, crowd workers, multiple choice HIT, crowdsourcing management},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.5555/2602339.2602400,
author = {Petrou, Lambros and Larkou, George and Laoudias, Christos and Zeinalipour-Yazti, Demetrios and Panayiotou, Christos G.},
title = {Demonstration Abstract: Crowdsourced Indoor Localization and Navigation with Anyplace},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this demonstration paper, we present the Anyplace system that relies on the abundance of sensory data on smartphones (e.g., WiFi signal strength and inertial measurements) to deliver reliable indoor geolocation information. Our system features two highly desirable properties, namely crowdsourcing and scalability. Anyplace implements a set of crowdsourcing-supportive mechanisms to handle the enormous amount of crowdsensed data, filter incorrect user contributions and exploit WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows a big-data architecture for efficient and scalable storage and retrieval of localization and mapping data.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {331–332},
numpages = {2},
keywords = {navigation, indoor localization, crowdsourcing},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inbook{10.1145/3500931.3501016,
author = {Gao, Mengke and Zhang, Yan and Gao, Yue},
title = {Research Progress of User Portrait Technology in Medical Field},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501016},
abstract = {In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {500–504},
numpages = {5}
}

@inproceedings{10.1145/3230833.3233288,
author = {Jirovsk\'{y}, V\'{a}clav and Pastorek, Andrej and M\"{u}hlh\"{a}user, Max and Tundis, Andrea},
title = {Cybercrime and Organized Crime},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233288},
doi = {10.1145/3230833.3233288},
abstract = {The way of live in the modern society has changed radically over the past few decades. In particular, thanks to the strong use of information technology, many activities have moved from the real world to the digital world. This has obviously introduced advantages in terms of data management and communication efficiency. Nevertheless, it has given also to the criminals the possibility to move into cybernetic space and, as a consequence, to exploit all the technological advantages available for carrying out their activities. In this context the paper provide an overview on the cybercrime and organized crime by focusing on the concept of crime as a service as well as the main issues related to big data by highlighting the social aspects.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {61},
numpages = {5},
keywords = {Data Security, Cyber-security, Privacy, Cyber-crime},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/3079856.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080241},
doi = {10.1145/3079856.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {666–677},
numpages = {12},
keywords = {Networks-On-Chip, Approximate Computing, Data Compression},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080241},
doi = {10.1145/3140659.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {666–677},
numpages = {12},
keywords = {Data Compression, Networks-On-Chip, Approximate Computing}
}

@inproceedings{10.1145/3340531.3418506,
author = {Huang, Ruihong},
title = {Approximate Event Pattern Matching over Heterogeneous and Dirty Sources},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3418506},
doi = {10.1145/3340531.3418506},
abstract = {Pattern matching is an important task in the field of Complex Event Processing (CEP). However, exact event pattern matching methods could suffer from low hit rate and loss for meaningful events identification due to the heterogeneous and dirty sources in the big data era. Since both events and patterns could be imprecise, the actual event trace may have different event names as well as structures from the pre-defined pattern. The low-quality data even intensifies the difficulty of matching. In this work, we propose to learn embedding representations for patterns and event traces separately and calculate their similarity as the scores for approximate matching.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3237–3240},
numpages = {4},
keywords = {heterogeneous source, complex event processing, approximate match, low-quality data, cep, dirty source},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2851581.2892379,
author = {Verma, Nitya and Voida, Amy},
title = {Mythologies of Business Intelligence},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892379},
doi = {10.1145/2851581.2892379},
abstract = {We present results from a case study of the use of business intelligence (BI) systems in a human services organization. In their organizational trajectory towards a "culture of data," our informants perceived four values associated with BI: data-driven, predictive and proactive, shared accountability, and inquisitive. Each value corresponds to a mythology of big data and BI. For each, we highlight the ways in which the enactment of the mythology is problematized by disconnects between aggregate and drill-down views of data that often impede the desired actionability. Our findings contribute initial empirical evidence of the ways in which the epistemological biases of BI systems influence organizations. We suggest design implications for better enabling data-driven decision making.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2341–2347},
numpages = {7},
keywords = {analytics, values, mythology, business intelligence},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3335550.3335577,
author = {Li, Ruixue and Peng, Can and Sun, Huiliang},
title = {Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335577},
doi = {10.1145/3335550.3335577},
abstract = {From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {92–97},
numpages = {6},
keywords = {Selection Strategy Analysis, Crowdsourcing platform, Appropriate products, Full cost},
location = {Phuket, Thailand},
series = {MSIE 2019}
}

@inproceedings{10.1145/3301761.3301767,
author = {Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng},
title = {Adopting Data Analysis and Visualization Technology to Construct Clinical Research Data Management and Analysis System},
year = {2018},
isbn = {9781450361279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301761.3301767},
doi = {10.1145/3301761.3301767},
abstract = {With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy-to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical research data management analysis system, which combines various technologies to help effectively promote medical clinical.},
booktitle = {Proceedings of the 2018 2nd International Conference on Software and E-Business},
pages = {49–53},
numpages = {5},
keywords = {Visualization, Data analysis, Clinical data},
location = {Zhuhai, China},
series = {ICSEB '18}
}

@article{10.1145/2694428.2694441,
author = {Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, AnHai and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R\'{e}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer},
title = {The Beckman Report on Database Research},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2694428.2694441},
doi = {10.1145/2694428.2694441},
abstract = {Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {61–70},
numpages = {10}
}

@inproceedings{10.1109/MET.2019.00018,
author = {Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan},
title = {Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00018},
doi = {10.1109/MET.2019.00018},
abstract = {In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {70–75},
numpages = {6},
keywords = {natural language processing, machine translation, data validation, social media, Douban, data quality assessment, sentiment analysis, metamorphic relation, Oracle problem, metamorphic testing},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@inproceedings{10.1145/3018661.3022759,
author = {Ensan, Faezeh and Noorian, Zeinab and Bagheri, Ebrahim},
title = {Mining Actionable Insights from Social Networksat WSDM 2017},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3022759},
doi = {10.1145/3018661.3022759},
abstract = {The first international workshop on Mining Actionable Insights from Social Networks (MAISoN'17) is to be held on February 10, 2017; co-located with the Tenth ACM International Web Search and Data Mining (WSDM) Conference in Cambridge, UK. MAISoN'17 aims at bringing together researchers and participants from different disciplines such as computer science, big data mining, machine learning, social network analysis and other related areas in order to identify challenging problems and share ideas, algorithms, and technologies for mining actionable insight from social network data. We organized a workshop program that includes the presentation of eight peer-reviewed papers and keynote talks, which foster discussions around state-of-the-art in social network mining and will hopefully lead to future collaborations and exchanges.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {821–822},
numpages = {2},
keywords = {predictive modeling, social network analysis, web mining},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/2523616.2525963,
author = {Heintz, Benjamin and Chandra, Abhishek and Sitaraman, Ramesh K.},
title = {Wide-Area Streaming Analytics: Distributing the Data Cube},
year = {2013},
isbn = {9781450324281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523616.2525963},
doi = {10.1145/2523616.2525963},
abstract = {To date, much research in data-intensive computing has focused on batch computation. Increasingly, however, it is necessary to derive knowledge from big data streams. As a motivating example, consider a content delivery network (CDN) such as Akamai [4], comprising thousands of servers in hundreds of globally distributed locations. Each of these servers produces a stream of log data, recording for example every user it serves, along with each video stream they access, when they play and pause streams, and more. Each server also records network- and system-level data such as TCP connection statistics. In aggregate, the servers produce billions of lines of log data from over a thousand locations daily.},
booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
articleno = {55},
numpages = {2},
location = {Santa Clara, California},
series = {SOCC '13}
}

@inproceedings{10.1145/2968219.2971593,
author = {De Masi, Alexandre and Ciman, Matteo and Gustarini, Mattia and Wac, Katarzyna},
title = {MQoL Smart Lab: Quality of Life Living Lab for Interdisciplinary Experiments},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2971593},
doi = {10.1145/2968219.2971593},
abstract = {As a base for hypothesis formulation and testing, accurate, timely and reproducible data collection is a challenge for all researchers. Data collection is especially challenging in uncontrolled environments, outside of the lab and when it involves many collaborating disciplines, where the data must serve quality research in all of them. In this paper, we present own "mQoL Smart Lab" for interdisciplinary research efforts on individuals' "Quality of Life" improvement. We present an evolution of our current in-house living lab platform enabling continuous, pervasive data collection from individuals' smartphones. We discuss opportunities for mQoL stemming from developments in machine learning and big data for advanced data analytics in different disciplines, better meeting the requirements put on the platform.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {635–640},
numpages = {6},
keywords = {people centric sensing, data analysis, data collection, platforms, smartphones, data science},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3423603.3424007,
author = {B\"{u}chler, Marco and Riegert, Sarah and Alpi, Federico and Cadeddu, Francesca},
title = {Towards Big Religious Data: RESILIENCE Research Infrastructure for Data on Religion in the Digital Age},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424007},
doi = {10.1145/3423603.3424007},
abstract = {Data in and for religion is arguably as old as humanity. Religious significance has been attached to an immense variety of artifacts and documents, often in written form, in nearly all spoken and written languages over the past millennia. The rise of the digital age gives to the scholar in religious studies the opportunity to build research over a much wider array of data than ever before; institutions which have data repositories (such as libraries, museums, universities, etc.) similarly have the chance to make their collections available to a larger community. On the other hand, however, there is a serious risk that a considerable amount of data gets lost during the "Digital transition". This paper presents the approach of the RESILIENCE Research Infrastructure in dealing with the issue of big data and data loss within the field of religious studies.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {9},
numpages = {5},
keywords = {big religious data, digital transformation, religious studies, research infrastructures},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3268866.3268877,
author = {Zhong, Junmei and Gao, Chuangui and Yi, Xiu},
title = {Categorization of Patient Disease into ICD-10 with NLP and SVM for Chinese Electronic Health Record Analysis},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268877},
doi = {10.1145/3268866.3268877},
abstract = {The electronic health record (EHR) analysis has become an increasingly important application for artificial intelligence (AI) algorithms to leverage the insight from the big data for improving the quality of human healthcare. In a lot of Chinese EHR analysis applications, it is very important to categorize the patients' diseases according to the medical coding standard. In this paper, we develop NLP and machine learning algorithms to automatically categorize each patient's individual diseases into the ICD-10 coding standard. Experimental results show that the support vector machine algorithm (SVM) accomplishes very promising classification results.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {101–106},
numpages = {6},
keywords = {Electronic health record, ICD-10, SVM, NLP, machine learning},
location = {Beijing, China},
series = {AIPR 2018}
}

@inproceedings{10.1145/3390557.3394127,
author = {Zhan, Lin and Junhua, Zhao and Fan, Li and Zhifei, Wang},
title = {Research on Intelligent Management Platform of Highspeed Railway Traffic Safety Equipment Based on CPS},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394127},
doi = {10.1145/3390557.3394127},
abstract = {From the view of high-speed railway traffic safety, this paper establishes an intelligent management platform for operation safety equipment based on CPS for "person-equipment-environment", and designs a framework of traffic safety system composed of perception control hardware, Internet of Things, cognitive decision-making and information services. The deep fusion of information system and traffic safety equipment is discussed, and the fault diagnosis method of driving equipment based on complex sensing technology is given, such as intelligent identification, online monitoring and ubiquitous sensing of the characteristics of safety protection equipment. Through the application of equipment fault diagnosis, it realizes the rapid retrieval and active collection of safety information, provides early warning and auxiliary decision-making, big data analysis and prediction, and improves the traffic safety.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {147–154},
numpages = {8},
keywords = {CPS, equipment fault diagnosis, traffic safety, High-speed railway},
location = {Xiamen, China},
series = {ICIAI 2020}
}

@inproceedings{10.1145/2638404.2638526,
author = {Wang, Maximilian J. and Mao, Guifen and Chen, Haiquan},
title = {Mining Multivariate Outliers: A Mixture Model-Based Framework},
year = {2014},
isbn = {9781450329231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638404.2638526},
doi = {10.1145/2638404.2638526},
abstract = {Mining outliers has become more and more important in recent years. It has wide applications in military surveillance for enemy activities, detection of potential terrorist attacks, credit card fraud detection, network intrusion, computer virus attack, clinical trials, severe weather prediction, athlete performance analysis, and many other data mining tasks. In today's big data age, multivariate data sets are very complex. Variables among different dimensions are usually correlated with different variations. Classical data mining methods with Euclidean distance measure are not working well for mining multivariate outliers. In this study, we propose a normal mixture model-based framework of multivariate outlier detection. We fit the model parameters through the robust EM algorithm. The K-means clustering algorithm is used to provide the initial inputs for the EM algorithm. The well-know Mahalanobis distance is used to determine the cutoff points for outlier detection via the chi-square distribution critical values. Implementation details of this framework are also discussed.},
booktitle = {Proceedings of the 2014 ACM Southeast Regional Conference},
articleno = {51},
numpages = {4},
keywords = {mahalanobis distance, normal mixture models, outlier detection, k-means clustering algorithm, data mining, EM algorithm},
location = {Kennesaw, Georgia},
series = {ACM SE '14}
}

@inproceedings{10.1145/3170427.3174367,
author = {Edge, Darren and Larson, Jonathan and White, Christopher},
title = {Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3174367},
doi = {10.1145/3170427.3174367},
abstract = {The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {data, business intelligence, hci, visual analytics, ai},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3325112.3325245,
author = {Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian},
title = {The Data Firehose and AI in Government: Why Data Management is a Key to Value and Ethics},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325245},
doi = {10.1145/3325112.3325245},
abstract = {Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {171–176},
numpages = {6},
keywords = {Data Management, Artificial Intelligence, Policy Analysis, Data Analytics, DMBOK},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@article{10.1145/3404820.3404824,
author = {Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake},
title = {Mapping County-Level Mobility Pattern Changes in the United States in Response to COVID-19},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
url = {https://doi.org/10.1145/3404820.3404824},
doi = {10.1145/3404820.3404824},
abstract = {To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.},
journal = {SIGSPATIAL Special},
month = {jun},
pages = {16–26},
numpages = {11}
}

@inproceedings{10.1145/2948992.2949007,
author = {Almeida, Ricardo and Maio, Paulo and Oliveira, Paulo and Barroso, Jo\~{a}o},
title = {Ontology Based Rewriting Data Cleaning Operations},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949007},
doi = {10.1145/2948992.2949007},
abstract = {Dealing with increasing amounts of data creates the need to deal with redundant, inconsistent and/or complementary repositories which may be different in their data models and/or in their schema. Current data cleaning techniques developed to tackle data quality problems are just suitable for scenarios were all repositories share the same model and schema. Recently, an ontology-based methodology was proposed to overcome this limitation. In this paper, this methodology is briefly described and applied to a real scenario in the health domain with data quality problems.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {85–88},
numpages = {4},
keywords = {Data Cleaning, Ontology, Rewriting Process, Schema, Vocabulary},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/3282933.3282935,
author = {Mack, Vincent Z. W. and Kam, Tin Seong},
title = {Is There Space for Violence? A Data-Driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict},
year = {2018},
isbn = {9781450360326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282933.3282935},
doi = {10.1145/3282933.3282935},
abstract = {With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
articleno = {1},
numpages = {10},
keywords = {political violence, geospatial autocorrelation, Africa, knowledge discovery, hotspot detection},
location = {Seattle, WA, USA},
series = {GeoHumanities'18}
}

@inproceedings{10.5555/2693848.2694087,
author = {Rabe, Markus and Scheidler, Anne Antonia},
title = {An Approach for Increasing the Level of Accuracy in Supply Chain Simulation by Using Patterns on Input Data},
year = {2014},
publisher = {IEEE Press},
abstract = {Setting up simulation scenarios in the field of Supply Chains (SCs) is a big challenge because complex input data must be specified and careful input data management as well as precise model design are necessary. SC simulation needs a large amount of input data -- especially in times of big data, in which the data is often approximated by statistical distributions from real world observations. This paper deals with the question how the model itself and its input can be effectively complemented. This takes into account the commonly known fact, that the accuracy of a model output depends on the model input. Therefore an approach for using techniques of Knowledge Discovery in Databases is introduced to derive logical relations from the data. We discuss how Knowledge Discovery would be applied, as a preprocessing step for simulation scenario setups, in order to provide benefits for the level of accuracy in simulation models.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {1897–1906},
numpages = {10},
location = {Savannah, Georgia},
series = {WSC '14}
}

